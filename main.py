import os
import sys
import re
import json
from datetime import datetime
import time
import traceback
import requests
import flet as ft
import logging
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver import ChromeOptions
from webdriver_manager.chrome import ChromeDriverManager
import urllib3
from bs4 import BeautifulSoup
import base64
from io import BytesIO
import matplotlib
matplotlib.use('Agg')  # –ù–µ–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –±—ç–∫–µ–Ω–¥

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–æ–¥—É–ª–µ–π
try:
    import pandas as pd
except ImportError:
    print("‚ùå –û—à–∏–±–∫–∞: –ú–æ–¥—É–ª—å 'pandas' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pandas")
    sys.exit(1)

try:
    import openpyxl
except ImportError:
    print("‚ùå –û—à–∏–±–∫–∞: –ú–æ–¥—É–ª—å 'openpyxl' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openpyxl")
    sys.exit(1)

try:
    from docx import Document
    from docx.shared import Inches
    from docx.enum.text import WD_ALIGN_PARAGRAPH
except ImportError:
    print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ú–æ–¥—É–ª—å 'python-docx' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –≠–∫—Å–ø–æ—Ä—Ç –≤ Word –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install python-docx")
    Document = None

try:
    import seaborn as sns
except ImportError:
    print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ú–æ–¥—É–ª—å 'seaborn' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ì—Ä–∞—Ñ–∏–∫–∏ –º–æ–≥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.")
    sns = None

try:
    import matplotlib.pyplot as plt
except ImportError:
    print("‚ùå –û—à–∏–±–∫–∞: –ú–æ–¥—É–ª—å 'matplotlib' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install matplotlib")
    sys.exit(1)

try:
    from PIL import Image  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
except ImportError:
    print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ú–æ–¥—É–ª—å 'Pillow' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–≥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.")
    Image = None
import xml.etree.ElementTree as ET
import threading
from collections import Counter
import string
import ast
import re
import hashlib
import pandas as pd
import os
import certifi
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
os.environ['SSL_CERT_FILE'] = certifi.where()
import threading
from urllib.parse import urljoin, urlparse

# –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ª–æ–≥–æ–≤ Selenium –∏ Chrome
logging.getLogger('selenium').setLevel(logging.WARNING)
logging.getLogger('webdriver_manager').setLevel(logging.WARNING)
logging.getLogger('urllib3').setLevel(logging.WARNING)
logging.getLogger('requests').setLevel(logging.WARNING)

# –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ª–æ–≥–æ–≤ Chrome
os.environ['WDM_LOG_LEVEL'] = '0'
os.environ['WDM_PRINT_FIRST_LINE'] = 'False'

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ SSL –¥–ª—è webdriver_manager
os.environ['WDM_SSL_VERIFY'] = '0'

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
LOG_FILE = "seo_log.txt"
SCREENSHOT_DIR = "screenshots"
REPORT_DIR = "reports"
os.makedirs(SCREENSHOT_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
sitemap_export_data = {}

# –ò–º–ø–æ—Ä—Ç SERP Tracker
try:
    from serp_tracker import SERPTracker, run_serp_tracking, run_detailed_site_analysis
except ImportError:
    print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ú–æ–¥—É–ª—å 'serp_tracker' –Ω–µ –Ω–∞–π–¥–µ–Ω. –§—É–Ω–∫—Ü–∏—è —Ç—Ä–µ–∫–∏–Ω–≥–∞ –ø–æ–∑–∏—Ü–∏–π –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")
    SERPTracker = None
    run_serp_tracking = None
    run_detailed_site_analysis = None

def log_to_file(text):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ª–æ–≥ –≤ —Ñ–∞–π–ª —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S %Z")
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{timestamp}] {text}\n")

def create_webdriver(ignore_ssl=False, window_size=None, anti_bot_mode=False):
    """–°–æ–∑–¥–∞–µ—Ç WebDriver —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫."""
    options = ChromeOptions()
    
    if anti_bot_mode:
        # –†–µ–∂–∏–º –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        options.add_argument("--headless=new")  # –î–æ–±–∞–≤–ª—è–µ–º headless —Ä–µ–∂–∏–º
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # –≠–º—É–ª—è—Ü–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –±—Ä–∞—É–∑–µ—Ä–∞
        options.add_argument("--disable-web-security")
        options.add_argument("--allow-running-insecure-content")
        options.add_argument("--disable-features=VizDisplayCompositor")
        options.add_argument("--disable-features=TranslateUI")
        options.add_argument("--disable-ipc-flooding-protection")
        
        # –°–ª—É—á–∞–π–Ω—ã–π User-Agent
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]
        import random
        options.add_argument(f"--user-agent={random.choice(user_agents)}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-software-rasterizer")
        options.add_argument("--disable-background-networking")
        options.add_argument("--disable-background-timer-throttling")
        options.add_argument("--disable-backgrounding-occluded-windows")
        options.add_argument("--disable-renderer-backgrounding")
        options.add_argument("--log-level=3")
        options.add_argument("--mute-audio")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-plugins")
        options.add_argument("--disable-logging")
        options.add_argument("--silent")
        options.add_argument("--disable-dev-tools")
        
        # –≠–º—É–ª—è—Ü–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
        resolutions = ["1920x1080", "1366x768", "1440x900", "1536x864"]
        if not window_size:
            window_size = random.choice(resolutions)
        
        # –°–ª—É—á–∞–π–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —è–∑—ã–∫–∞ –∏ –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏
        languages = ["ru-RU,ru;q=0.9,en;q=0.8", "en-US,en;q=0.9", "ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3"]
        options.add_argument(f"--lang={random.choice(languages)}")
        
    else:
        # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
        options.add_argument("--headless=new")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-software-rasterizer")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-background-networking")
        options.add_argument("--disable-background-timer-throttling")
        options.add_argument("--disable-backgrounding-occluded-windows")
        options.add_argument("--disable-renderer-backgrounding")
        options.add_argument("--disable-features=TranslateUI")
        options.add_argument("--disable-ipc-flooding-protection")
        options.add_argument("--disable-web-security")
        options.add_argument("--disable-features=VizDisplayCompositor")
        options.add_argument("--log-level=3")
        options.add_argument("--mute-audio")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-plugins")
        options.add_argument("--disable-logging")
        options.add_argument("--silent")
        options.add_argument("--disable-dev-tools")
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        options.add_experimental_option('useAutomationExtension', False)
    
    options.add_experimental_option("prefs", {
        "profile.default_content_setting_values.notifications": 2,
        "profile.default_content_settings.popups": 0,
        "profile.default_content_setting_values.media_stream": 2,
        "profile.default_content_setting_values.geolocation": 2,
        "profile.default_content_setting_values.images": 2,  # –û—Ç–∫–ª—é—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        "profile.managed_default_content_settings.images": 2
    })
    
    if window_size:
        options.add_argument(f"--window-size={window_size}")
    
    if ignore_ssl:
        options.add_argument("--ignore-certificate-errors")
        options.add_argument("--ignore-ssl-errors")
    
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        
        if anti_bot_mode:
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è –æ–±—Ö–æ–¥–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
            driver.execute_script("Object.defineProperty(navigator, 'languages', {get: () => ['ru-RU', 'ru', 'en-US', 'en']})")
        
        driver.set_page_load_timeout(60)
        driver.implicitly_wait(10)
        driver.set_script_timeout(30)
        return driver
    except Exception as e:
        log_to_file(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è WebDriver: {str(e)}")
        raise e

def try_multiple_access_methods(site_url, ignore_ssl=False):
    """–ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Å–∞–π—Ç—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."""
    methods_results = []
    
    # –ú–µ—Ç–æ–¥ 1: –û–±—ã—á–Ω—ã–π requests
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        r = requests.get(site_url, timeout=15, verify=not ignore_ssl, headers=headers, allow_redirects=True)
        methods_results.append(('requests', r.status_code, r.text[:500], r.headers))
    except Exception as e:
        methods_results.append(('requests', 'error', str(e), {}))
    
    # –ú–µ—Ç–æ–¥ 2: requests —Å —Å–µ—Å—Å–∏–µ–π
    try:
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        r = session.get(site_url, timeout=15, verify=not ignore_ssl, allow_redirects=True)
        methods_results.append(('session', r.status_code, r.text[:500], r.headers))
    except Exception as e:
        methods_results.append(('session', 'error', str(e), {}))
    
    # –ú–µ—Ç–æ–¥ 3: requests —Å –ø—Ä–æ–∫—Å–∏-–∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'max-age=0',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
        }
        r = requests.get(site_url, timeout=15, verify=not ignore_ssl, headers=headers, allow_redirects=True)
        methods_results.append(('headers', r.status_code, r.text[:500], r.headers))
    except Exception as e:
        methods_results.append(('headers', 'error', str(e), {}))
    
    return methods_results

def check_site_accessibility(site_url, ignore_ssl=False):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–∞–π—Ç–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."""
    log_text = f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–∞: {site_url}\n"
    
    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–æ—Å—Ç—É–ø–∞
    methods_results = try_multiple_access_methods(site_url, ignore_ssl)
    
    successful_methods = []
    blocked_methods = []
    
    for method, status, content, headers in methods_results:
        log_text += f"\nüì° –ú–µ—Ç–æ–¥ {method}:\n"
        
        if status == 'error':
            log_text += f"‚ùå –û—à–∏–±–∫–∞: {content}\n"
            blocked_methods.append(method)
        elif status == 200:
            log_text += f"‚úÖ –£—Å–ø–µ—à–Ω–æ (—Å—Ç–∞—Ç—É—Å: {status})\n"
            successful_methods.append(method)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            if any(blocked_text in content.lower() for blocked_text in [
                'access denied', '–¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω', 'blocked', '–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω',
                'cloudflare', 'captcha', 'recaptcha', 'bot', 'robot'
            ]):
                log_text += f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º\n"
                blocked_methods.append(method)
            else:
                log_text += f"‚úÖ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ—Å—Ç—É–ø–Ω–æ\n"
        else:
            log_text += f"‚ö†Ô∏è –°—Ç–∞—Ç—É—Å: {status}\n"
            if status in [403, 429, 503]:
                blocked_methods.append(method)
            else:
                successful_methods.append(method)
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if successful_methods:
        log_text += f"\n‚úÖ –£—Å–ø–µ—à–Ω—ã–µ –º–µ—Ç–æ–¥—ã: {', '.join(successful_methods)}\n"
        return True, log_text, successful_methods[0]
    else:
        log_text += f"\n‚ùå –í—Å–µ –º–µ—Ç–æ–¥—ã –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã: {', '.join(blocked_methods)}\n"
        return False, log_text, None

def parse_summary(summary_content, report_type='full'):
    """–ü–∞—Ä—Å–∏—Ç —Å–≤–æ–¥–∫—É –¥–ª—è Excel."""
    if report_type == 'images':
        data = {'–°—Å—ã–ª–∫–∞': [], 'Alt': [], 'Title': [], '–†–∞–∑–º–µ—Ä': []}
        current = {}
        for line in summary_content.split('\n'):
            line = line.strip()
            if line.startswith('–°—Å—ã–ª–∫–∞: '):
                if current:
                    data['–°—Å—ã–ª–∫–∞'].append(current.get('src', ''))
                    data['Alt'].append(current.get('alt', ''))
                    data['Title'].append(current.get('title', ''))
                    data['–†–∞–∑–º–µ—Ä'].append(current.get('size', ''))
                current = {'src': line[8:]}
            elif line.startswith('Alt: '):
                current['alt'] = line[5:]
            elif line.startswith('Title: '):
                current['title'] = line[7:]
            elif line.startswith('–†–∞–∑–º–µ—Ä: '):
                current['size'] = line[8:]
        if current:
            data['–°—Å—ã–ª–∫–∞'].append(current.get('src', ''))
            data['Alt'].append(current.get('alt', ''))
            data['Title'].append(current.get('title', ''))
            data['–†–∞–∑–º–µ—Ä'].append(current.get('size', ''))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        if not data['–°—Å—ã–ª–∫–∞']:
            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            data['–°—Å—ã–ª–∫–∞'] = ['–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö']
            data['Alt'] = ['-']
            data['Title'] = ['-']
            data['–†–∞–∑–º–µ—Ä'] = ['-']
        
        return data
    elif report_type == 'parser':
        data = {'–°—Å—ã–ª–∫–∞': [], '–†–µ–∑—É–ª—å—Ç–∞—Ç': [], '–°—Ç–∞—Ç—É—Å': []}
        for line in summary_content.split('\n'):
            line = line.strip()
            if line.startswith('- '):
                parts = line[2:].split(': ')
                if len(parts) == 2:
                    url = parts[0]
                    details = parts[1].split(' ')
                    emoji = details[0]
                    status = ' '.join(details[1:])
                    data['–°—Å—ã–ª–∫–∞'].append(url)
                    data['–†–µ–∑—É–ª—å—Ç–∞—Ç'].append(emoji)
                    data['–°—Ç–∞—Ç—É—Å'].append(status)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        if not data['–°—Å—ã–ª–∫–∞']:
            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            data['–°—Å—ã–ª–∫–∞'] = ['–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö']
            data['–†–µ–∑—É–ª—å—Ç–∞—Ç'] = ['-']
            data['–°—Ç–∞—Ç—É—Å'] = ['–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö']
        
        return data
    else:
        sections = {'–•–æ—Ä–æ—à–µ–µ': [], '–ü—Ä–æ–±–ª–µ–º—ã': [], '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏': []}
        current_section = None
        for line in summary_content.split('\n'):
            line = line.strip()
            if line.startswith('**–•–æ—Ä–æ—à–µ–µ:**'):
                current_section = '–•–æ—Ä–æ—à–µ–µ'
            elif line.startswith('**–ü—Ä–æ–±–ª–µ–º—ã:**'):
                current_section = '–ü—Ä–æ–±–ª–µ–º—ã'
            elif line.startswith('**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**'):
                current_section = '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏'
            elif line.startswith('‚úÖ ') or line.startswith('‚ùå ') or line.startswith('üìù '):
                if current_section:
                    sections[current_section].append(line[2:].strip())
            elif line.startswith('- '):  # –î–ª—è —Å–ø–∏—Å–∫–æ–≤, –∫–∞–∫ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö/—Å—Å—ã–ª–∫–∞—Ö
                if current_section:
                    sections[current_section].append(line[2:].strip())
        max_len = max(len(sections[s]) for s in sections)
        if max_len > 0:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            for s in sections:
                sections[s] += [''] * (max_len - len(sections[s]))
        else:
            # –ï—Å–ª–∏ –≤—Å–µ —Å–ø–∏—Å–∫–∏ –ø—É—Å—Ç—ã–µ, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            for s in sections:
                sections[s] = ['']
        return sections

def save_results(site_url, log_content, summary_content, report_type='full', format='excel'):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Excel."""
    report_path = f"{REPORT_DIR}/{report_type}_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
    sections = parse_summary(summary_content, report_type)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    if not sections or all(len(v) == 0 for v in sections.values()):
        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å —Å–æ–æ–±—â–µ–Ω–∏–µ–º –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        if report_type == 'parser':
            sections = {
                '–°—Å—ã–ª–∫–∞': ['–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞'],
                '–†–µ–∑—É–ª—å—Ç–∞—Ç': ['-'],
                '–°—Ç–∞—Ç—É—Å': ['–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–µ—Ä–∞']
            }
        elif report_type == 'images':
            sections = {
                '–°—Å—ã–ª–∫–∞': ['–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞'],
                'Alt': ['-'],
                'Title': ['-'],
                '–†–∞–∑–º–µ—Ä': ['-']
            }
        else:
            sections = {
                '–•–æ—Ä–æ—à–µ–µ': ['–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞'],
                '–ü—Ä–æ–±–ª–µ–º—ã': ['–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞'],
                '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏': ['–ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –∑–∞–Ω–æ–≤–æ']
            }
    
    df = pd.DataFrame(sections)
    df.to_excel(report_path, index=False)
    return report_path

def check_resource(url, ignore_ssl):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ä–µ—Å—É—Ä—Å–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å, –≤—Ä–µ–º—è, –∏—Å—Ç–æ—Ä–∏—é —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤."""
    try:
        response = requests.get(url, timeout=5, verify=not ignore_ssl, allow_redirects=True)
        return url, response.status_code, response.elapsed.total_seconds(), response.history
    except Exception as e:
        return url, f"Error: {str(e)}", 0, []

def get_image_size(url, ignore_ssl):
    """–ü–æ–ª—É—á–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ö–ë."""
    try:
        response = requests.get(url, timeout=5, verify=not ignore_ssl)
        if response.status_code == 200:
            return len(response.content) / 1024
        return 0
    except:
        return 0

def check_seo_files(site_url, ignore_ssl):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å robots.txt –∏ sitemap.xml."""
    results = []
    for file in ["robots.txt", "sitemap.xml"]:
        url = f"{site_url.rstrip('/')}/{file}"
        try:
            response = requests.get(url, timeout=5, verify=not ignore_ssl)
            results.append((file, response.status_code == 200, response.text if response.status_code == 200 else ""))
        except Exception as e:
            results.append((file, False, str(e)))
    return results

def analyze_robots_txt(robots_content):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç robots.txt –Ω–∞ –æ—à–∏–±–∫–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
    errors = []
    positives = []
    found_directives = []
    recommendations = []
    if not robots_content:
        errors.append("robots.txt –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        recommendations.append("–°–æ–∑–¥–∞–π—Ç–µ robots.txt —Å –±–∞–∑–æ–≤—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä: 'User-agent: * \nDisallow: /private/'")
        return errors, positives, found_directives, recommendations

    lines = [line.strip() for line in robots_content.splitlines() if line.strip() and not line.strip().startswith("#")]
    user_agents = set()
    disallow_rules = []
    allow_rules = []
    sitemap_urls = []
    crawl_delays = {}
    host = None

    for line in lines:
        parts = line.split(":", 1)
        if len(parts) != 2:
            continue
        directive, value = parts[0].lower().strip(), parts[1].strip()
        found_directives.append(line)
        if directive == "user-agent":
            user_agents.add(value)
        elif directive == "disallow":
            disallow_rules.append(value)
        elif directive == "allow":
            allow_rules.append(value)
        elif directive == "sitemap":
            sitemap_urls.append(value)
        elif directive == "crawl-delay":
            try:
                ua, delay = value.split(maxsplit=1)
                crawl_delays[ua] = float(delay)
            except ValueError:
                errors.append(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π Crawl-delay: {value}")
                recommendations.append("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Crawl-delay —É–∫–∞–∑–∞–Ω –∫–∞–∫ 'User-agent: <bot> <delay>'.")
        elif directive == "host":
            host = value

    if user_agents:
        positives.append(f"–ù–∞–π–¥–µ–Ω–æ {len(user_agents)} User-agent(s): {', '.join(user_agents)}")
    if disallow_rules:
        positives.append(f"–ù–∞–π–¥–µ–Ω–æ {len(disallow_rules)} –ø—Ä–∞–≤–∏–ª Disallow")
    if allow_rules:
        positives.append(f"–ù–∞–π–¥–µ–Ω–æ {len(allow_rules)} –ø—Ä–∞–≤–∏–ª Allow")
    if sitemap_urls:
        positives.append(f"–ù–∞–π–¥–µ–Ω–æ {len(sitemap_urls)} Sitemap URL(s): {', '.join(sitemap_urls)}")
    if host:
        positives.append(f"–ù–∞–π–¥–µ–Ω Host: {host}")
    if crawl_delays:
        positives.append(f"–ù–∞–π–¥–µ–Ω–æ {len(crawl_delays)} –ø—Ä–∞–≤–∏–ª Crawl-delay")

    if not user_agents:
        errors.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å–µ–∫—Ü–∏—è User-agent")
        recommendations.append("–î–æ–±–∞–≤—å—Ç–µ 'User-agent: *' –¥–ª—è –æ–±—â–∏—Ö –ø—Ä–∞–≤–∏–ª.")
    if not disallow_rules and not allow_rules:
        errors.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∞–≤–∏–ª–∞ Disallow –∏–ª–∏ Allow")
        recommendations.append("–î–æ–±–∞–≤—å—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –ø—Ä–∞–≤–∏–ª–æ, –Ω–∞–ø—Ä–∏–º–µ—Ä 'Disallow: /private/'.")
    if "*" not in user_agents and len(user_agents) > 1:
        errors.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—â–∏–π User-agent (*) –ø—Ä–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö")
        recommendations.append("–î–æ–±–∞–≤—å—Ç–µ 'User-agent: *' –¥–ª—è –æ–±—â–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫.")
    if any(not rule.startswith("/") for rule in disallow_rules + allow_rules):
        errors.append("–ü—Ä–∞–≤–∏–ª–∞ Disallow/Allow –Ω–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å '/'")
        recommendations.append("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø—É—Ç–∏ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å '/', –Ω–∞–ø—Ä–∏–º–µ—Ä 'Disallow: /admin/'.")
    if len(sitemap_urls) > 1 and not all(url.startswith("http") for url in sitemap_urls):
        errors.append("–ù–µ–∫–æ—Ç–æ—Ä—ã–µ Sitemap URL –Ω–µ —è–≤–ª—è—é—Ç—Å—è –∞–±—Å–æ–ª—é—Ç–Ω—ã–º–∏")
        recommendations.append("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–ª–Ω—ã–µ URL, –Ω–∞–ø—Ä–∏–º–µ—Ä 'Sitemap: https://example.com/sitemap.xml'.")

    return errors, positives, found_directives, recommendations

def check_robots_summary(site_url, ignore_ssl):
    """–û—Ç–¥–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ robots.txt —Å –≤—ã–≤–æ–¥–æ–º —á—Ç–æ —Ö–æ—Ä–æ—à–æ –∏ —á—Ç–æ –ø–ª–æ—Ö–æ."""
    seo_files = check_seo_files(site_url, ignore_ssl)
    robots_status = next((status for file, status, content in seo_files if file == "robots.txt"), False)
    robots_content = next((content for file, status, content in seo_files if file == "robots.txt"), "")
    
    if not robots_status:
        return "‚ùå robots.txt –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω\nüìù –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –°–æ–∑–¥–∞–π—Ç–µ robots.txt."
    
    errors, positives, found_directives, recommendations = analyze_robots_txt(robots_content)
    
    summary = "### –ü—Ä–æ–≤–µ—Ä–∫–∞ robots.txt\n\n"
    if positives:
        summary += "**–•–æ—Ä–æ—à–µ–µ:**\n" + "\n".join(f"‚úÖ {p}" for p in positives) + "\n\n"
    if errors:
        summary += "**–ü—Ä–æ–±–ª–µ–º—ã:**\n" + "\n".join(f"‚ùå {e}" for e in errors) + "\n\n"
    if recommendations:
        summary += "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n" + "\n".join(f"üìù {r}" for r in recommendations) + "\n\n"
    summary += f"**–ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–∏–≤—ã:** {', '.join(found_directives) if found_directives else '–ù–µ—Ç –¥–∏—Ä–µ–∫—Ç–∏–≤'}\n"
    
    return summary

def get_site_pages(site_url, ignore_ssl, max_pages=15000):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ —Å–∞–π—Ç–µ."""
    site_pages = set()
    
    try:
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –±–∞–∑–æ–≤—ã–π URL
        base_url = site_url.rstrip('/')
        
        # –ü–æ–ª—É—á–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        response = requests.get(site_url, verify=not ignore_ssl, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–º –∂–µ –¥–æ–º–µ–Ω–µ
            for link in soup.find_all('a', href=True):
                href = link['href'].strip()
                if not href:
                    continue
                    
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
                if href.startswith('/'):
                    # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞
                    full_url = base_url + href
                elif href.startswith(base_url):
                    # –ê–±—Å–æ–ª—é—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–º –∂–µ –¥–æ–º–µ–Ω–µ
                    full_url = href
                elif href.startswith('http') and base_url in href:
                    # –°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ—Ç –∂–µ –¥–æ–º–µ–Ω
                    full_url = href
                else:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏
                    continue
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL (—É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Å–ª–µ—à–∏ –∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã)
                full_url = full_url.rstrip('/')
                if '#' in full_url:
                    full_url = full_url.split('#')[0]
                if '?' in full_url:
                    full_url = full_url.split('?')[0]
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π URL
                if full_url.startswith('http'):
                    site_pages.add(full_url)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            site_pages.add(base_url)
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–∞–π—Ç–∞: {e}")
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
    return sorted(list(site_pages))[:max_pages]

def validate_sitemap(sitemap_content, site_url, ignore_ssl):
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç sitemap.xml —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π sitemap index –∏ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π."""
    errors = []
    positives = []
    recommendations = []
    urls_in_sitemap = []
    page_details = []  # –ù–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π –ø–æ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    sitemap_info = {}  # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ sitemap
    pages_not_in_sitemap = []  # –°—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Å–∞–π—Ç–µ, –Ω–æ –Ω–µ –≤ sitemap
    pages_in_sitemap_not_on_site = []  # –°—Ç—Ä–∞–Ω–∏—Ü—ã –≤ sitemap, –Ω–æ –Ω–µ –Ω–∞ —Å–∞–π—Ç–µ
    
    try:
        root = ET.fromstring(sitemap_content)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø sitemap
        if 'sitemapindex' in root.tag:
            # –≠—Ç–æ sitemap index
            positives.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω sitemap index (–∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)")
            sitemap_elems = list(root.iter('{http://www.sitemaps.org/schemas/sitemap/0.9}sitemap'))
            sitemap_urls = [sitemap_elem.findtext('{http://www.sitemaps.org/schemas/sitemap/0.9}loc') for sitemap_elem in sitemap_elems]
            sitemap_info['type'] = 'sitemapindex'
            sitemap_info['sub_sitemaps'] = sitemap_urls
            positives.append(f"–ù–∞–π–¥–µ–Ω–æ {len(sitemap_urls)} –ø–æ–¥—á–∏–Ω–µ–Ω–Ω—ã—Ö sitemap")
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –ø–æ–¥—á–∏–Ω–µ–Ω–Ω—ã–µ sitemap
            all_urls = []
            all_sources = {}
            all_metadata = {}
            all_errors = []
            
            for sub_sitemap_url in sitemap_urls:
                if sub_sitemap_url:
                    sub_result = process_sitemap_recursively(sub_sitemap_url, ignore_ssl)
                    all_urls.extend(sub_result['urls'])
                    all_sources.update(sub_result['sources'])
                    all_metadata.update(sub_result['metadata'])
                    all_errors.extend(sub_result['errors'])
            
            urls_in_sitemap = all_urls
            errors.extend(all_errors)
            
            if all_urls:
                positives.append(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_urls)} URL –≤–æ –≤—Å–µ—Ö sitemap")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ URL –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–ª—è –≤—Å–µ—Ö
                for i, url in enumerate(all_urls):
                    # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è URL
                    url_metadata = all_metadata.get(url, {})
                    lastmod = url_metadata.get('lastmod', '-')
                    priority = url_metadata.get('priority', '-')
                    changefreq = url_metadata.get('changefreq', '-')
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–ª—è –≤—Å–µ—Ö URL
                    status = '–Ω–µ –û–ö'
                    if url and url.startswith('http'):
                        _, http_status, _, _ = check_resource(url, ignore_ssl)
                        if isinstance(http_status, int) and http_status == 200:
                            status = '–û–ö'
                        else:
                            status = f'–Ω–µ –û–ö ({http_status})'
                    
                    page_details.append({
                        'url': url,
                        'status': status,
                        'lastmod': lastmod,
                        'priority': priority,
                        'changefreq': changefreq,
                        'source_sitemap': all_sources.get(url, '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                    })
            
        elif 'urlset' in root.tag:
            # –≠—Ç–æ –æ–±—ã—á–Ω—ã–π sitemap
            positives.append("–ö–æ—Ä–Ω–µ–≤–æ–π —ç–ª–µ–º–µ–Ω—Ç –≤–µ—Ä–Ω—ã–π (–æ–±—ã—á–Ω—ã–π sitemap)")
            sitemap_info['type'] = 'urlset'
            url_elems = list(root.iter('{http://www.sitemaps.org/schemas/sitemap/0.9}url'))
            urls = [url_elem.findtext('{http://www.sitemaps.org/schemas/sitemap/0.9}loc') for url_elem in url_elems]
            urls_in_sitemap = urls
            positives.append(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL –≤ sitemap")
            
            if len(urls) > 50000:
                errors.append("Sitemap —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 50,000 URL")
                recommendations.append("–†–∞–∑–¥–µ–ª–∏—Ç–µ sitemap –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤.")
            
            if not all(url and url.startswith('http') for url in urls if url):
                errors.append("–ù–µ–∫–æ—Ç–æ—Ä—ã–µ URL –≤ sitemap –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã –∏–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ")
                recommendations.append("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ URL –≤ sitemap.")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ URL –≤ sitemap –∏ —Å–±–æ—Ä –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π
            for url_elem in url_elems:
                url = url_elem.findtext('{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
                lastmod = url_elem.findtext('{http://www.sitemaps.org/schemas/sitemap/0.9}lastmod') or '-'
                priority = url_elem.findtext('{http://www.sitemaps.org/schemas/sitemap/0.9}priority') or '-'
                changefreq = url_elem.findtext('{http://www.sitemaps.org/schemas/sitemap/0.9}changefreq') or '-'
                status = '–Ω–µ –û–ö'
                http_status = None
                if url and url.startswith('http'):
                    _, http_status, _, _ = check_resource(url, ignore_ssl)
                    if isinstance(http_status, int) and http_status == 200:
                        status = '–û–ö'
                    else:
                        status = f'–Ω–µ –û–ö ({http_status})'
                page_details.append({
                    'url': url,
                    'status': status,
                    'lastmod': lastmod,
                    'priority': priority,
                    'changefreq': changefreq,
                    'source_sitemap': '–æ—Å–Ω–æ–≤–Ω–æ–π sitemap'
                })
        else:
            errors.append("–ù–µ–≤–µ—Ä–Ω—ã–π –∫–æ—Ä–Ω–µ–≤–æ–π —ç–ª–µ–º–µ–Ω—Ç sitemap.xml (–æ–∂–∏–¥–∞–µ—Ç—Å—è urlset –∏–ª–∏ sitemapindex)")
            recommendations.append("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ sitemap —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ö–µ–º–µ http://www.sitemaps.org/schemas/sitemap/0.9")
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ –û–ö, –¥–æ–±–∞–≤–∏—Ç—å –≤ errors
        broken_urls = [d['url'] for d in page_details if d['status'] != '–û–ö']
        if broken_urls:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ URL –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ
            errors.append(f"–ù–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ URL –≤ sitemap ({len(broken_urls)} URL):")
            for url in broken_urls:
                errors.append(f"  - {url}")
            recommendations.append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ –∏–ª–∏ —É–¥–∞–ª–∏—Ç–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ URL –∏–∑ sitemap.")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Å–∞–π—Ç–µ vs sitemap
        try:
            site_pages = get_site_pages(site_url, ignore_ssl)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            def normalize_url(url):
                """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
                if not url:
                    return url
                url = url.rstrip('/')
                if '#' in url:
                    url = url.split('#')[0]
                if '?' in url:
                    url = url.split('?')[0]
                return url
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å–µ URL
            normalized_site_pages = set(normalize_url(url) for url in site_pages)
            normalized_sitemap_urls = set(normalize_url(url) for url in urls_in_sitemap)
            
            # –°—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Å–∞–π—Ç–µ, –Ω–æ –Ω–µ –≤ sitemap
            pages_not_in_sitemap = [url for url in site_pages if normalize_url(url) not in normalized_sitemap_urls]
            
            # –°—Ç—Ä–∞–Ω–∏—Ü—ã –≤ sitemap, –Ω–æ –Ω–µ –Ω–∞ —Å–∞–π—Ç–µ (–Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ)
            pages_in_sitemap_not_on_site = [url for url in urls_in_sitemap if normalize_url(url) not in normalized_site_pages and url in broken_urls]
            
            if pages_not_in_sitemap:
                errors.append(f"–°—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Å–∞–π—Ç–µ –Ω–µ –≤ sitemap ({len(pages_not_in_sitemap)} URL):")
                for url in pages_not_in_sitemap[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                    errors.append(f"  - {url}")
                if len(pages_not_in_sitemap) > 10:
                    errors.append(f"  ... –∏ –µ—â–µ {len(pages_not_in_sitemap) - 10} URL")
                recommendations.append("–î–æ–±–∞–≤—å—Ç–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ sitemap.")
            
            if pages_in_sitemap_not_on_site:
                errors.append(f"–°—Ç—Ä–∞–Ω–∏—Ü—ã –≤ sitemap –Ω–µ –Ω–∞ —Å–∞–π—Ç–µ ({len(pages_in_sitemap_not_on_site)} URL):")
                for url in pages_in_sitemap_not_on_site[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                    errors.append(f"  - {url}")
                if len(pages_in_sitemap_not_on_site) > 10:
                    errors.append(f"  ... –∏ –µ—â–µ {len(pages_in_sitemap_not_on_site) - 10} URL")
                recommendations.append("–£–¥–∞–ª–∏—Ç–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ sitemap.")
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–∞–π—Ç–∞: {e}")
            
    except ET.ParseError as e:
        errors.append(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ sitemap.xml: {str(e)}")
        recommendations.append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ XML —Å—Ç—Ä—É–∫—Ç—É—Ä—É sitemap.")
        page_details = []
    
    return errors, positives, recommendations, urls_in_sitemap, page_details, sitemap_info, pages_not_in_sitemap, pages_in_sitemap_not_on_site

def check_sitemap_summary(site_url, ignore_ssl):
    """–û—Ç–¥–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ sitemap.xml —Å –≤—ã–≤–æ–¥–æ–º —á—Ç–æ —Ö–æ—Ä–æ—à–æ –∏ —á—Ç–æ –ø–ª–æ—Ö–æ –∏ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—è–º–∏ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º."""
    seo_files = check_seo_files(site_url, ignore_ssl)
    sitemap_status = next((status for file, status, content in seo_files if file == "sitemap.xml"), False)
    sitemap_content = next((content for file, status, content in seo_files if file == "sitemap.xml"), "")
    
    if not sitemap_status:
        return "‚ùå sitemap.xml –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω\nüìù –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –°–æ–∑–¥–∞–π—Ç–µ sitemap.xml."
    
    errors, positives, recommendations, urls_in_sitemap, page_details, sitemap_info, pages_not_in_sitemap, pages_in_sitemap_not_on_site = validate_sitemap(sitemap_content, site_url, ignore_ssl)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
    global sitemap_export_data
    sitemap_export_data = {
        'urls': urls_in_sitemap,
        'page_details': page_details,
        'sitemap_info': sitemap_info,
        'errors': errors,
        'positives': positives,
        'recommendations': recommendations,
        'pages_not_in_sitemap': pages_not_in_sitemap,
        'pages_in_sitemap_not_on_site': pages_in_sitemap_not_on_site
    }
    
    summary = "### –ü—Ä–æ–≤–µ—Ä–∫–∞ sitemap.xml\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∏–ø–µ sitemap
    if sitemap_info.get('type') == 'sitemapindex':
        summary += "**üîó –¢–∏–ø sitemap:** Sitemap Index (–∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)\n\n"
        if sitemap_info.get('sub_sitemaps'):
            summary += "**üìã –ü–æ–¥—á–∏–Ω–µ–Ω–Ω—ã–µ sitemap:**\n"
            for i, sub_sitemap in enumerate(sitemap_info['sub_sitemaps'][:10], 1):
                summary += f"{i}. {sub_sitemap}\n"
            if len(sitemap_info['sub_sitemaps']) > 10:
                summary += f"... –∏ –µ—â–µ {len(sitemap_info['sub_sitemaps']) - 10} sitemap\n"
            summary += "\n"
    else:
        summary += "**üîó –¢–∏–ø sitemap:** –û–±—ã—á–Ω—ã–π sitemap\n\n"
    
    if positives:
        summary += "**–•–æ—Ä–æ—à–µ–µ:**\n" + "\n".join(f"‚úÖ {p}" for p in positives) + "\n\n"
    if errors:
        summary += "**–ü—Ä–æ–±–ª–µ–º—ã:**\n"
        for e in errors:
            if e.startswith("–ù–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ URL –≤ sitemap"):
                # –î–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö URL –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Ö –æ—Ç–¥–µ–ª—å–Ω–æ
                summary += f"‚ùå {e}\n"
            else:
                summary += f"‚ùå {e}\n"
        summary += "\n"
    if recommendations:
        summary += "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n" + "\n".join(f"üìù {r}" for r in recommendations) + "\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª —Å –ø—Ä–æ–±–ª–µ–º–Ω—ã–º–∏ URL
    broken_urls = [d for d in page_details if d['status'] != '–û–ö']
    if broken_urls:
        summary += "**üî¥ –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ URL (–Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ):**\n"
        for i, d in enumerate(broken_urls, 1):
            summary += f"{i}. {d['url']} - {d['status']}\n"
        summary += "\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ URL
    working_urls = [d for d in page_details if d['status'] == '–û–ö']
    if working_urls:
        summary += f"**üü¢ –î–æ—Å—Ç—É–ø–Ω—ã–µ URL: {len(working_urls)} URL**\n\n"
    
    if page_details:
        summary += "**–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º (–ø–µ—Ä–≤—ã–µ 50):**\n"
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 50 URL –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ
        display_details = page_details[:50]
        for d in display_details:
            summary += f"URL: {d['url']}\n"
            summary += f"–°—Ç–∞—Ç—É—Å: {d['status']}\n"
            if 'source_sitemap' in d:
                summary += f"–ò—Å—Ç–æ—á–Ω–∏–∫: {d['source_sitemap']}\n"
            if d['lastmod'] != '-':
                summary += f"lastmod: {d['lastmod']}\n"
            if d['priority'] != '-':
                summary += f"priority: {d['priority']}\n"
            if d['changefreq'] != '-':
                summary += f"changefreq: {d['changefreq']}\n"
            summary += "\n"
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –±–æ–ª—å—à–µ URL, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if len(page_details) > 50:
            summary += f"**... –∏ –µ—â–µ {len(page_details) - 50} URL (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç–∫—Å–ø–æ—Ä—Ç –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞)**\n\n"
    return summary

def analyze_keywords(driver, site_url, target_keywords):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤."""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∏–¥–∏–º—ã–π —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ JavaScript
        visible_text = driver.execute_script("""
            function getVisibleText() {
                // –£–¥–∞–ª—è–µ–º —Å–∫—Ä—ã—Ç—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                const hiddenElements = document.querySelectorAll('script, style, noscript, [style*="display: none"], [style*="visibility: hidden"], .hidden, .invisible');
                hiddenElements.forEach(el => el.style.display = 'none');
                
                // –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –≤–∏–¥–∏–º—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                const walker = document.createTreeWalker(
                    document.body,
                    NodeFilter.SHOW_TEXT,
                    {
                        acceptNode: function(node) {
                            const style = window.getComputedStyle(node.parentElement);
                            if (style.display === 'none' || style.visibility === 'hidden' || style.opacity === '0') {
                                return NodeFilter.FILTER_REJECT;
                            }
                            return NodeFilter.FILTER_ACCEPT;
                        }
                    }
                );
                
                let text = '';
                let node;
                while (node = walker.nextNode()) {
                    text += node.textContent + ' ';
                }
                
                return text;
            }
            return getVisibleText();
        """)
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        text = re.sub(r'\s+', ' ', visible_text).strip().lower()
        
        # –£–¥–∞–ª—è–µ–º –º–µ—Ç–∞-—Ç–µ–≥–∏ –∏ —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        text = re.sub(r'<[^>]+>', '', text)  # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
        text = re.sub(r'javascript:', '', text, flags=re.IGNORECASE)
        text = re.sub(r'http[s]?://[^\s]+', '', text)  # –£–¥–∞–ª—è–µ–º URL
        text = re.sub(r'www\.[^\s]+', '', text)
        text = re.sub(r'[^\w\s–∞-—è—ë]', ' ', text)  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
        stop_words = {'–∏', '–≤', '–Ω–∞', '–Ω–µ', '—Å', '–∞', '–æ', '–¥–ª—è', '–ø–æ', '–∏–∑', '–∫', '—É', '–æ—Ç', '–Ω–æ', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '—Ç–æ', '–∏–ª–∏', '–∑–∞', '–ø—Ä–∏', 'meta', 'title', 'description', 'keywords', 'og', 'twitter', 'schema', 'json', 'ld'}
        words = re.findall(r'\w+', text)
        word_freq = {}
        for word in words:
            if word not in stop_words and len(word) > 3:
                word_freq[word] = word_freq.get(word, 0) + 1
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏ –≤—ã–±–æ—Ä —Ç–æ–ø-10
        keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        total_words = len(words) or 1  # –ò–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
        keyword_density = {word: freq / total_words for word, freq in keywords}

        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∫–ª–æ–Ω–µ–Ω–∏–π —Ä—É—Å—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞
        def generate_declensions(word):
            """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞."""
            declensions = [word]
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è —Å–ª–æ–≤
            special_declensions = {
                '–¥–æ—Å—Ç–∞–≤–∫–∞': ['–¥–æ—Å—Ç–∞–≤–∫–∏', '–¥–æ—Å—Ç–∞–≤–∫—É', '–¥–æ—Å—Ç–∞–≤–∫–æ–π', '–¥–æ—Å—Ç–∞–≤–∫–µ', '–¥–æ—Å—Ç–∞–≤–∫–∏', '–¥–æ—Å—Ç–∞–≤–æ–∫', '–¥–æ—Å—Ç–∞–≤–∫–∞–º', '–¥–æ—Å—Ç–∞–≤–∫–∞–º–∏', '–¥–æ—Å—Ç–∞–≤–∫–∞—Ö'],
                '—Ü–≤–µ—Ç—ã': ['—Ü–≤–µ—Ç–æ–≤', '—Ü–≤–µ—Ç–∞–º', '—Ü–≤–µ—Ç—ã', '—Ü–≤–µ—Ç–∞–º–∏', '—Ü–≤–µ—Ç–∞—Ö', '—Ü–≤–µ—Ç–æ–∫', '—Ü–≤–µ—Ç–∫–∞', '—Ü–≤–µ—Ç–∫—É', '—Ü–≤–µ—Ç–∫–æ–º', '—Ü–≤–µ—Ç–∫–µ', '—Ü–≤–µ—Ç–∫–∏'],
                '—Ü–≤–µ—Ç–æ–∫': ['—Ü–≤–µ—Ç–∫–∞', '—Ü–≤–µ—Ç–∫—É', '—Ü–≤–µ—Ç–æ–∫', '—Ü–≤–µ—Ç–∫–æ–º', '—Ü–≤–µ—Ç–∫–µ', '—Ü–≤–µ—Ç–∫–∏', '—Ü–≤–µ—Ç–æ–≤', '—Ü–≤–µ—Ç–∞–º', '—Ü–≤–µ—Ç–∞–º–∏', '—Ü–≤–µ—Ç–∞—Ö'],
                '–∫—É–ø–∏—Ç—å': ['–∫—É–ø–∏—Ç—å', '–∫—É–ø–∏–ª', '–∫—É–ø–∏–ª–∞', '–∫—É–ø–∏–ª–∏', '–∫—É–ø–∏–º', '–∫—É–ø–∏—Ç–µ', '–∫—É–ø–∏—à—å', '–∫—É–ø–∏—Ç', '–∫—É–ø—è—Ç', '–∫—É–ø–∏–ª', '–∫—É–ø–∏–ª–∞', '–∫—É–ø–∏–ª–∏'],
                '–∑–∞–∫–∞–∑–∞—Ç—å': ['–∑–∞–∫–∞–∑–∞—Ç—å', '–∑–∞–∫–∞–∑–∞–ª', '–∑–∞–∫–∞–∑–∞–ª–∞', '–∑–∞–∫–∞–∑–∞–ª–∏', '–∑–∞–∫–∞–∂–µ–º', '–∑–∞–∫–∞–∂–µ—Ç–µ', '–∑–∞–∫–∞–∂–µ—Ç', '–∑–∞–∫–∞–∂—É—Ç', '–∑–∞–∫–∞–∑–∞–ª', '–∑–∞–∫–∞–∑–∞–ª–∞', '–∑–∞–∫–∞–∑–∞–ª–∏'],
                '—Ü–µ–Ω–∞': ['—Ü–µ–Ω—ã', '—Ü–µ–Ω—É', '—Ü–µ–Ω–æ–π', '—Ü–µ–Ω–µ', '—Ü–µ–Ω—ã', '—Ü–µ–Ω–∞–º–∏', '—Ü–µ–Ω–∞—Ö'],
                '—Å—Ç–æ–∏–º–æ—Å—Ç—å': ['—Å—Ç–æ–∏–º–æ—Å—Ç–∏', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Å—Ç–æ–∏–º–æ—Å—Ç—å—é', '—Å—Ç–æ–∏–º–æ—Å—Ç–∏', '—Å—Ç–æ–∏–º–æ—Å—Ç—è–º–∏', '—Å—Ç–æ–∏–º–æ—Å—Ç—è—Ö'],
                '–º–∞–≥–∞–∑–∏–Ω': ['–º–∞–≥–∞–∑–∏–Ω–∞', '–º–∞–≥–∞–∑–∏–Ω—É', '–º–∞–≥–∞–∑–∏–Ω', '–º–∞–≥–∞–∑–∏–Ω–æ–º', '–º–∞–≥–∞–∑–∏–Ω–µ', '–º–∞–≥–∞–∑–∏–Ω—ã', '–º–∞–≥–∞–∑–∏–Ω–æ–≤', '–º–∞–≥–∞–∑–∏–Ω–∞–º', '–º–∞–≥–∞–∑–∏–Ω–∞–º–∏', '–º–∞–≥–∞–∑–∏–Ω–∞—Ö'],
                '—Å–∞–π—Ç': ['—Å–∞–π—Ç–∞', '—Å–∞–π—Ç—É', '—Å–∞–π—Ç', '—Å–∞–π—Ç–æ–º', '—Å–∞–π—Ç–µ', '—Å–∞–π—Ç—ã', '—Å–∞–π—Ç–æ–≤', '—Å–∞–π—Ç–∞–º', '—Å–∞–π—Ç–∞–º–∏', '—Å–∞–π—Ç–∞—Ö'],
                '—É—Å–ª—É–≥–∞': ['—É—Å–ª—É–≥–∏', '—É—Å–ª—É–≥—É', '—É—Å–ª—É–≥–æ–π', '—É—Å–ª—É–≥–µ', '—É—Å–ª—É–≥–∏', '—É—Å–ª—É–≥–∞–º–∏', '—É—Å–ª—É–≥–∞—Ö'],
                '—Ç–æ–≤–∞—Ä': ['—Ç–æ–≤–∞—Ä–∞', '—Ç–æ–≤–∞—Ä—É', '—Ç–æ–≤–∞—Ä', '—Ç–æ–≤–∞—Ä–æ–º', '—Ç–æ–≤–∞—Ä–µ', '—Ç–æ–≤–∞—Ä—ã', '—Ç–æ–≤–∞—Ä–æ–≤', '—Ç–æ–≤–∞—Ä–∞–º', '—Ç–æ–≤–∞—Ä–∞–º–∏', '—Ç–æ–≤–∞—Ä–∞—Ö'],
                '–∫–æ–º–ø–∞–Ω–∏—è': ['–∫–æ–º–ø–∞–Ω–∏–∏', '–∫–æ–º–ø–∞–Ω–∏—é', '–∫–æ–º–ø–∞–Ω–∏–µ–π', '–∫–æ–º–ø–∞–Ω–∏–∏', '–∫–æ–º–ø–∞–Ω–∏–∏', '–∫–æ–º–ø–∞–Ω–∏–π', '–∫–æ–º–ø–∞–Ω–∏—è–º', '–∫–æ–º–ø–∞–Ω–∏—è–º–∏', '–∫–æ–º–ø–∞–Ω–∏—è—Ö'],
                '—Ñ–∏—Ä–º–∞': ['—Ñ–∏—Ä–º—ã', '—Ñ–∏—Ä–º—É', '—Ñ–∏—Ä–º–æ–π', '—Ñ–∏—Ä–º–µ', '—Ñ–∏—Ä–º—ã', '—Ñ–∏—Ä–º', '—Ñ–∏—Ä–º–∞–º', '—Ñ–∏—Ä–º–∞–º–∏', '—Ñ–∏—Ä–º–∞—Ö'],
                '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç': ['–∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç–æ–º', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ'],
                '–æ–Ω–ª–∞–π–Ω': ['–æ–Ω–ª–∞–π–Ω', '–æ–Ω–ª–∞–π–Ω–æ–º'],
                '–±—ã—Å—Ç—Ä–æ': ['–±—ã—Å—Ç—Ä–æ', '–±—ã—Å—Ç—Ä–µ–µ', '–±—ã—Å—Ç—Ä–µ–π'],
                '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ': ['–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ', '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ'],
                '–¥–µ—à–µ–≤–æ': ['–¥–µ—à–µ–≤–æ', '–¥–µ—à–µ–≤–ª–µ', '–¥–µ—à–µ–≤–µ–π'],
                '–¥–æ—Ä–æ–≥–æ': ['–¥–æ—Ä–æ–≥–æ', '–¥–æ—Ä–æ–∂–µ'],
                '–º–æ—Å–∫–≤–∞': ['–º–æ—Å–∫–≤—ã', '–º–æ—Å–∫–≤–µ', '–º–æ—Å–∫–≤–æ–π', '–º–æ—Å–∫–≤–µ'],
                '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥': ['—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥–∞', '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥—É', '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥–æ–º', '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥–µ'],
                '—Ä–æ—Å—Å–∏—è': ['—Ä–æ—Å—Å–∏–∏', '—Ä–æ—Å—Å–∏—é', '—Ä–æ—Å—Å–∏–µ–π', '—Ä–æ—Å—Å–∏–∏'],
                '–≥–æ—Ä–æ–¥': ['–≥–æ—Ä–æ–¥–∞', '–≥–æ—Ä–æ–¥—É', '–≥–æ—Ä–æ–¥', '–≥–æ—Ä–æ–¥–æ–º', '–≥–æ—Ä–æ–¥–µ', '–≥–æ—Ä–æ–¥–∞', '–≥–æ—Ä–æ–¥–æ–≤', '–≥–æ—Ä–æ–¥–∞–º', '–≥–æ—Ä–æ–¥–∞–º–∏', '–≥–æ—Ä–æ–¥–∞—Ö'],
                '—Ä–µ–≥–∏–æ–Ω': ['—Ä–µ–≥–∏–æ–Ω–∞', '—Ä–µ–≥–∏–æ–Ω—É', '—Ä–µ–≥–∏–æ–Ω', '—Ä–µ–≥–∏–æ–Ω–æ–º', '—Ä–µ–≥–∏–æ–Ω–µ', '—Ä–µ–≥–∏–æ–Ω—ã', '—Ä–µ–≥–∏–æ–Ω–æ–≤', '—Ä–µ–≥–∏–æ–Ω–∞–º', '—Ä–µ–≥–∏–æ–Ω–∞–º–∏', '—Ä–µ–≥–∏–æ–Ω–∞—Ö'],
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è —Ü–≤–µ—Ç–æ–≤
                '—Ä–æ–∑—ã': ['—Ä–æ–∑', '—Ä–æ–∑–∞–º', '—Ä–æ–∑—ã', '—Ä–æ–∑–∞–º–∏', '—Ä–æ–∑–∞—Ö', '—Ä–æ–∑–∞', '—Ä–æ–∑—ã', '—Ä–æ–∑–µ', '—Ä–æ–∑–æ–π', '—Ä–æ–∑–µ'],
                '—Ç—é–ª—å–ø–∞–Ω—ã': ['—Ç—é–ª—å–ø–∞–Ω–æ–≤', '—Ç—é–ª—å–ø–∞–Ω–∞–º', '—Ç—é–ª—å–ø–∞–Ω—ã', '—Ç—é–ª—å–ø–∞–Ω–∞–º–∏', '—Ç—é–ª—å–ø–∞–Ω–∞—Ö', '—Ç—é–ª—å–ø–∞–Ω', '—Ç—é–ª—å–ø–∞–Ω–∞', '—Ç—é–ª—å–ø–∞–Ω—É', '—Ç—é–ª—å–ø–∞–Ω–æ–º', '—Ç—é–ª—å–ø–∞–Ω–µ'],
                '–ª–∏–ª–∏–∏': ['–ª–∏–ª–∏–π', '–ª–∏–ª–∏—è–º', '–ª–∏–ª–∏–∏', '–ª–∏–ª–∏—è–º–∏', '–ª–∏–ª–∏—è—Ö', '–ª–∏–ª–∏—è', '–ª–∏–ª–∏–∏', '–ª–∏–ª–∏–∏', '–ª–∏–ª–∏–µ–π', '–ª–∏–ª–∏–∏'],
                '–æ—Ä—Ö–∏–¥–µ–∏': ['–æ—Ä—Ö–∏–¥–µ–π', '–æ—Ä—Ö–∏–¥–µ—è–º', '–æ—Ä—Ö–∏–¥–µ–∏', '–æ—Ä—Ö–∏–¥–µ—è–º–∏', '–æ—Ä—Ö–∏–¥–µ—è—Ö', '–æ—Ä—Ö–∏–¥–µ—è', '–æ—Ä—Ö–∏–¥–µ–∏', '–æ—Ä—Ö–∏–¥–µ–µ', '–æ—Ä—Ö–∏–¥–µ–µ–π', '–æ—Ä—Ö–∏–¥–µ–µ'],
                '—Ö—Ä–∏–∑–∞–Ω—Ç–µ–º—ã': ['—Ö—Ä–∏–∑–∞–Ω—Ç–µ–º', '—Ö—Ä–∏–∑–∞–Ω—Ç–µ–º–∞–º', '—Ö—Ä–∏–∑–∞–Ω—Ç–µ–º—ã', '—Ö—Ä–∏–∑–∞–Ω—Ç–µ–º–∞–º–∏', '—Ö—Ä–∏–∑–∞–Ω—Ç–µ–º–∞—Ö', '—Ö—Ä–∏–∑–∞–Ω—Ç–µ–º–∞', '—Ö—Ä–∏–∑–∞–Ω—Ç–µ–º—ã', '—Ö—Ä–∏–∑–∞–Ω—Ç–µ–º–µ', '—Ö—Ä–∏–∑–∞–Ω—Ç–µ–º–æ–π', '—Ö—Ä–∏–∑–∞–Ω—Ç–µ–º–µ']
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏
            if word in special_declensions:
                declensions.extend(special_declensions[word])
            else:
                # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —Å–∫–ª–æ–Ω–µ–Ω–∏—è –¥–ª—è –ª—é–±—ã—Ö —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤
                word_len = len(word)
                
                # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∂–µ–Ω—Å–∫–æ–≥–æ —Ä–æ–¥–∞ –Ω–∞ -–∞
                if word.endswith('–∞') and word_len > 2:
                    base = word[:-1]
                    declensions.extend([base + '—ã', base + '—É', base + '–æ–π', base + '–µ', base + '–∞–º–∏', base + '–∞—Ö'])
                
                # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∂–µ–Ω—Å–∫–æ–≥–æ —Ä–æ–¥–∞ –Ω–∞ -—è
                elif word.endswith('—è') and word_len > 2:
                    base = word[:-1]
                    declensions.extend([base + '–∏', base + '—é', base + '–µ–π', base + '–µ', base + '—è–º–∏', base + '—è—Ö'])
                
                # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∂–µ–Ω—Å–∫–æ–≥–æ —Ä–æ–¥–∞ –Ω–∞ -—å
                elif word.endswith('—å') and word_len > 2:
                    base = word[:-1]
                    declensions.extend([base + '—è', base + '—é', base + '—å—é', base + '–∏', base + '—è–º–∏', base + '—è—Ö'])
                
                # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –º—É–∂—Å–∫–æ–≥–æ —Ä–æ–¥–∞ –Ω–∞ -–π
                elif word.endswith('–π') and word_len > 2:
                    base = word[:-1]
                    declensions.extend([base + '—è', base + '—é', base + '–µ–º', base + '–µ', base + '—è–º–∏', base + '—è—Ö'])
                
                # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–æ–¥–∞ –Ω–∞ -–æ
                elif word.endswith('–æ') and word_len > 2:
                    base = word[:-1]
                    declensions.extend([base + '–∞', base + '—É', base + '–æ–º', base + '–µ', base + '–∞–º–∏', base + '–∞—Ö'])
                
                # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–æ–¥–∞ –Ω–∞ -–µ
                elif word.endswith('–µ') and word_len > 2:
                    base = word[:-1]
                    declensions.extend([base + '—è', base + '—é', base + '–µ–º', base + '–µ', base + '—è–º–∏', base + '—è—Ö'])
                
                # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –º—É–∂—Å–∫–æ–≥–æ —Ä–æ–¥–∞ –Ω–∞ —Å–æ–≥–ª–∞—Å–Ω—É—é
                elif word_len > 2 and not word.endswith(('–∞', '—è', '—å', '–π', '–æ', '–µ')):
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–ª—è –º—É–∂—Å–∫–æ–≥–æ —Ä–æ–¥–∞
                    declensions.extend([word + '–∞', word + '—É', word + '–æ–º', word + '–µ', word + '—ã', word + '–æ–≤', word + '–∞–º', word + '–∞–º–∏', word + '–∞—Ö'])
                
                # –ì–ª–∞–≥–æ–ª—ã (–∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤ –Ω–∞ -—Ç—å)
                elif word.endswith('—Ç—å') and word_len > 3:
                    base = word[:-2]  # —É–±–∏—Ä–∞–µ–º '—Ç—å'
                    declensions.extend([
                        base + '–ª', base + '–ª–∞', base + '–ª–∏',  # –ø—Ä–æ—à–µ–¥—à–µ–µ –≤—Ä–µ–º—è
                        base + '—é', base + '–µ—à—å', base + '–µ—Ç',  # –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è
                        base + '–µ–º', base + '–µ—Ç–µ', base + '—é—Ç',
                        base + '–π', base + '–π—Ç–µ'  # –ø–æ–≤–µ–ª–∏—Ç–µ–ª—å–Ω–æ–µ –Ω–∞–∫–ª–æ–Ω–µ–Ω–∏–µ
                    ])
                
                # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞ -—ã–π, -–æ–π, -–∏–π
                elif word.endswith(('—ã–π', '–æ–π', '–∏–π')) and word_len > 3:
                    base = word[:-2]
                    declensions.extend([
                        base + '–æ–≥–æ', base + '–æ–º—É', base + '—ã–º', base + '–æ–º',  # –º—É–∂—Å–∫–æ–π —Ä–æ–¥
                        base + '–∞—è', base + '–æ–π', base + '—É—é', base + '–æ–π',    # –∂–µ–Ω—Å–∫–∏–π —Ä–æ–¥
                        base + '–æ–µ', base + '–æ–≥–æ', base + '–æ–º—É', base + '—ã–º',  # —Å—Ä–µ–¥–Ω–∏–π —Ä–æ–¥
                        base + '—ã–µ', base + '—ã—Ö', base + '—ã–º', base + '—ã–º–∏'    # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ
                    ])
                
                # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞ -–∞—è, -—è—è
                elif word.endswith(('–∞—è', '—è—è')) and word_len > 3:
                    base = word[:-2]
                    declensions.extend([
                        base + '–æ–π', base + '—É—é', base + '–æ–π',  # –∂–µ–Ω—Å–∫–∏–π —Ä–æ–¥
                        base + '–æ–µ', base + '–æ–≥–æ', base + '–æ–º—É', base + '—ã–º',  # —Å—Ä–µ–¥–Ω–∏–π —Ä–æ–¥
                        base + '—ã–µ', base + '—ã—Ö', base + '—ã–º', base + '—ã–º–∏'    # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ
                    ])
                
                # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞ -–æ–µ, -–µ–µ
                elif word.endswith(('–æ–µ', '–µ–µ')) and word_len > 3:
                    base = word[:-2]
                    declensions.extend([
                        base + '–æ–≥–æ', base + '–æ–º—É', base + '—ã–º', base + '–æ–º',  # –º—É–∂—Å–∫–æ–π —Ä–æ–¥
                        base + '–∞—è', base + '–æ–π', base + '—É—é', base + '–æ–π',    # –∂–µ–Ω—Å–∫–∏–π —Ä–æ–¥
                        base + '—ã–µ', base + '—ã—Ö', base + '—ã–º', base + '—ã–º–∏'    # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ
                    ])
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è
            return list(set(declensions))

        # –ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π
        target_analysis = {}
        if target_keywords:
            target_list = [kw.strip().lower() for kw in target_keywords.split(',')]
            
            for keyword_phrase in target_list:
                # –†–∞–∑–±–∏–≤–∞–µ–º —Ñ—Ä–∞–∑—É –Ω–∞ —Å–ª–æ–≤–∞
                words_in_phrase = keyword_phrase.split()
                
                if len(words_in_phrase) == 1:
                    # –û–¥–Ω–æ —Å–ª–æ–≤–æ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∫–ª–æ–Ω–µ–Ω–∏—è
                    word = words_in_phrase[0]
                    declensions = generate_declensions(word)
                    
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ —Å–∫–ª–æ–Ω–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ)
                    total_count = 0
                    declension_counts = {}
                    
                    for declension in declensions:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ —Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤
                        pattern = r'\b' + re.escape(declension) + r'\b'
                        matches = re.findall(pattern, text, re.IGNORECASE)
                        
                        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
                        count = len(matches)
                        
                        if count > 0:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–¥–æ—Å—Ç–∞–≤–∫–∏" –∏ "–¥–æ—Å—Ç–∞–≤–∫–∏" - —ç—Ç–æ –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ)
                            is_duplicate = False
                            for existing_declension in declension_counts:
                                if declension.lower() == existing_declension.lower():
                                    is_duplicate = True
                                    break
                            
                            if not is_duplicate:
                                declension_counts[declension] = count
                                total_count += count
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
                    density = (total_count / total_words) * 100 if total_words > 0 else 0
                    
                    target_analysis[keyword_phrase] = {
                        "freq": total_count,
                        "density": density,
                        "declensions_found": declension_counts
                    }
                
                else:
                    # –§—Ä–∞–∑–∞ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–≤ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∫–ª–æ–Ω–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
                    word_declensions = []
                    for word in words_in_phrase:
                        declensions = generate_declensions(word)
                        word_declensions.append(declensions)
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ñ—Ä–∞–∑
                    phrase_variations = []
                    
                    def generate_combinations(current_phrase, word_index):
                        if word_index == len(words_in_phrase):
                            phrase_variations.append(current_phrase)
                            return
                        
                        for declension in word_declensions[word_index]:
                            new_phrase = current_phrase + " " + declension if current_phrase else declension
                            generate_combinations(new_phrase, word_index + 1)
                    
                    generate_combinations("", 0)
                    
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∫–∞–∂–¥–æ–π –≤–∞—Ä–∏–∞—Ü–∏–∏ —Ñ—Ä–∞–∑—ã
                    total_count = 0
                    phrase_counts = {}
                    
                    for phrase_variation in phrase_variations:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ —Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤ –¥–ª—è —Ñ—Ä–∞–∑
                        pattern = r'\b' + re.escape(phrase_variation) + r'\b'
                        matches = re.findall(pattern, text, re.IGNORECASE)
                        
                        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
                        count = len(matches)
                        
                        if count > 0:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç
                            is_duplicate = False
                            for existing_phrase in phrase_counts:
                                if phrase_variation.lower() == existing_phrase.lower():
                                    is_duplicate = True
                                    break
                            
                            if not is_duplicate:
                                phrase_counts[phrase_variation] = count
                                total_count += count
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
                    density = (total_count / total_words) * 100 if total_words > 0 else 0
                    
                    target_analysis[keyword_phrase] = {
                        "freq": total_count,
                        "density": density,
                        "declensions_found": phrase_counts
                    }

        return [{"word": word, "freq": freq} for word, freq in keywords], keyword_density, target_analysis
    except Exception as e:
        return [], {}, f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {str(e)}"

def check_open_graph(driver):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–≥–∏ Open Graph."""
    og_tags = {
        "og:title": "–ù–µ –Ω–∞–π–¥–µ–Ω",
        "og:description": "–ù–µ –Ω–∞–π–¥–µ–Ω",
        "og:image": "–ù–µ –Ω–∞–π–¥–µ–Ω",
        "og:url": "–ù–µ –Ω–∞–π–¥–µ–Ω",
        "og:type": "–ù–µ –Ω–∞–π–¥–µ–Ω"
    }
    for meta in driver.find_elements(By.TAG_NAME, "meta"):
        property_attr = meta.get_attribute("property")
        if property_attr and property_attr.startswith("og:"):
            content = meta.get_attribute("content") or "–ë–µ–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"
            og_tags[property_attr] = content
    return og_tags

def check_schema_markup(driver):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏ Schema.org."""
    schema_tags = driver.find_elements(By.XPATH, "//*[@itemscope]")
    if not schema_tags:
        return False, "–ú–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∞ Schema.org –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    valid_types = ["Article", "Product", "Organization", "Person", "WebPage"]
    for tag in schema_tags:
        itemtype = tag.get_attribute("itemtype")
        if itemtype and any(valid_type in itemtype for valid_type in valid_types):
            return True, f"–ù–∞–π–¥–µ–Ω–∞ –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∞: {itemtype}"
    return False, "–ú–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –Ω–æ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º—ã–º —Ç–∏–ø–∞–º"

def check_noindex_nofollow_noarchive(driver):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ meta —Ç–µ–≥–æ–≤ noindex, nofollow, noarchive –∏ —Ç–µ–≥–∞ <noindex>."""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ meta robots
    meta_robots = driver.find_elements(By.XPATH, "//meta[@name='robots']")
    noindex_meta = False
    nofollow_meta = False
    noarchive_meta = False
    robots_content = "–ù–µ –Ω–∞–π–¥–µ–Ω"
    
    if meta_robots:
        content = meta_robots[0].get_attribute("content").lower()
        noindex_meta = "noindex" in content
        nofollow_meta = "nofollow" in content
        noarchive_meta = "noarchive" in content
        robots_content = content
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–≥–∞ <noindex>
    noindex_tag = driver.find_elements(By.XPATH, "//noindex")
    has_noindex_tag = len(noindex_tag) > 0
    
    return noindex_meta, nofollow_meta, noarchive_meta, has_noindex_tag, robots_content

def check_hidden_blocks(driver):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å display: none."""
    hidden_elements = driver.execute_script("""
        let elements = document.querySelectorAll('[style*="display: none"]');
        let results = [];
        elements.forEach(el => {
            if (el.textContent.trim().length > 0) {
                results.push(el.tagName + ': ' + el.textContent.trim().substring(0, 50));
            }
        });
        return results;
    """)
    return hidden_elements

def check_canonical(driver):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç canonical —Ç–µ–≥."""
    canonical = driver.find_elements(By.XPATH, "//link[@rel='canonical']")
    if canonical:
        href = canonical[0].get_attribute("href")
        return True, href
    return False, "–ù–µ –Ω–∞–π–¥–µ–Ω"

def check_pagination_links(driver):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç rel=next/prev."""
    next_link = driver.find_elements(By.XPATH, "//link[@rel='next']")
    prev_link = driver.find_elements(By.XPATH, "//link[@rel='prev']")
    return bool(next_link), bool(prev_link)

def check_external_links(driver, site_url):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ nofollow –∏ broken."""
    domain = re.sub(r'^https?://(www\.)?', '', site_url).rstrip('/')
    links = driver.find_elements(By.TAG_NAME, "a")
    external_links = [link for link in links if link.get_attribute("href") and domain not in link.get_attribute("href")]
    nofollow_count = sum(1 for link in external_links if "nofollow" in link.get_attribute("rel").lower())
    broken_externals = []
    for link in external_links:
        href = link.get_attribute("href")
        if href and "javascript:void" not in href:
            _, status, _, _ = check_resource(href, True)
            if not isinstance(status, int) or status != 200:
                broken_externals.append(href)
    return len(external_links), nofollow_count, broken_externals

def check_mirrors_and_redirects(site_url, ignore_ssl):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–µ—Ä–∫–∞–ª–∞ –∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã (www/non-www, http/https)."""
    variants = [
        site_url.replace("https://", "http://"),
        site_url.replace("https://www.", "https://") if "www." in site_url else site_url.replace("https://", "https://www."),
        site_url.replace("https://", "http://www.") if not "www." in site_url else site_url.replace("https://www.", "http://www."),
    ]
    issues = []
    for var in set(variants):  # –£–±—Ä–∞—Ç—å –¥—É–±–ª–∏
        if var == site_url:
            continue
        _, status, _, history = check_resource(var, ignore_ssl)
        if not history or history[0].status_code != 301 or status != 200 or history[-1].url != site_url:
            issues.append(f"–ü—Ä–æ–±–ª–µ–º–∞ —Å –∑–µ—Ä–∫–∞–ª–æ–º {var}: —Å—Ç–∞—Ç—É—Å {status}, —Ñ–∏–Ω–∞–ª—å–Ω—ã–π URL {history[-1].url if history else '–Ω–µ—Ç —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞'}")
    return issues

def check_redirect_chain(site_url, ignore_ssl):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ø–æ—á–∫–∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤."""
    _, _, _, history = check_resource(site_url, ignore_ssl)
    if len(history) > 3:
        return True, len(history)
    return False, 0

def check_duplicates(driver):
    """–ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (canonical –Ω–∞ —Å–µ–±—è)."""
    has_canonical, href = check_canonical(driver)
    if has_canonical and href == driver.current_url:
        return False, "Canonical —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–µ–±—è"
    elif has_canonical:
        return True, f"Canonical —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –¥—Ä—É–≥–æ–π URL: {href}"
    return True, "Canonical –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"

def check_ads(driver):
    """–ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∫–ª–∞–º—ã (–∫–æ–ª-–≤–æ iframes/—Å–∫—Ä–∏–ø—Ç–æ–≤ –æ—Ç ad —Å–µ—Ç–µ–π)."""
    ad_iframes = len(driver.find_elements(By.XPATH, "//iframe[contains(@src, 'googleads') or contains(@src, 'doubleclick')]"))
    ad_scripts = len(driver.find_elements(By.XPATH, "//script[contains(@src, 'ads') or contains(@src, 'doubleclick')]"))
    total_ads = ad_iframes + ad_scripts
    if total_ads > 5:
        return True, total_ads
    return False, total_ads

def check_security(driver, site_url):
    """–ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (HTTPS, mixed content)."""
    issues = []
    if not site_url.startswith("https"):
        issues.append("–°–∞–π—Ç –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç HTTPS")
    html = driver.page_source
    mixed = re.findall(r'src="http://', html)
    if mixed:
        issues.append(f"–ù–∞–π–¥–µ–Ω–æ {len(mixed)} mixed content (http –Ω–∞ https —Å—Ç—Ä–∞–Ω–∏—Ü–µ)")
    return issues

def get_background_images(html):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç background-image –∏–∑ CSS –≤ HTML."""
    soup = BeautifulSoup(html, 'html.parser')
    bg_images = []
    for element in soup.find_all(style=True):
        style = element['style']
        match = re.search(r'background-image:\s*url\(["\']?(.*?)["\']?\)', style)
        if match:
            bg_images.append(match.group(1))
    return bg_images

def generate_performance_chart(load_times, resource_times, js_css_times):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∑–∫–∏."""
    plt.figure(figsize=(8, 5))
    resolutions = ["1920x1080", "768x1024", "375x667"]
    bar_width = 0.25
    index = range(len(resolutions))

    if len(load_times) != len(resource_times) or len(load_times) != len(js_css_times):
        raise ValueError("–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö load_times, resource_times –∏ js_css_times –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å")

    plt.bar(index, load_times, bar_width, label="–û–±—â–∞—è –∑–∞–≥—Ä—É–∑–∫–∞", color='b')
    plt.bar([i + bar_width for i in index], resource_times, bar_width, label="–†–µ—Å—É—Ä—Å—ã", color='g')
    plt.bar([i + 2 * bar_width for i in index], js_css_times, bar_width, label="JS/CSS", color='r')

    plt.xlabel("–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ")
    plt.ylabel("–í—Ä–µ–º—è (—Å–µ–∫)")
    plt.title("–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏")
    plt.xticks([i + bar_width for i in index], resolutions)
    plt.legend()
    buffer = BytesIO()
    plt.savefig(buffer, format="png")
    plt.close()
    buffer.seek(0)
    return base64.b64encode(buffer.getvalue()).decode()

def analyze_performance(site_url, ignore_ssl):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∞–π—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π."""
    performance_data = {
        "load_times": [],
        "resource_times": [],
        "js_css_times": []
    }
    resolutions = [(1920, 1080), (768, 1024), (375, 667)]
    for width, height in resolutions:
        try:
            driver_local = create_webdriver(ignore_ssl=ignore_ssl, window_size=f"{width},{height}")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è WebDriver –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {str(e)}")
            performance_data["load_times"].append(0)
            performance_data["resource_times"].append(0)
            performance_data["js_css_times"].append(0)
            continue
        try:
            start_time = time.time()
            driver_local.get(site_url)
            load_time = time.time() - start_time
            performance_data["load_times"].append(load_time)

            # –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
            resources = driver_local.execute_script("""
                return window.performance.getEntriesByType("resource");
            """)
            resource_time = sum(entry['duration'] / 1000 for entry in resources) / len(resources) if resources else 0
            performance_data["resource_times"].append(resource_time)

            # –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JS/CSS
            js_css_time = driver_local.execute_script("""
                let times = 0;
                performance.getEntriesByType("resource").forEach(entry => {
                    if (entry.name.endsWith('.js') || entry.name.endsWith('.css')) {
                        times += entry.duration / 1000;
                    }
                });
                return times;
            """)
            performance_data["js_css_times"].append(js_css_time)
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è {width}x{height}: {str(e)}")
            performance_data["load_times"].append(0)
            performance_data["resource_times"].append(0)
            performance_data["js_css_times"].append(0)
        finally:
            try:
                if 'driver_local' in locals() and driver_local:
                    driver_local.quit()
            except Exception as e:
                log_to_file(f"–û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è WebDriver –≤ analyze_performance: {str(e)}")
    return performance_data

def format_summary_section(positives, errors, recommendations, title):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–¥–µ–ª —Å–≤–æ–¥–∫–∏ –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥."""
    output = f"### {title}\n\n"
    if positives:
        output += "**–•–æ—Ä–æ—à–µ–µ:**\n" + "\n".join(f"‚úÖ {p}" for p in positives) + "\n\n"
    if errors:
        output += "**–ü—Ä–æ–±–ª–µ–º—ã:**\n" + "\n".join(f"‚ùå {e}" for e in errors) + "\n\n"
    if recommendations:
        output += "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n" + "\n".join(f"üìù {r}" for r in recommendations) + "\n\n"
    return output

def format_links_section(link_statuses):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ —Å —Å—Ç–∞—Ç—É—Å–∞–º–∏ –∏ –∫—Ä–∞—Å–∏–≤—ã–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏."""
    output = f"### –°—Å—ã–ª–∫–∏ ({len(link_statuses)} –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö)\n\n"
    for url, status in link_statuses.items():
        if isinstance(status, int):
            if status == 200:
                status_icon = "üü¢"
                status_text = f"200 OK"
            elif status == 301:
                status_icon = "üü°"
                status_text = f"301 Moved Permanently"
            elif status == 302:
                status_icon = "üü°"
                status_text = f"302 Found"
            elif status == 404:
                status_icon = "üî¥"
                status_text = f"404 Not Found"
            elif status == 500:
                status_icon = "üî¥"
                status_text = f"500 Internal Server Error"
            elif 300 <= status < 400:
                status_icon = "üü°"
                status_text = f"{status} Redirect"
            elif 400 <= status < 500:
                status_icon = "üî¥"
                status_text = f"{status} Client Error"
            elif 500 <= status < 600:
                status_icon = "üî¥"
                status_text = f"{status} Server Error"
            else:
                status_icon = "‚ö™"
                status_text = f"{status} Unknown"
        else:
            status_icon = "üî¥"
            status_text = f"Error: {status}"
        
        output += f"{status_icon} **{status_text}** - {url}\n"
    return output

def check_links_summary(link_statuses):
    """–°–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–∫—É —Å—Å—ã–ª–æ–∫ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º —Å –∫—Ä–∞—Å–∏–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    categories = {
        "üü¢ 200 OK": [],
        "üü° 301 Moved Permanently": [],
        "üü° 302 Found": [],
        "üü° Other Redirects (3xx)": [],
        "üî¥ 404 Not Found": [],
        "üî¥ 500 Internal Server Error": [],
        "üî¥ Other Client Errors (4xx)": [],
        "üî¥ Other Server Errors (5xx)": [],
        "‚ö™ Other/Errors": []
    }
    
    for url, status in link_statuses.items():
        if isinstance(status, int):
            if status == 200:
                categories["üü¢ 200 OK"].append(url)
            elif status == 301:
                categories["üü° 301 Moved Permanently"].append(url)
            elif status == 302:
                categories["üü° 302 Found"].append(url)
            elif 300 <= status < 400:
                categories["üü° Other Redirects (3xx)"].append(url)
            elif status == 404:
                categories["üî¥ 404 Not Found"].append(url)
            elif status == 500:
                categories["üî¥ 500 Internal Server Error"].append(url)
            elif 400 <= status < 500:
                categories["üî¥ Other Client Errors (4xx)"].append(url)
            elif 500 <= status < 600:
                categories["üî¥ Other Server Errors (5xx)"].append(url)
            else:
                categories["‚ö™ Other/Errors"].append(url)
        else:
            categories["‚ö™ Other/Errors"].append(url)

    summary = f"### üìä –°–≤–æ–¥–∫–∞ –°—Å—ã–ª–æ–∫ ({len(link_statuses)} –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö)\n\n"
    for cat, urls in categories.items():
        if urls:
            summary += f"**{cat}** ({len(urls)} —Å—Å—ã–ª–æ–∫):\n"
            for url in urls:
                summary += f"  ‚Ä¢ {url}\n"
            summary += "\n"
    
    return summary
    return summary

def run_test(site_url: str, summary_area: ft.TextField, page: ft.Page, progress_bar: ft.ProgressBar, ignore_ssl: bool, target_keywords: str):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∞–π—Ç–∞."""
    if not re.match(r'^https?://', site_url):
        summary_area.value = "‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π URL\n"
        page.update()
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–±—ã—Ç–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
    stop_event = page.data.get('stop_event')
    if stop_event and stop_event.is_set():
        summary_area.value = "‚èπ –¢–µ—Å—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"
        page.update()
        return

    driver = None
    general_positives = []
    general_errors = []
    general_recs = []
    seo_positives = []
    seo_errors = []
    seo_recs = []
    perf_positives = []
    perf_errors = []
    perf_recs = []
    broken_links = []
    link_statuses = {}  # –î–ª—è –≤–∫–ª–∞–¥–∫–∏ –°—Å—ã–ª–∫–∏
    sitemap_errors = []  # –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ sitemap –≤ —Å–≤–æ–¥–∫–µ
    urls_in_sitemap = []
    site_links = []
    total_checks = 32  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
    current_check = 0
    log_text = ""  # –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –ª–æ–≥–æ–≤

    # --- –ù–æ–≤—ã–π –±–ª–æ–∫: –ü–æ–ª—É—á–∞–µ–º sitemap-—ã –∏–∑ robots.txt –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ ---
    sitemap_urls = []
    try:
        robots_url = site_url.rstrip('/') + '/robots.txt'
        r = requests.get(robots_url, timeout=10, verify=not ignore_ssl)
        if r.status_code == 200:
            for line in r.text.splitlines():
                if line.strip().lower().startswith('sitemap:'):
                    sitemap_url = line.split(':', 1)[1].strip()
                    if sitemap_url:
                        sitemap_urls.append(sitemap_url)
    except Exception as ex:
        pass  # robots.txt –º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å ‚Äî —ç—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
    if not sitemap_urls:
        sitemap_urls = [site_url.rstrip('/') + '/sitemap.xml']
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –∏–∑ –≤—Å–µ—Ö sitemap —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π sitemap index
    all_sitemap_links = []
    sitemap_sources = {}
    sitemap_errors = []
    
    for sitemap_url in sitemap_urls:
        try:
            result = process_sitemap_recursively(sitemap_url, ignore_ssl)
            all_sitemap_links.extend(result['urls'])
            sitemap_sources.update(result['sources'])
            sitemap_errors.extend(result['errors'])
        except Exception as ex:
            sitemap_errors.append(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {sitemap_url}: {str(ex)}")
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    all_sitemap_links = list(set([link for link in all_sitemap_links if link]))
    urls_in_sitemap = all_sitemap_links
    
    if sitemap_errors:
        log_text += f"‚ö†Ô∏è –û—à–∏–±–∫–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ sitemap: {', '.join(sitemap_errors[:3])}{'...' if len(sitemap_errors) > 3 else ''}\n"
    # --- –ö–æ–Ω–µ—Ü –Ω–æ–≤–æ–≥–æ –±–ª–æ–∫–∞ ---

    def update_progress():
        nonlocal current_check
        current_check += 1
        progress_bar.value = min(current_check / total_checks, 1.0)
        page.update()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–±—ã—Ç–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        if stop_event and stop_event.is_set():
            return False  # –°–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        return True  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º

    try:
        log_text += f"\n=== üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∞–π—Ç–∞: {site_url} ===\n"
        general_positives.append(f"üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∞–π—Ç–∞: {site_url} ({datetime.now().strftime('%Y-%m-%d %H:%M:%S %Z')})")
        if not update_progress():
            return

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–∞ —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        log_text += "\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–∞ —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫...\n"
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π –º–µ—Ç–æ–¥
        anti_bot_mode = False
        try:
            start_time = time.time()
            r = requests.get(site_url, timeout=10, verify=not ignore_ssl, allow_redirects=True)
            load_time = time.time() - start_time
            page_size = len(r.content) / 1024
            log_text += f"üîé HTTP —Å—Ç–∞—Ç—É—Å: {r.status_code}\n"
            log_text += f"‚è± –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ (HTTP): {load_time:.2f} —Å–µ–∫\n"
            log_text += f"üìè –†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_size:.2f} –ö–ë\n"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            if any(blocked_text in r.text.lower() for blocked_text in [
                'access denied', '–¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω', 'blocked', '–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω',
                'cloudflare', 'captcha', 'recaptcha', 'bot', 'robot'
            ]):
                log_text += "‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞, –ø—Ä–æ–±—É–µ–º –æ–±—Ö–æ–¥...\n"
                general_errors.append("–°–∞–π—Ç –±–ª–æ–∫–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø (–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ –æ—Ç –±–æ—Ç–æ–≤)")
                anti_bot_mode = True
            else:
                general_positives.append(f"–°–∞–π—Ç –¥–æ—Å—Ç—É–ø–µ–Ω (HTTP —Å—Ç–∞—Ç—É—Å: {r.status_code}, –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {load_time:.2f} —Å–µ–∫)")
                general_positives.append(f"–†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_size:.2f} –ö–ë")
                anti_bot_mode = False
                
            if page_size > 2000:
                general_errors.append("–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–ª–∏—à–∫–æ–º —Ç—è–∂—ë–ª–∞—è")
                general_recs.append("–°–æ–∂–º–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–π—Ç–µ CSS/JS, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ lazy loading.")
            if load_time > 3:
                general_errors.append("–í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ")
                general_recs.append("–í–∫–ª—é—á–∏—Ç–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ CDN, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Å–µ—Ä–≤–µ—Ä.")
            if r.status_code != 200:
                seo_errors.append("–ü—Ä–æ–±–ª–µ–º–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: —Å—Ç–∞—Ç—É—Å –Ω–µ 200")
                seo_recs.append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ —Å—Ç–∞—Ç—É—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
                
        except Exception as e:
            log_text += f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}\n"
            log_text += "üîÑ –ü—Ä–æ–±—É–µ–º –æ–±—Ö–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫...\n"
            anti_bot_mode = True
            general_errors.append(f"–°–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            seo_errors.append("–ü—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        
        update_progress()

        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥—Ä–∞–π–≤–µ—Ä–∞ –¥–ª—è SEO –ø—Ä–æ–≤–µ—Ä–æ–∫ —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        try:
            if anti_bot_mode:
                log_text += "üõ°Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ WebDriver –≤ —Ä–µ–∂–∏–º–µ –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫...\n"
                driver = create_webdriver(ignore_ssl=ignore_ssl, anti_bot_mode=True)
            else:
                driver = create_webdriver(ignore_ssl=ignore_ssl)
            
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É
            try:
                driver.get(site_url)
                log_text += "‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n"
            except Exception as e:
                log_text += f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}\n"
                if not anti_bot_mode:
                    log_text += "üîÑ –ü—Ä–æ–±—É–µ–º –≤ —Ä–µ–∂–∏–º–µ –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫...\n"
                    driver.quit()
                    driver = create_webdriver(ignore_ssl=ignore_ssl, anti_bot_mode=True)
                    driver.get(site_url)
                    log_text += "‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ —Ä–µ–∂–∏–º–µ –æ–±—Ö–æ–¥–∞\n"
                
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è WebDriver: {str(e)}")
            summary_area.value = f"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å WebDriver: {str(e)}"
            page.update()
            return

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ ChromeDriver
        driver_version = driver.capabilities['chrome']['chromedriverVersion'].split(' ')[0]
        log_text += f"üìå –í–µ—Ä—Å–∏—è ChromeDriver: {driver_version}\n"
        general_positives.append(f"–í–µ—Ä—Å–∏—è ChromeDriver: {driver_version}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–µ—Ä–∫–∞–ª –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ URL
        log_text += "\nüîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–µ—Ä–∫–∞–ª –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ URL\n"
        mirror_issues = check_mirrors_and_redirects(site_url, ignore_ssl)
        if mirror_issues:
            seo_errors.extend(mirror_issues)
            seo_recs.append("–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ 301 —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã –¥–ª—è –∑–µ—Ä–∫–∞–ª (www/non-www, http/https).")
        else:
            seo_positives.append("–ó–µ—Ä–∫–∞–ª–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ø–æ—á–µ–∫ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
        log_text += "\nüîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ø–æ—á–µ–∫ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤\n"
        has_long_chain, chain_len = check_redirect_chain(site_url, ignore_ssl)
        if has_long_chain:
            seo_errors.append(f"–î–ª–∏–Ω–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ ({chain_len} —à–∞–≥–æ–≤)")
            seo_recs.append("–°–æ–∫—Ä–∞—Ç–∏—Ç–µ —Ü–µ–ø–æ—á–∫—É —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –¥–æ 1-2 —à–∞–≥–æ–≤.")
        else:
            seo_positives.append("–¶–µ–ø–æ—á–∫–∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –≤ –Ω–æ—Ä–º–µ")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ SEO-—Ñ–∞–π–ª–æ–≤
        log_text += "\nü§ñ –ü—Ä–æ–≤–µ—Ä–∫–∞ SEO-—Ñ–∞–π–ª–æ–≤\n"
        seo_files = check_seo_files(site_url, ignore_ssl)
        for file, status, content in seo_files:
            log_text += f"üìã –ü—Ä–æ–≤–µ—Ä–∫–∞ {file}\n"
            if status:
                log_text += f"‚úÖ {file} –¥–æ—Å—Ç—É–ø–µ–Ω\n"
                seo_positives.append(f"{file} –Ω–∞–π–¥–µ–Ω")
                if file == "robots.txt":
                    errors, positives, found_directives, recommendations = analyze_robots_txt(content)
                    log_text += f"üìã –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–∏–≤—ã: {', '.join(found_directives) if found_directives else '–ù–µ—Ç –¥–∏—Ä–µ–∫—Ç–∏–≤'}\n"
                    seo_positives.extend(positives)
                    seo_errors.extend(errors)
                    seo_recs.extend(recommendations)
                else:
                    errors, positives, recommendations, _, page_details, sitemap_info, pages_not_in_sitemap, pages_in_sitemap_not_on_site = validate_sitemap(content, site_url, ignore_ssl)
                    seo_positives.extend(positives)
                    seo_errors.extend(errors)
                    sitemap_errors = errors  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–≤–æ–¥–∫–∏
                    seo_recs.extend(recommendations)
                    log_text += f"üìã –°–æ–¥–µ—Ä–∂–∏–º–æ–µ sitemap.xml: {content[:200]}...\n"
                    seo_positives.append("sitemap.xml —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã–µ")
            else:
                log_text += f"‚ùå {file} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {content}\n"
                seo_errors.append(f"{file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                seo_recs.append(f"–°–æ–∑–¥–∞–π—Ç–µ {file}.")
            update_progress()

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        log_text += "\n‚è± –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n"
        performance_data = analyze_performance(site_url, ignore_ssl)
        load_times = performance_data["load_times"]
        resource_times = performance_data["resource_times"]
        js_css_times = performance_data["js_css_times"]

        log_text += f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {', '.join(f'{t:.2f} —Å–µ–∫' for t in load_times)}\n"
        log_text += f"‚è± –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤: {', '.join(f'{t:.2f} —Å–µ–∫' for t in resource_times)}\n"
        log_text += f"‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JS/CSS: {', '.join(f'{t:.2f} —Å–µ–∫' for t in js_css_times)}\n"
        perf_positives.append(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {', '.join(f'{t:.2f} —Å–µ–∫' for t in load_times)}")
        perf_positives.append(f"–í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤: {', '.join(f'{t:.2f} —Å–µ–∫' for t in resource_times)}")
        perf_positives.append(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JS/CSS: {', '.join(f'{t:.2f} —Å–µ–∫' for t in js_css_times)}")
        for i, (lt, rt, jct) in enumerate(zip(load_times, resource_times, js_css_times)):
            res = ['1920x1080', '768x1024', '375x667'][i]
            if lt > 3:
                perf_errors.append(f"–°–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ {res}: {lt:.2f} —Å–µ–∫")
                perf_recs.append("–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Å–µ—Ä–≤–µ—Ä –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ.")
            else:
                perf_positives.append(f"–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ {res}: {lt:.2f} —Å–µ–∫")
            if rt > 1:
                perf_errors.append(f"–î–æ–ª–≥–æ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–∞ {res}: {rt:.2f} —Å–µ–∫")
                perf_recs.append("–°–æ–∂–º–∏—Ç–µ —Ä–µ—Å—É—Ä—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ CDN.")
            if jct > 1:
                perf_errors.append(f"–î–æ–ª–≥–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JS/CSS –Ω–∞ {res}: {jct:.2f} —Å–µ–∫")
                perf_recs.append("–ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–π—Ç–µ —Å–∫—Ä–∏–ø—Ç—ã.")
        update_progress()

        # Core Web Vitals
        log_text += "\n‚ö° Core Web Vitals\n"
        try:
            lcp, fid, cls = get_core_web_vitals(driver)
            core_vitals_summary = f"LCP: {lcp:.2f} –º—Å | FID: {fid:.2f} –º—Å | CLS: {cls:.3f}\n"
            if lcp > 2500:
                perf_errors.append(f"LCP —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {lcp:.0f} –º—Å")
                perf_recs.append("–°–æ–∫—Ä–∞—Ç–∏—Ç–µ –≤—Ä–µ–º—è Largest Contentful Paint –¥–æ <2.5 —Å–µ–∫.")
            if cls > 0.1:
                perf_errors.append(f"CLS —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {cls:.3f}")
                perf_recs.append("–°–Ω–∏–∑—å—Ç–µ Cumulative Layout Shift –¥–æ <0.1.")
            log_text += core_vitals_summary
            perf_positives.append(core_vitals_summary)
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è Core Web Vitals: {str(e)}")
            perf_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å Core Web Vitals")
        update_progress()

        # –ú–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∞
        log_text += "\nüîé –ú–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∞ (Schema.org, JSON-LD, OpenGraph, Twitter)\n"
        try:
            schema_items, jsonld_blocks, og_tags, twitter_tags = get_microdata(driver)
            if schema_items:
                seo_positives.append(f"Schema.org items: {', '.join(schema_items)}")
            else:
                seo_errors.append("Schema.org –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ Schema.org –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫—É.")
            if jsonld_blocks:
                seo_positives.append(f"JSON-LD –±–ª–æ–∫–æ–≤: {len(jsonld_blocks)}")
            else:
                seo_errors.append("JSON-LD –Ω–µ –Ω–∞–π–¥–µ–Ω")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ JSON-LD –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —Å–∞–π—Ç–∞.")
            if og_tags:
                seo_positives.append(f"OpenGraph: {', '.join(og_tags.keys())}")
            else:
                seo_errors.append("OpenGraph –Ω–µ –Ω–∞–π–¥–µ–Ω")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ OpenGraph –¥–ª—è —Å–æ—Ü—Å–µ—Ç–µ–π.")
            if twitter_tags:
                seo_positives.append(f"Twitter Cards: {', '.join(twitter_tags.keys())}")
            else:
                seo_errors.append("Twitter Cards –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ Twitter Cards –¥–ª—è —Å–æ—Ü—Å–µ—Ç–µ–π.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫—É")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ SEO-—ç–ª–µ–º–µ–Ω—Ç–æ–≤
        try:
            title_tag = driver.title if driver.title else "–ù–µ –Ω–∞–π–¥–µ–Ω"
            log_text += f"üìù –¢–µ–≥ title: {title_tag} (–î–ª–∏–Ω–∞: {len(title_tag)})\n"
            seo_positives.append(f"–¢–µ–≥ title: {title_tag} (–î–ª–∏–Ω–∞: {len(title_tag)})")
            if len(title_tag) > 60:
                seo_errors.append("–î–ª–∏–Ω–∞ title –ø—Ä–µ–≤—ã—à–∞–µ—Ç 60 —Å–∏–º–≤–æ–ª–æ–≤")
                seo_recs.append("–°–æ–∫—Ä–∞—Ç–∏—Ç–µ title –¥–æ 60 —Å–∏–º–≤–æ–ª–æ–≤.")
            elif title_tag == "–ù–µ –Ω–∞–π–¥–µ–Ω":
                seo_errors.append("–¢–µ–≥ title –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ —Ç–µ–≥ title.")
            else:
                seo_positives.append("–¢–µ–≥ title –≤ –Ω–æ—Ä–º–µ")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è title: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å title")
        update_progress()

        try:
            meta_desc = driver.find_elements(By.XPATH, "//meta[@name='description']")
            if meta_desc:
                desc_content = meta_desc[0].get_attribute("content")
                log_text += f"üìù –ú–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ: {desc_content} (–î–ª–∏–Ω–∞: {len(desc_content)})\n"
                seo_positives.append(f"–ú–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ: {desc_content} (–î–ª–∏–Ω–∞: {len(desc_content)})")
                if len(desc_content) > 160:
                    seo_errors.append("–î–ª–∏–Ω–∞ –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–µ–≤—ã—à–∞–µ—Ç 160 —Å–∏–º–≤–æ–ª–æ–≤")
                    seo_recs.append("–°–æ–∫—Ä–∞—Ç–∏—Ç–µ –¥–æ 160 —Å–∏–º–≤–æ–ª–æ–≤.")
            else:
                seo_errors.append("–ú–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏—è: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        try:
            h1_tags = driver.find_elements(By.TAG_NAME, "h1")
            if len(h1_tags) == 1:
                seo_positives.append("–û–¥–∏–Ω H1 –Ω–∞–π–¥–µ–Ω")
            elif len(h1_tags) > 1:
                seo_errors.append(f"–ù–∞–π–¥–µ–Ω–æ {len(h1_tags)} —Ç–µ–≥–æ–≤ H1")
                seo_recs.append("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω H1.")
            else:
                seo_errors.append("–¢–µ–≥ H1 –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ —Ç–µ–≥ H1.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Open Graph
        log_text += "\nüåê –ü—Ä–æ–≤–µ—Ä–∫–∞ Open Graph\n"
        try:
            og_tags = check_open_graph(driver)
            for tag, content in og_tags.items():
                log_text += f"  - {tag}: {content}\n"
                seo_positives.append(f"{tag}: {content}")
                if "–ù–µ –Ω–∞–π–¥–µ–Ω" in content or "–ë–µ–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ" in content:
                    seo_errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–≥ Open Graph: {tag}")
                    seo_recs.append(f"–î–æ–±–∞–≤—å—Ç–µ {tag}.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Open Graph: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å Open Graph")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏
        log_text += "\nüìã –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏ Schema.org\n"
        try:
            has_schema, schema_result = check_schema_markup(driver)
            log_text += f"  - {schema_result}\n"
            if has_schema:
                seo_positives.append(schema_result)
            else:
                seo_errors.append(schema_result)
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫—É Schema.org.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫—É")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ noindex, nofollow, noarchive –∏ —Ç–µ–≥–∞ <noindex>
        log_text += "\nü§ñ –ü—Ä–æ–≤–µ—Ä–∫–∞ meta robots –∏ —Ç–µ–≥–∞ <noindex>\n"
        try:
            noindex_meta, nofollow_meta, noarchive_meta, has_noindex_tag, robots_content = check_noindex_nofollow_noarchive(driver)
            if robots_content != "–ù–µ –Ω–∞–π–¥–µ–Ω":
                log_text += f"üìù Meta robots content: {robots_content}\n"
                seo_positives.append(f"Meta robots: {robots_content}")
                if noindex_meta:
                    seo_errors.append("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–º–µ–µ—Ç noindex –≤ meta robots (–Ω–µ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç—Å—è)")
                    seo_recs.append("–£–¥–∞–ª–∏—Ç–µ noindex, –µ—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–æ–ª–∂–Ω–∞ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è.")
                if nofollow_meta:
                    seo_errors.append("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–º–µ–µ—Ç nofollow –≤ meta robots (—Å—Å—ã–ª–∫–∏ –Ω–µ —Å–ª–µ–¥—É—é—Ç—Å—è)")
                    seo_recs.append("–£–¥–∞–ª–∏—Ç–µ nofollow, –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã —Å–ª–µ–¥–æ–≤–∞—Ç—å.")
                if noarchive_meta:
                    seo_errors.append("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–º–µ–µ—Ç noarchive –≤ meta robots (–Ω–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –∫–æ–ø–∏–∏)")
                    seo_recs.append("–£–¥–∞–ª–∏—Ç–µ noarchive, –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –∫–æ–ø–∏—è.")
            else:
                seo_positives.append("Meta robots –Ω–µ –Ω–∞–π–¥–µ–Ω (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é index, follow, archive)")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–≥–∞ <noindex>
            if has_noindex_tag:
                seo_errors.append("–ù–∞–π–¥–µ–Ω —Ç–µ–≥ <noindex> (–∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç—Å—è)")
                seo_recs.append("–£–¥–∞–ª–∏—Ç–µ —Ç–µ–≥ <noindex>, –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è.")
            else:
                seo_positives.append("–¢–µ–≥ <noindex> –Ω–µ –Ω–∞–π–¥–µ–Ω (–∫–æ–Ω—Ç–µ–Ω—Ç –º–æ–∂–µ—Ç –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è)")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ meta robots –∏ —Ç–µ–≥–∞ <noindex>: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å meta robots –∏ —Ç–µ–≥ <noindex>")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∫—Ä—ã—Ç—ã—Ö –±–ª–æ–∫–æ–≤ display: none
        log_text += "\nüïµÔ∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∫—Ä—ã—Ç—ã—Ö –±–ª–æ–∫–æ–≤ (display: none)\n"
        try:
            hidden_blocks = check_hidden_blocks(driver)
            if hidden_blocks:
                log_text += f"‚ùå –ù–∞–π–¥–µ–Ω–æ {len(hidden_blocks)} —Å–∫—Ä—ã—Ç—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º:\n"
                for block in hidden_blocks[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–º –≤—ã–≤–æ–¥ –ø–µ—Ä–≤—ã–º–∏ 5
                    log_text += f"  - {block}...\n"
                seo_errors.append(f"–ù–∞–π–¥–µ–Ω–æ {len(hidden_blocks)} —Å–∫—Ä—ã—Ç—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (display: none) —Å —Ç–µ–∫—Å—Ç–æ–º")
                seo_recs.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–∫—Ä—ã—Ç—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —Å–ø–∞–º; –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏ –º–æ–≥—É—Ç penalize –∑–∞ —Å–∫—Ä—ã—Ç—ã–π —Ç–µ–∫—Å—Ç.")
            else:
                seo_positives.append("–ù–µ—Ç —Å–∫—Ä—ã—Ç—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å display: none —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —Ç–µ–∫—Å—Ç")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∫—Ä—ã—Ç—ã—Ö –±–ª–æ–∫–æ–≤: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –±–ª–æ–∫–∏")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ canonical
        log_text += "\nüìå –ü—Ä–æ–≤–µ—Ä–∫–∞ canonical\n"
        try:
            has_canonical, canonical_href = check_canonical(driver)
            if has_canonical:
                seo_positives.append(f"Canonical –Ω–∞–π–¥–µ–Ω: {canonical_href}")
                if canonical_href != site_url:
                    seo_errors.append("Canonical —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –¥—Ä—É–≥–æ–π URL (–≤–æ–∑–º–æ–∂–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç)")
                    seo_recs.append("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ canonical —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é.")
            else:
                seo_errors.append("Canonical –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ canonical –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ canonical: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å canonical")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        log_text += "\nüìÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ (rel=next/prev)\n"
        try:
            has_next, has_prev = check_pagination_links(driver)
            if has_next or has_prev:
                seo_positives.append(f"–ü–∞–≥–∏–Ω–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–∞: next={has_next}, prev={has_prev}")
            else:
                seo_positives.append("–ü–∞–≥–∏–Ω–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è)")
            # –î–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å canonical
            if has_next or has_prev and has_canonical and canonical_href == site_url:
                seo_positives.append("Canonical –Ω–∞ –ø–∞–≥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            elif has_next or has_prev:
                seo_errors.append("–ü—Ä–æ–±–ª–µ–º–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: canonical –Ω–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
                seo_recs.append("–î–ª—è –ø–∞–≥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ self-canonical –∏–ª–∏ –Ω–∞ –ø–µ—Ä–≤—É—é.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–∞–≥–∏–Ω–∞—Ü–∏—é")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
        log_text += "\nüîó –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫\n"
        try:
            ext_count, nofollow_count, broken_ext = check_external_links(driver, site_url)
            seo_positives.append(f"–í–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {ext_count}, —Å nofollow: {nofollow_count}")
            if broken_ext:
                seo_errors.append(f"–ë–∏—Ç—ã–µ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏: {len(broken_ext)} ({', '.join(broken_ext[:5])}...)")
                seo_recs.append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ –±–∏—Ç—ã–µ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏.")
            if nofollow_count < ext_count / 2:
                seo_errors.append("–ú–∞–ª–æ nofollow –Ω–∞ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–∫–∞—Ö")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ nofollow –Ω–∞ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –¥–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (–±–∞–∑–æ–≤–∞—è)
        log_text += "\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n"
        try:
            is_duplicate, dup_msg = check_duplicates(driver)
            if is_duplicate:
                seo_errors.append(f"–ü—Ä–æ–±–ª–µ–º–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {dup_msg}")
                seo_recs.append("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ canonical –¥–ª—è —É–∫–∞–∑–∞–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –≤–µ—Ä—Å–∏–∏.")
            else:
                seo_positives.append(dup_msg)
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∫–ª–∞–º—ã
        log_text += "\nüì¢ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∫–ª–∞–º—ã\n"
        try:
            has_ads_issue, ads_count = check_ads(driver)
            if has_ads_issue:
                seo_errors.append(f"–ú–Ω–æ–≥–æ —Ä–µ–∫–ª–∞–º—ã ({ads_count} —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
                seo_recs.append("–°–æ–∫—Ä–∞—Ç–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–ª–∞–º—ã, —á—Ç–æ–±—ã –Ω–µ —É—Ö—É–¥—à–∞—Ç—å UX –∏ SEO.")
            else:
                seo_positives.append("–†–µ–∫–ª–∞–º–∞ –≤ –Ω–æ—Ä–º–µ")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–∫–ª–∞–º—ã: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∫–ª–∞–º—É")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        log_text += "\nüõ°Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≥—Ä–æ–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏\n"
        try:
            security_issues = check_security(driver, site_url)
            if security_issues:
                seo_errors.extend(security_issues)
                seo_recs.append("–ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ HTTPS, –∏—Å–ø—Ä–∞–≤—å—Ç–µ mixed content.")
            else:
                seo_positives.append("–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤ –Ω–æ—Ä–º–µ")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        log_text += "\nüñº –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n"
        try:
            images = driver.find_elements(By.TAG_NAME, "img")
            html = driver.page_source
            bg_images = get_background_images(html)
            total_images = len(images) + len(bg_images)
            log_text += f"üñº –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (img + bg): {total_images}\n"
            images_list = []
            if total_images == 0:
                seo_positives.append("–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            else:
                seo_positives.append(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_images}")
                images_no_alt = []
                images_no_title = []
                large_images = []
                for img in images:
                    alt = img.get_attribute("alt")
                    title = img.get_attribute("title")
                    src = img.get_attribute("src") or "No src"
                    size_kb = get_image_size(src, ignore_ssl)
                    images_list.append({'src': src, 'alt': alt, 'title': title, 'size': size_kb})
                    if not alt:
                        images_no_alt.append(src)
                    if not title:
                        images_no_title.append(src)
                    if size_kb > 300:
                        large_images.append(src)
                for bg in bg_images:
                    alt = ""  # bg images usually don't have alt/title
                    title = ""
                    size_kb = get_image_size(bg, ignore_ssl)
                    images_list.append({'src': bg, 'alt': alt, 'title': title, 'size': size_kb})
                    images_no_alt.append(bg)  # Since no alt
                    images_no_title.append(bg)  # Since no title
                    if size_kb > 300:
                        large_images.append(bg)
                if images_no_alt:
                    log_text += f"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ alt ({len(images_no_alt)}):\n" + "\n".join(f"- {s}" for s in images_no_alt) + "\n"
                    seo_errors.append(f"{len(images_no_alt)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ alt")
                    seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ alt –∞—Ç—Ä–∏–±—É—Ç—ã –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è SEO –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏.")
                else:
                    seo_positives.append("–í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–º–µ—é—Ç alt")
                if images_no_title:
                    log_text += f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ title ({len(images_no_title)}):\n" + "\n".join(f"- {s}" for s in images_no_title) + "\n"
                    seo_errors.append(f"{len(images_no_title)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ title")
                    seo_recs.append("–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ title –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –¥–ª—è –ø–æ–¥—Å–∫–∞–∑–æ–∫.")
                else:
                    seo_positives.append("–í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–º–µ—é—Ç title")
                if large_images:
                    log_text += f"‚ùå –ö—Ä—É–ø–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (>300 –ö–ë) ({len(large_images)}):\n" + "\n".join(f"- {s}" for s in large_images) + "\n"
                    seo_errors.append(f"{len(large_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π >300 –ö–ë")
                    seo_recs.append("–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if target_keywords and target_keywords.strip():
            log_text += "\nüîç –ê–ù–ê–õ–ò–ó –¶–ï–õ–ï–í–´–• –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í\n"
            try:
                keywords, density, target_analysis = analyze_keywords(driver, site_url, target_keywords)
                
                if isinstance(target_analysis, dict) and target_analysis:
                    log_text += "=" * 60 + "\n"
                    for tkw, data in target_analysis.items():
                        log_text += f"üéØ –¶–ï–õ–ï–í–û–ï –ö–õ–Æ–ß–ï–í–û–ï –°–õ–û–í–û: '{tkw}'\n"
                        log_text += f"üìä –û–ë–©–ê–Ø –ß–ê–°–¢–û–¢–ê (—Å–æ —Å–∫–ª–æ–Ω–µ–Ω–∏—è–º–∏): {data['freq']} —Ä–∞–∑\n"
                        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                        density_value = data['density']
                        if isinstance(density_value, (int, float)):
                            log_text += f"üìà –ü–õ–û–¢–ù–û–°–¢–¨: {density_value:.2%}\n"
                        else:
                            log_text += f"üìà –ü–õ–û–¢–ù–û–°–¢–¨: {density_value}\n"
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è
                        if 'declensions_found' in data and data['declensions_found']:
                            log_text += "üìù –ù–ê–ô–î–ï–ù–ù–´–ï –°–ö–õ–û–ù–ï–ù–ò–Ø:\n"
                            for declension, count in data['declensions_found'].items():
                                if count > 0:
                                    log_text += f"  ‚úÖ '{declension}': {count} —Ä–∞–∑\n"
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π
                            log_text += "\nüìÑ –ü–û–õ–ù–´–ô –¢–ï–ö–°–¢ –° –ü–û–î–°–í–ï–¢–ö–û–ô:\n"
                            html = driver.page_source
                            soup = BeautifulSoup(html, 'html.parser')
                            text = soup.get_text(separator=' ', strip=True)
                            
                            # –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è
                            highlighted_text = text
                            for declension, count in data['declensions_found'].items():
                                if count > 0:
                                    # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è –Ω–∞ –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
                                    pattern = r'\b' + re.escape(declension) + r'\b'
                                    highlighted_text = re.sub(pattern, f"„Äê{declension}„Äë", highlighted_text, flags=re.IGNORECASE)
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π
                            preview = highlighted_text[:500] + "..." if len(highlighted_text) > 500 else highlighted_text
                            log_text += f"{preview}\n"
                            
                        else:
                            log_text += "‚ùå –°–∫–ª–æ–Ω–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\n"
                        
                        log_text += "=" * 60 + "\n\n"
                        
                        # –û—Ü–µ–Ω–∫–∞ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                        if isinstance(density_value, (int, float)):
                            if density_value < 0.01:
                                seo_errors.append(f"–ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ '{tkw}' ({density_value:.2%})")
                                seo_recs.append(f"–£–≤–µ–ª–∏—á—å—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ '{tkw}' –¥–æ 1-2%.")
                            elif density_value > 0.03:
                                seo_errors.append(f"–í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ '{tkw}' ({density_value:.2%})")
                                seo_recs.append(f"–°–Ω–∏–∑—å—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ '{tkw}' –¥–æ 1-2%.")
                            else:
                                seo_positives.append(f"–ù–æ—Ä–º–∞–ª—å–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ '{tkw}' ({density_value:.2%})")
                        else:
                            seo_errors.append(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è '{tkw}': {density_value}")
                else:
                    log_text += "‚ùå –¶–µ–ª–µ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ\n"
                    seo_errors.append("–¶–µ–ª–µ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            except Exception as e:
                log_to_file(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {str(e)}")
                seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ (–±–∏—Ç—ã–µ) –∏ —Å–±–æ—Ä —Å—Ç–∞—Ç—É—Å–æ–≤
        try:
            links = driver.find_elements(By.TAG_NAME, "a")
            log_text += f"üîó –°—Å—ã–ª–æ–∫: {len(links)}\n"
            general_positives.append(f"–°—Å—ã–ª–æ–∫: {len(links)}")
            site_links = [link.get_attribute("href") for link in links if link.get_attribute("href") and site_url in link.get_attribute("href")]
            for i, link in enumerate(links, 1):
                href = link.get_attribute("href") or "–ù–µ—Ç href"
                if href and "javascript:void" not in href and not href.startswith("#"):
                    result = check_resource(href, ignore_ssl)
                    href, status, _, _ = result
                    link_statuses[href] = status
                    if not isinstance(status, int) or status != 200:
                        broken_links.append(href)
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–æ–∫: {str(e)}")
            general_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Å—ã–ª–∫–∏")
        if broken_links:
            general_errors.append(f"–ù–∞–π–¥–µ–Ω—ã –±–∏—Ç—ã–µ —Å—Å—ã–ª–∫–∏: {len(broken_links)} ({', '.join(broken_links)})")
            general_recs.append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ –±–∏—Ç—ã–µ —Å—Å—ã–ª–∫–∏.")
        else:
            general_positives.append("–ù–µ—Ç –±–∏—Ç—ã—Ö —Å—Å—ã–ª–æ–∫")
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å sitemap (–∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
        if urls_in_sitemap:
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–∞–π—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            site_pages = get_site_pages(site_url, ignore_ssl)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            def normalize_url(url):
                """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
                if not url:
                    return url
                url = url.rstrip('/')
                if '#' in url:
                    url = url.split('#')[0]
                if '?' in url:
                    url = url.split('?')[0]
                return url
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å–µ URL
            normalized_site_pages = set(normalize_url(url) for url in site_pages)
            normalized_sitemap_urls = set(normalize_url(url) for url in urls_in_sitemap)
            
            # –°—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Å–∞–π—Ç–µ, –Ω–æ –Ω–µ –≤ sitemap
            pages_not_in_sitemap = [url for url in site_pages if normalize_url(url) not in normalized_sitemap_urls]
            
            # –°—Ç—Ä–∞–Ω–∏—Ü—ã –≤ sitemap, –Ω–æ –Ω–µ –Ω–∞ —Å–∞–π—Ç–µ (–Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ)
            # –í run_test –º—ã –Ω–µ –∏–º–µ–µ–º page_details, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ—Å—Ç–æ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º URL
            pages_in_sitemap_not_on_site = [url for url in urls_in_sitemap if normalize_url(url) not in normalized_site_pages]
            
            if pages_not_in_sitemap:
                seo_errors.append(f"–°—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Å–∞–π—Ç–µ –Ω–µ –≤ sitemap: {len(pages_not_in_sitemap)} ({', '.join(pages_not_in_sitemap[:5])}...)")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ sitemap.")
            if pages_in_sitemap_not_on_site:
                seo_errors.append(f"–°—Ç—Ä–∞–Ω–∏—Ü—ã –≤ sitemap –Ω–µ –Ω–∞ —Å–∞–π—Ç–µ: {len(pages_in_sitemap_not_on_site)} ({', '.join(pages_in_sitemap_not_on_site[:5])}...)")
                seo_recs.append("–£–¥–∞–ª–∏—Ç–µ –ª–∏—à–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ sitemap –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–∞–π—Ç.")
        update_progress()

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        try:
            chart_base64 = generate_performance_chart(load_times, resource_times, js_css_times)
            page.add(ft.Image(src_base64=chart_base64, width=800, height=500))
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {str(e)}")
        update_progress()

        log_to_file(f"{site_url} - –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω")

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≤–æ–¥–æ–∫
        seo_area = ft.TextField()
        perf_area = ft.TextField()
        links_area = ft.TextField()
        seo_area.value = format_summary_section(seo_positives, seo_errors, seo_recs, "SEO –ê–Ω–∞–ª–∏–∑")
        perf_area.value = format_summary_section(perf_positives, perf_errors, perf_recs, "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        if not sitemap_errors:
            general_positives.append("‚úÖ Sitemap OK")
        full_summary = format_summary_section(general_positives + seo_positives + perf_positives,
                                                    general_errors + seo_errors + perf_errors,
                                                    general_recs + seo_recs + perf_recs,
                                                    "–û–±—â–∞—è –°–≤–æ–¥–∫–∞")
        summary_area.value = full_summary  # –¢–æ–ª—å–∫–æ –∏—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞!
        page.data['full_summary'] = full_summary
        page.data['seo_summary'] = seo_area.value  # –°–æ—Ö—Ä–∞–Ω—è–µ–º SEO —Å–≤–æ–¥–∫—É –æ—Ç–¥–µ–ª—å–Ω–æ
        links_area.value = format_links_section(link_statuses)
        links_summary = check_links_summary(link_statuses)
        page.data['links_summary'] = links_summary

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        images_summary = "### –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n\n"
        if total_images == 0:
            images_summary += "–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n"
        else:
            for img in images_list:
                alt_emoji = "üü¢" if img['alt'] else "üî¥"
                title_emoji = "üü¢" if img['title'] else "üî¥"
                size_emoji = "üü¢" if img['size'] <= 300 else "üî¥"
                images_summary += f"–°—Å—ã–ª–∫–∞: {img['src']}\n"
                images_summary += f"Alt: {alt_emoji} {img['alt'] or '–ù–µ—Ç'}\n"
                images_summary += f"Title: {title_emoji} {img['title'] or '–ù–µ—Ç'}\n"
                images_summary += f"–†–∞–∑–º–µ—Ä: {size_emoji} {img['size']:.2f} –ö–ë\n\n"
        page.data['images_summary'] = images_summary

        page.data['robots_summary'] = check_robots_summary(site_url, ignore_ssl)
        sitemap_summary = check_sitemap_summary(site_url, ignore_ssl)
        page.data['sitemap_summary'] = sitemap_summary

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        save_results(site_url, summary_area.value, full_summary)

    except Exception as e:
        summary_area.value = f"‚ùå –û—à–∏–±–∫–∞: {str(e)}\n"
        general_errors.append(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}")
        log_to_file(f"{site_url} - –û—à–∏–±–∫–∞: {str(e)}")
    finally:
        try:
            if 'driver' in locals() and driver:
                driver.quit()
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è WebDriver: {str(e)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        if stop_event and stop_event.is_set():
            summary_area.value = "‚èπ –¢–µ—Å—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"
        else:
            progress_bar.value = 1.0
        
        # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
        page.data['stop_btn_visible'] = False
        page.data['run_btn_visible'] = True
        page.update()







def run_links_test(site_url: str, summary_area: ft.TextField, page: ft.Page, progress_bar: ft.ProgressBar, ignore_ssl: bool, target_keywords: str, max_links: int = 15000):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ robots –∏ sitemap."""
    if not re.match(r'^https?://', site_url):
        summary_area.value = "‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π URL\n"
        page.update()
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–±—ã—Ç–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
    stop_event = page.data.get('stop_event')
    if stop_event and stop_event.is_set():
        summary_area.value = "‚èπ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"
        page.update()
        return

    driver = None
    general_positives = []
    general_errors = []
    general_recs = []
    seo_positives = []
    seo_errors = []
    seo_recs = []
    perf_positives = []
    perf_errors = []
    perf_recs = []
    broken_links = []
    link_statuses = {}  # –î–ª—è –≤–∫–ª–∞–¥–∫–∏ –°—Å—ã–ª–∫–∏
    site_links = []
    total_checks = 25  # –£–º–µ–Ω—å—à–µ–Ω–æ –±–µ–∑ robots –∏ sitemap
    current_check = 0
    log_text = ""  # –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –ª–æ–≥–æ–≤
    total_images = 0  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    images_list = []  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

    def update_progress():
        nonlocal current_check
        current_check += 1
        progress_bar.value = min(current_check / total_checks, 1.0)
        page.update()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–±—ã—Ç–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        if stop_event and stop_event.is_set():
            return False  # –°–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        return True  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º

    try:
        log_text += f"\n=== üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Å—ã–ª–æ–∫: {site_url} ===\n"
        general_positives.append(f"üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Å—ã–ª–æ–∫: {site_url} ({datetime.now().strftime('%Y-%m-%d %H:%M:%S %Z')})")
        if not update_progress():
            return

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–∞ —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        log_text += "\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–∞ —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫...\n"
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π –º–µ—Ç–æ–¥
        anti_bot_mode = False
        try:
            start_time = time.time()
            r = requests.get(site_url, timeout=5, verify=not ignore_ssl, allow_redirects=True)
            load_time = time.time() - start_time
            page_size = len(r.content) / 1024
            log_text += f"üîé HTTP —Å—Ç–∞—Ç—É—Å: {r.status_code}\n"
            log_text += f"‚è± –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ (HTTP): {load_time:.2f} —Å–µ–∫\n"
            log_text += f"üìè –†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_size:.2f} –ö–ë\n"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            if any(blocked_text in r.text.lower() for blocked_text in [
                'access denied', '–¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω', 'blocked', '–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω',
                'cloudflare', 'captcha', 'recaptcha', 'bot', 'robot'
            ]):
                log_text += "‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞, –ø—Ä–æ–±—É–µ–º –æ–±—Ö–æ–¥...\n"
                general_errors.append("–°–∞–π—Ç –±–ª–æ–∫–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø (–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ –æ—Ç –±–æ—Ç–æ–≤)")
                anti_bot_mode = True
            else:
                general_positives.append(f"–°–∞–π—Ç –¥–æ—Å—Ç—É–ø–µ–Ω (HTTP —Å—Ç–∞—Ç—É—Å: {r.status_code}, –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {load_time:.2f} —Å–µ–∫)")
                general_positives.append(f"–†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_size:.2f} –ö–ë")
                anti_bot_mode = False
                
            if page_size > 2000:
                general_errors.append("–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–ª–∏—à–∫–æ–º —Ç—è–∂—ë–ª–∞—è")
                general_recs.append("–°–æ–∂–º–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–π—Ç–µ CSS/JS, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ lazy loading.")
            if load_time > 3:
                general_errors.append("–í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ")
                general_recs.append("–í–∫–ª—é—á–∏—Ç–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ CDN, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Å–µ—Ä–≤–µ—Ä.")
            if r.status_code != 200:
                seo_errors.append("–ü—Ä–æ–±–ª–µ–º–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: —Å—Ç–∞—Ç—É—Å –Ω–µ 200")
                seo_recs.append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ —Å—Ç–∞—Ç—É—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
                
        except Exception as e:
            log_text += f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}\n"
            log_text += "üîÑ –ü—Ä–æ–±—É–µ–º –æ–±—Ö–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫...\n"
            anti_bot_mode = True
            general_errors.append(f"–°–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            seo_errors.append("–ü—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        
        update_progress()

        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥—Ä–∞–π–≤–µ—Ä–∞ –¥–ª—è SEO –ø—Ä–æ–≤–µ—Ä–æ–∫ —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        try:
            if anti_bot_mode:
                log_text += "üõ°Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ WebDriver –≤ —Ä–µ–∂–∏–º–µ –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫...\n"
                driver = create_webdriver(ignore_ssl=ignore_ssl, anti_bot_mode=True)
            else:
                driver = create_webdriver(ignore_ssl=ignore_ssl)
            
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É
            try:
                driver.get(site_url)
                log_text += "‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n"
            except Exception as e:
                log_text += f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}\n"
                if not anti_bot_mode:
                    log_text += "üîÑ –ü—Ä–æ–±—É–µ–º –≤ —Ä–µ–∂–∏–º–µ –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫...\n"
                    driver.quit()
                    driver = create_webdriver(ignore_ssl=ignore_ssl, anti_bot_mode=True)
                    driver.get(site_url)
                    log_text += "‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ —Ä–µ–∂–∏–º–µ –æ–±—Ö–æ–¥–∞\n"
                
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è WebDriver: {str(e)}")
            summary_area.value = f"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å WebDriver: {str(e)}"
            page.update()
            return

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ ChromeDriver
        driver_version = driver.capabilities['chrome']['chromedriverVersion'].split(' ')[0]
        log_text += f"üìå –í–µ—Ä—Å–∏—è ChromeDriver: {driver_version}\n"
        general_positives.append(f"–í–µ—Ä—Å–∏—è ChromeDriver: {driver_version}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–µ—Ä–∫–∞–ª –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ URL
        log_text += "\nüîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–µ—Ä–∫–∞–ª –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ URL\n"
        mirror_issues = check_mirrors_and_redirects(site_url, ignore_ssl)
        if mirror_issues:
            seo_errors.extend(mirror_issues)
            seo_recs.append("–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ 301 —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã –¥–ª—è –∑–µ—Ä–∫–∞–ª (www/non-www, http/https).")
        else:
            seo_positives.append("–ó–µ—Ä–∫–∞–ª–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ø–æ—á–µ–∫ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
        log_text += "\nüîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ø–æ—á–µ–∫ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤\n"
        has_long_chain, chain_len = check_redirect_chain(site_url, ignore_ssl)
        if has_long_chain:
            seo_errors.append(f"–î–ª–∏–Ω–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ ({chain_len} —à–∞–≥–æ–≤)")
            seo_recs.append("–°–æ–∫—Ä–∞—Ç–∏—Ç–µ —Ü–µ–ø–æ—á–∫—É —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –¥–æ 1-2 —à–∞–≥–æ–≤.")
        else:
            seo_positives.append("–¶–µ–ø–æ—á–∫–∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –≤ –Ω–æ—Ä–º–µ")
        update_progress()

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        log_text += "\n‚è± –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n"
        performance_data = analyze_performance(site_url, ignore_ssl)
        load_times = performance_data["load_times"]
        resource_times = performance_data["resource_times"]
        js_css_times = performance_data["js_css_times"]

        log_text += f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {', '.join(f'{t:.2f} —Å–µ–∫' for t in load_times)}\n"
        log_text += f"‚è± –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤: {', '.join(f'{t:.2f} —Å–µ–∫' for t in resource_times)}\n"
        log_text += f"‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JS/CSS: {', '.join(f'{t:.2f} —Å–µ–∫' for t in js_css_times)}\n"
        perf_positives.append(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {', '.join(f'{t:.2f} —Å–µ–∫' for t in load_times)}")
        perf_positives.append(f"–í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤: {', '.join(f'{t:.2f} —Å–µ–∫' for t in resource_times)}")
        perf_positives.append(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JS/CSS: {', '.join(f'{t:.2f} —Å–µ–∫' for t in js_css_times)}")
        for i, (lt, rt, jct) in enumerate(zip(load_times, resource_times, js_css_times)):
            res = ['1920x1080', '768x1024', '375x667'][i]
            if lt > 3:
                perf_errors.append(f"–°–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ {res}: {lt:.2f} —Å–µ–∫")
                perf_recs.append("–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Å–µ—Ä–≤–µ—Ä –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ.")
            else:
                perf_positives.append(f"–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ {res}: {lt:.2f} —Å–µ–∫")
            if rt > 1:
                perf_errors.append(f"–î–æ–ª–≥–æ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–∞ {res}: {rt:.2f} —Å–µ–∫")
                perf_recs.append("–°–æ–∂–º–∏—Ç–µ —Ä–µ—Å—É—Ä—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ CDN.")
            if jct > 1:
                perf_errors.append(f"–î–æ–ª–≥–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JS/CSS –Ω–∞ {res}: {jct:.2f} —Å–µ–∫")
                perf_recs.append("–ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–π—Ç–µ —Å–∫—Ä–∏–ø—Ç—ã.")
        update_progress()

        # Core Web Vitals
        log_text += "\n‚ö° Core Web Vitals\n"
        try:
            lcp, fid, cls = get_core_web_vitals(driver)
            core_vitals_summary = f"LCP: {lcp:.2f} –º—Å | FID: {fid:.2f} –º—Å | CLS: {cls:.3f}\n"
            if lcp > 2500:
                perf_errors.append(f"LCP —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {lcp:.0f} –º—Å")
                perf_recs.append("–°–æ–∫—Ä–∞—Ç–∏—Ç–µ –≤—Ä–µ–º—è Largest Contentful Paint –¥–æ <2.5 —Å–µ–∫.")
            if cls > 0.1:
                perf_errors.append(f"CLS —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {cls:.3f}")
                perf_recs.append("–°–Ω–∏–∑—å—Ç–µ Cumulative Layout Shift –¥–æ <0.1.")
            log_text += core_vitals_summary
            perf_positives.append(core_vitals_summary)
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è Core Web Vitals: {str(e)}")
            perf_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å Core Web Vitals")
        update_progress()

        # –ú–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∞
        log_text += "\nüîé –ú–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∞ (Schema.org, JSON-LD, OpenGraph, Twitter)\n"
        try:
            schema_items, jsonld_blocks, og_tags, twitter_tags = get_microdata(driver)
            if schema_items:
                seo_positives.append(f"Schema.org items: {', '.join(schema_items)}")
            else:
                seo_errors.append("Schema.org –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ Schema.org –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫—É.")
            if jsonld_blocks:
                seo_positives.append(f"JSON-LD –±–ª–æ–∫–æ–≤: {len(jsonld_blocks)}")
            else:
                seo_errors.append("JSON-LD –Ω–µ –Ω–∞–π–¥–µ–Ω")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ JSON-LD –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —Å–∞–π—Ç–∞.")
            if og_tags:
                seo_positives.append(f"OpenGraph: {', '.join(og_tags.keys())}")
            else:
                seo_errors.append("OpenGraph –Ω–µ –Ω–∞–π–¥–µ–Ω")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ OpenGraph –¥–ª—è —Å–æ—Ü—Å–µ—Ç–µ–π.")
            if twitter_tags:
                seo_positives.append(f"Twitter Cards: {', '.join(twitter_tags.keys())}")
            else:
                seo_errors.append("Twitter Cards –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ Twitter Cards –¥–ª—è —Å–æ—Ü—Å–µ—Ç–µ–π.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫—É")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ SEO-—ç–ª–µ–º–µ–Ω—Ç–æ–≤
        try:
            title_tag = driver.title if driver.title else "–ù–µ –Ω–∞–π–¥–µ–Ω"
            log_text += f"üìù –¢–µ–≥ title: {title_tag} (–î–ª–∏–Ω–∞: {len(title_tag)})\n"
            seo_positives.append(f"–¢–µ–≥ title: {title_tag} (–î–ª–∏–Ω–∞: {len(title_tag)})")
            if len(title_tag) > 60:
                seo_errors.append("–î–ª–∏–Ω–∞ title –ø—Ä–µ–≤—ã—à–∞–µ—Ç 60 —Å–∏–º–≤–æ–ª–æ–≤")
                seo_recs.append("–°–æ–∫—Ä–∞—Ç–∏—Ç–µ title –¥–æ 60 —Å–∏–º–≤–æ–ª–æ–≤.")
            elif title_tag == "–ù–µ –Ω–∞–π–¥–µ–Ω":
                seo_errors.append("–¢–µ–≥ title –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ —Ç–µ–≥ title.")
            else:
                seo_positives.append("–¢–µ–≥ title –≤ –Ω–æ—Ä–º–µ")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è title: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å title")
        update_progress()

        try:
            meta_desc = driver.find_elements(By.XPATH, "//meta[@name='description']")
            if meta_desc:
                desc_content = meta_desc[0].get_attribute("content")
                log_text += f"üìù –ú–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ: {desc_content} (–î–ª–∏–Ω–∞: {len(desc_content)})\n"
                seo_positives.append(f"–ú–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ: {desc_content} (–î–ª–∏–Ω–∞: {len(desc_content)})")
                if len(desc_content) > 160:
                    seo_errors.append("–î–ª–∏–Ω–∞ –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–µ–≤—ã—à–∞–µ—Ç 160 —Å–∏–º–≤–æ–ª–æ–≤")
                    seo_recs.append("–°–æ–∫—Ä–∞—Ç–∏—Ç–µ –¥–æ 160 —Å–∏–º–≤–æ–ª–æ–≤.")
            else:
                seo_errors.append("–ú–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏—è: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        try:
            h1_tags = driver.find_elements(By.TAG_NAME, "h1")
            if len(h1_tags) == 1:
                seo_positives.append("–û–¥–∏–Ω H1 –Ω–∞–π–¥–µ–Ω")
            elif len(h1_tags) > 1:
                seo_errors.append(f"–ù–∞–π–¥–µ–Ω–æ {len(h1_tags)} —Ç–µ–≥–æ–≤ H1")
                seo_recs.append("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω H1.")
            else:
                seo_errors.append("–¢–µ–≥ H1 –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ —Ç–µ–≥ H1.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Open Graph
        log_text += "\nüåê –ü—Ä–æ–≤–µ—Ä–∫–∞ Open Graph\n"
        try:
            og_tags = check_open_graph(driver)
            for tag, content in og_tags.items():
                log_text += f"  - {tag}: {content}\n"
                seo_positives.append(f"{tag}: {content}")
                if "–ù–µ –Ω–∞–π–¥–µ–Ω" in content or "–ë–µ–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ" in content:
                    seo_errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–≥ Open Graph: {tag}")
                    seo_recs.append(f"–î–æ–±–∞–≤—å—Ç–µ {tag}.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Open Graph: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å Open Graph")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏
        log_text += "\nüìã –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏ Schema.org\n"
        try:
            has_schema, schema_result = check_schema_markup(driver)
            log_text += f"  - {schema_result}\n"
            if has_schema:
                seo_positives.append(schema_result)
            else:
                seo_errors.append(schema_result)
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫—É Schema.org.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫—É")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ noindex, nofollow, noarchive –∏ —Ç–µ–≥–∞ <noindex>
        log_text += "\nü§ñ –ü—Ä–æ–≤–µ—Ä–∫–∞ meta robots –∏ —Ç–µ–≥–∞ <noindex>\n"
        try:
            noindex_meta, nofollow_meta, noarchive_meta, has_noindex_tag, robots_content = check_noindex_nofollow_noarchive(driver)
            if robots_content != "–ù–µ –Ω–∞–π–¥–µ–Ω":
                log_text += f"üìù Meta robots content: {robots_content}\n"
                seo_positives.append(f"Meta robots: {robots_content}")
                if noindex_meta:
                    seo_errors.append("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–º–µ–µ—Ç noindex –≤ meta robots (–Ω–µ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç—Å—è)")
                    seo_recs.append("–£–¥–∞–ª–∏—Ç–µ noindex, –µ—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–æ–ª–∂–Ω–∞ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è.")
                if nofollow_meta:
                    seo_errors.append("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–º–µ–µ—Ç nofollow –≤ meta robots (—Å—Å—ã–ª–∫–∏ –Ω–µ —Å–ª–µ–¥—É—é—Ç—Å—è)")
                    seo_recs.append("–£–¥–∞–ª–∏—Ç–µ nofollow, –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã —Å–ª–µ–¥–æ–≤–∞—Ç—å.")
                if noarchive_meta:
                    seo_errors.append("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–º–µ–µ—Ç noarchive –≤ meta robots (–Ω–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –∫–æ–ø–∏–∏)")
                    seo_recs.append("–£–¥–∞–ª–∏—Ç–µ noarchive, –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –∫–æ–ø–∏—è.")
            else:
                seo_positives.append("Meta robots –Ω–µ –Ω–∞–π–¥–µ–Ω (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é index, follow, archive)")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–≥–∞ <noindex>
            if has_noindex_tag:
                seo_errors.append("–ù–∞–π–¥–µ–Ω —Ç–µ–≥ <noindex> (–∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç—Å—è)")
                seo_recs.append("–£–¥–∞–ª–∏—Ç–µ —Ç–µ–≥ <noindex>, –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è.")
            else:
                seo_positives.append("–¢–µ–≥ <noindex> –Ω–µ –Ω–∞–π–¥–µ–Ω (–∫–æ–Ω—Ç–µ–Ω—Ç –º–æ–∂–µ—Ç –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è)")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ meta robots –∏ —Ç–µ–≥–∞ <noindex>: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å meta robots –∏ —Ç–µ–≥ <noindex>")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∫—Ä—ã—Ç—ã—Ö –±–ª–æ–∫–æ–≤ display: none
        log_text += "\nüïµÔ∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∫—Ä—ã—Ç—ã—Ö –±–ª–æ–∫–æ–≤ (display: none)\n"
        try:
            hidden_blocks = check_hidden_blocks(driver)
            if hidden_blocks:
                log_text += f"‚ùå –ù–∞–π–¥–µ–Ω–æ {len(hidden_blocks)} —Å–∫—Ä—ã—Ç—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º:\n"
                for block in hidden_blocks[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–º –≤—ã–≤–æ–¥ –ø–µ—Ä–≤—ã–º–∏ 5
                    log_text += f"  - {block}...\n"
                seo_errors.append(f"–ù–∞–π–¥–µ–Ω–æ {len(hidden_blocks)} —Å–∫—Ä—ã—Ç—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (display: none) —Å —Ç–µ–∫—Å—Ç–æ–º")
                seo_recs.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–∫—Ä—ã—Ç—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —Å–ø–∞–º; –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏ –º–æ–≥—É—Ç penalize –∑–∞ —Å–∫—Ä—ã—Ç—ã–π —Ç–µ–∫—Å—Ç.")
            else:
                seo_positives.append("–ù–µ—Ç —Å–∫—Ä—ã—Ç—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å display: none —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —Ç–µ–∫—Å—Ç")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∫—Ä—ã—Ç—ã—Ö –±–ª–æ–∫–æ–≤: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –±–ª–æ–∫–∏")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ canonical
        log_text += "\nüìå –ü—Ä–æ–≤–µ—Ä–∫–∞ canonical\n"
        try:
            has_canonical, canonical_href = check_canonical(driver)
            if has_canonical:
                seo_positives.append(f"Canonical –Ω–∞–π–¥–µ–Ω: {canonical_href}")
                if canonical_href != site_url:
                    seo_errors.append("Canonical —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –¥—Ä—É–≥–æ–π URL (–≤–æ–∑–º–æ–∂–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç)")
                    seo_recs.append("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ canonical —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é.")
            else:
                seo_errors.append("Canonical –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ canonical –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ canonical: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å canonical")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        log_text += "\nüìÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ (rel=next/prev)\n"
        try:
            has_next, has_prev = check_pagination_links(driver)
            if has_next or has_prev:
                seo_positives.append(f"–ü–∞–≥–∏–Ω–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–∞: next={has_next}, prev={has_prev}")
            else:
                seo_positives.append("–ü–∞–≥–∏–Ω–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è)")
            # –î–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å canonical
            if has_next or has_prev and has_canonical and canonical_href == site_url:
                seo_positives.append("Canonical –Ω–∞ –ø–∞–≥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            elif has_next or has_prev:
                seo_errors.append("–ü—Ä–æ–±–ª–µ–º–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: canonical –Ω–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
                seo_recs.append("–î–ª—è –ø–∞–≥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ self-canonical –∏–ª–∏ –Ω–∞ –ø–µ—Ä–≤—É—é.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–∞–≥–∏–Ω–∞—Ü–∏—é")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
        log_text += "\nüîó –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫\n"
        try:
            ext_count, nofollow_count, broken_ext = check_external_links(driver, site_url)
            seo_positives.append(f"–í–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {ext_count}, —Å nofollow: {nofollow_count}")
            if broken_ext:
                seo_errors.append(f"–ë–∏—Ç—ã–µ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏: {len(broken_ext)} ({', '.join(broken_ext[:5])}...)")
                seo_recs.append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ –±–∏—Ç—ã–µ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏.")
            if nofollow_count < ext_count / 2:
                seo_errors.append("–ú–∞–ª–æ nofollow –Ω–∞ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–∫–∞—Ö")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ nofollow –Ω–∞ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –¥–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        log_text += "\nüîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n"
        try:
            duplicates = check_duplicates(driver)
            if duplicates:
                seo_errors.append(f"–ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
                seo_recs.append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞.")
            else:
                seo_positives.append("–î—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∫–ª–∞–º—ã
        log_text += "\nüì¢ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∫–ª–∞–º—ã\n"
        try:
            ads = check_ads(driver)
            if ads:
                seo_errors.append(f"–ù–∞–π–¥–µ–Ω–æ {len(ads)} —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –±–ª–æ–∫–æ–≤")
                seo_recs.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –±–ª–æ–∫–∏ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–∞–≤–∏–ª–∞–º.")
            else:
                seo_positives.append("–†–µ–∫–ª–∞–º–Ω—ã–µ –±–ª–æ–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–∫–ª–∞–º—ã: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∫–ª–∞–º—É")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        log_text += "\nüîí –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏\n"
        try:
            security_issues = check_security(driver, site_url)
            if security_issues:
                seo_errors.extend(security_issues)
                seo_recs.append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.")
            else:
                seo_positives.append("–ü—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        log_text += "\nüñº –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n"
        try:
            images = driver.find_elements(By.TAG_NAME, "img")
            total_images = len(images)
            images_list = []
            for img in images:
                src = img.get_attribute("src") or "–ù–µ—Ç src"
                alt = img.get_attribute("alt") or ""
                title = img.get_attribute("title") or ""
                try:
                    size = get_image_size(src, ignore_ssl)
                except:
                    size = 0
                images_list.append({"src": src, "alt": alt, "title": title, "size": size})
            log_text += f"üñº –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_images}\n"
            general_positives.append(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_images}")
            if total_images == 0:
                seo_errors.append("–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                seo_recs.append("–î–æ–±–∞–≤—å—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è SEO.")
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {str(e)}")
            seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if target_keywords and target_keywords.strip():
            log_text += "\nüîç –ê–ù–ê–õ–ò–ó –¶–ï–õ–ï–í–´–• –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í\n"
            try:
                keywords, density, target_analysis = analyze_keywords(driver, site_url, target_keywords)
                
                if isinstance(target_analysis, dict) and target_analysis:
                    log_text += "=" * 60 + "\n"
                    for tkw, data in target_analysis.items():
                        log_text += f"üéØ –¶–ï–õ–ï–í–û–ï –ö–õ–Æ–ß–ï–í–û–ï –°–õ–û–í–û: '{tkw}'\n"
                        log_text += f"üìä –û–ë–©–ê–Ø –ß–ê–°–¢–û–¢–ê (—Å–æ —Å–∫–ª–æ–Ω–µ–Ω–∏—è–º–∏): {data['freq']} —Ä–∞–∑\n"
                        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                        density_value = data['density']
                        if isinstance(density_value, (int, float)):
                            log_text += f"üìà –ü–õ–û–¢–ù–û–°–¢–¨: {density_value:.2%}\n"
                        else:
                            log_text += f"üìà –ü–õ–û–¢–ù–û–°–¢–¨: {density_value}\n"
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è
                        if 'declensions_found' in data and data['declensions_found']:
                            log_text += "üìù –ù–ê–ô–î–ï–ù–ù–´–ï –°–ö–õ–û–ù–ï–ù–ò–Ø:\n"
                            for declension, count in data['declensions_found'].items():
                                if count > 0:
                                    log_text += f"  ‚úÖ '{declension}': {count} —Ä–∞–∑\n"
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π
                            log_text += "\nüìÑ –ü–û–õ–ù–´–ô –¢–ï–ö–°–¢ –° –ü–û–î–°–í–ï–¢–ö–û–ô:\n"
                            html = driver.page_source
                            soup = BeautifulSoup(html, 'html.parser')
                            text = soup.get_text(separator=' ', strip=True)
                            
                            # –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è
                            highlighted_text = text
                            for declension, count in data['declensions_found'].items():
                                if count > 0:
                                    # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è –Ω–∞ –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
                                    pattern = r'\b' + re.escape(declension) + r'\b'
                                    highlighted_text = re.sub(pattern, f"„Äê{declension}„Äë", highlighted_text, flags=re.IGNORECASE)
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π
                            preview = highlighted_text[:500] + "..." if len(highlighted_text) > 500 else highlighted_text
                            log_text += f"{preview}\n"
                        else:
                            log_text += "‚ùå –°–∫–ª–æ–Ω–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\n"
                        
                        log_text += "=" * 60 + "\n\n"
                        
                        # –û—Ü–µ–Ω–∫–∞ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                        if isinstance(density_value, (int, float)):
                            if density_value < 0.01:
                                seo_errors.append(f"–ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ '{tkw}' ({density_value:.2%})")
                                seo_recs.append(f"–£–≤–µ–ª–∏—á—å—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ '{tkw}' –¥–æ 1-2%.")
                            elif density_value > 0.03:
                                seo_errors.append(f"–í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ '{tkw}' ({density_value:.2%})")
                                seo_recs.append(f"–°–Ω–∏–∑—å—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ '{tkw}' –¥–æ 1-2%.")
                            else:
                                seo_positives.append(f"–ù–æ—Ä–º–∞–ª—å–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ '{tkw}' ({density_value:.2%})")
                        else:
                            seo_errors.append(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è '{tkw}': {density_value}")
                else:
                    log_text += "‚ùå –¶–µ–ª–µ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ\n"
                    seo_errors.append("–¶–µ–ª–µ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            except Exception as e:
                log_to_file(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {str(e)}")
                seo_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞")
        update_progress()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ (–±–∏—Ç—ã–µ) –∏ —Å–±–æ—Ä —Å—Ç–∞—Ç—É—Å–æ–≤
        try:
            links = driver.find_elements(By.TAG_NAME, "a")
            log_text += f"üîó –ù–∞–π–¥–µ–Ω–æ —Ç–µ–≥–æ–≤ <a>: {len(links)}\n"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            links_to_check = links
            checked_links_count = 0
            
            for i, link in enumerate(links_to_check, 1):
                href = link.get_attribute("href") or "–ù–µ—Ç href"
                if href and "javascript:void" not in href and not href.startswith("#"):
                    result = check_resource(href, ignore_ssl)
                    href, status, _, _ = result
                    link_statuses[href] = status
                    checked_links_count += 1
                    if not isinstance(status, int) or status != 200:
                        broken_links.append(href)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–∫–∞—Ö
            log_text += f"üîó –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {checked_links_count}\n"
            general_positives.append(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {checked_links_count}")
            site_links = [link.get_attribute("href") for link in links if link.get_attribute("href") and site_url in link.get_attribute("href")]
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–æ–∫: {str(e)}")
            general_errors.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Å—ã–ª–∫–∏")
        if broken_links:
            general_errors.append(f"–ù–∞–π–¥–µ–Ω—ã –±–∏—Ç—ã–µ —Å—Å—ã–ª–∫–∏: {len(broken_links)} ({', '.join(broken_links)})")
            general_recs.append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ –±–∏—Ç—ã–µ —Å—Å—ã–ª–∫–∏.")
        else:
            general_positives.append("–ù–µ—Ç –±–∏—Ç—ã—Ö —Å—Å—ã–ª–æ–∫")
        update_progress()

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        try:
            chart_base64 = generate_performance_chart(load_times, resource_times, js_css_times)
            page.add(ft.Image(src_base64=chart_base64, width=800, height=500))
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {str(e)}")
        update_progress()

        log_to_file(f"{site_url} - –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω (—Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏)")

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≤–æ–¥–æ–∫
        seo_area = ft.TextField()
        perf_area = ft.TextField()
        links_area = ft.TextField()
        seo_area.value = format_summary_section(seo_positives, seo_errors, seo_recs, "SEO –ê–Ω–∞–ª–∏–∑")
        perf_area.value = format_summary_section(perf_positives, perf_errors, perf_recs, "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        full_summary = format_summary_section(general_positives + seo_positives + perf_positives,
                                                    general_errors + seo_errors + perf_errors,
                                                    general_recs + seo_recs + perf_recs,
                                                    "–û–±—â–∞—è –°–≤–æ–¥–∫–∞")
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫—É —Å—Å—ã–ª–æ–∫ —Å –∫–Ω–æ–ø–∫–∞–º–∏ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
        links_summary_with_buttons = "### –°—Å—ã–ª–∫–∏\n\n"
        for url, status in link_statuses.items():
            status_emoji = "üü¢" if isinstance(status, int) and status == 200 else "üî¥"
            links_summary_with_buttons += f"{status_emoji} {url} (–°—Ç–∞—Ç—É—Å: {status})\n"
            links_summary_with_buttons += f"   [–î–µ—Ç–∞–ª–∏] - –∫–Ω–æ–ø–∫–∞ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–µ—Ç–∞–ª–µ–π\n\n"
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫—É —Å—Å—ã–ª–æ–∫ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–µ—Ç–∞–ª—è—Ö
        links_summary_simple = f"### –°—Å—ã–ª–∫–∏ ({len(link_statuses)} –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö)\n\n"
        for url, status in link_statuses.items():
            status_emoji = "üü¢" if isinstance(status, int) and status == 200 else "üî¥"
            links_summary_simple += f"{status_emoji} {url} (–°—Ç–∞—Ç—É—Å: {status})\n"
        links_summary_simple += f"\nüí° –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {len(link_statuses)} —Å—Å—ã–ª–æ–∫ –∏–∑ {len(links)} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ–≥–æ–≤ <a>"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–æ–∫ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
        page.data['link_statuses'] = link_statuses
        
        summary_area.value = full_summary  # –¢–æ–ª—å–∫–æ –∏—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞!
        page.data['full_summary'] = full_summary
        page.data['seo_summary'] = seo_area.value  # –°–æ—Ö—Ä–∞–Ω—è–µ–º SEO —Å–≤–æ–¥–∫—É –æ—Ç–¥–µ–ª—å–Ω–æ
        links_area.value = format_links_section(link_statuses)
        links_summary = check_links_summary(link_statuses)
        page.data['links_summary'] = links_summary
        page.data['links_summary_with_buttons'] = links_summary_with_buttons

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        images_summary = "### –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n\n"
        if total_images == 0:
            images_summary += "–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n"
        else:
            for img in images_list:
                alt_emoji = "üü¢" if img['alt'] else "üî¥"
                title_emoji = "üü¢" if img['title'] else "üî¥"
                size_emoji = "üü¢" if img['size'] <= 300 else "üî¥"
                images_summary += f"–°—Å—ã–ª–∫–∞: {img['src']}\n"
                images_summary += f"Alt: {alt_emoji} {img['alt'] or '–ù–µ—Ç'}\n"
                images_summary += f"Title: {title_emoji} {img['title'] or '–ù–µ—Ç'}\n"
                images_summary += f"–†–∞–∑–º–µ—Ä: {size_emoji} {img['size']:.2f} –ö–ë\n\n"
        page.data['images_summary'] = images_summary

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        save_results(site_url, summary_area.value, full_summary)

    except Exception as e:
        summary_area.value = f"‚ùå –û—à–∏–±–∫–∞: {str(e)}\n"
        general_errors.append(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}")
        log_to_file(f"{site_url} - –û—à–∏–±–∫–∞: {str(e)}")
    finally:
        try:
            if 'driver' in locals() and driver:
                driver.quit()
        except Exception as e:
            log_to_file(f"–û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è WebDriver: {str(e)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        if stop_event and stop_event.is_set():
            summary_area.value = "‚èπ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"
        else:
            progress_bar.value = 1.0
        
        # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
        page.data['links_stop_btn_visible'] = False
        page.data['links_run_btn_visible'] = True
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ —á–µ—Ä–µ–∑ page.data
        page.data['links_export_btn_visible'] = True
        page.data['links_export_word_btn_visible'] = True
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
        page.update()

def run_multiple_links_test(urls: list, summary_area: ft.TextField, page: ft.Page, progress_bar: ft.ProgressBar, ignore_ssl: bool, target_keywords: str):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ robots –∏ sitemap."""
    if not urls:
        summary_area.value = "‚ùå –ù–µ—Ç —Å—Å—ã–ª–æ–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n"
        page.update()
        return

    all_results = []
    all_seo_summaries = []
    all_links_summaries = []
    all_images_summaries = []
    all_full_summaries = []
    total_urls = len(urls)
    current_url = 0
    
    summary_area.value = f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É {total_urls} —Å—Å—ã–ª–æ–∫...\n"
    page.update()

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    max_urls_per_batch = 1000  # –ú–∞–∫—Å–∏–º—É–º 1000 —Å—Å—ã–ª–æ–∫ –∑–∞ —Ä–∞–∑
    if total_urls > max_urls_per_batch:
        summary_area.value += f"‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ {max_urls_per_batch} —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n"
        urls = urls[:max_urls_per_batch]
        total_urls = len(urls)

    for url in urls:
        url = url.strip()
        if not url:
            continue
            
        current_url += 1
        summary_area.value += f"\nüìã –ü—Ä–æ–≤–µ—Ä—è–µ–º {current_url}/{total_urls}: {url}\n"
        progress_bar.value = current_url / total_urls
        page.update()
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –æ–±–ª–∞—Å—Ç—å –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–¥–Ω–æ–π —Å—Å—ã–ª–∫–∏
            temp_summary = ft.TextField()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –æ–¥–Ω–æ–π —Å—Å—ã–ª–∫–∏ —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –ª–∏–º–∏—Ç–æ–º
            run_links_test(url, temp_summary, page, progress_bar, ignore_ssl, target_keywords, 15000)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if hasattr(temp_summary, 'value') and temp_summary.value:
                all_results.append({
                    'url': url,
                    'summary': temp_summary.value
                })
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–Ω–æ–ø–æ–∫
                if 'seo_summary' in page.data:
                    all_seo_summaries.append(f"## üîó {url}\n\n{page.data['seo_summary']}\n\n{'='*60}\n\n")
                if 'links_summary' in page.data:
                    all_links_summaries.append(f"## üîó {url}\n\n{page.data['links_summary']}\n\n{'='*60}\n\n")
                if 'images_summary' in page.data:
                    all_images_summaries.append(f"## üîó {url}\n\n{page.data['images_summary']}\n\n{'='*60}\n\n")
                if 'full_summary' in page.data:
                    all_full_summaries.append(f"## üîó {url}\n\n{page.data['full_summary']}\n\n{'='*60}\n\n")
                
        except Exception as e:
            all_results.append({
                'url': url,
                'summary': f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {str(e)}"
            })
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–Ω–æ–ø–æ–∫
    if all_seo_summaries:
        page.data['multiple_seo_summary'] = "# üìä SEO –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤\n\n" + "".join(all_seo_summaries)
    if all_links_summaries:
        page.data['multiple_links_summary'] = "# üìä –ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤\n\n" + "".join(all_links_summaries)
    if all_images_summaries:
        page.data['multiple_images_summary'] = "# üìä –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤\n\n" + "".join(all_images_summaries)
    if all_full_summaries:
        page.data['multiple_full_summary'] = "# üìä –û–±—â–∞—è —Å–≤–æ–¥–∫–∞ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤\n\n" + "".join(all_full_summaries)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—â—É—é —Å–≤–æ–¥–∫—É
    combined_summary = f"# üìä –°–≤–æ–¥–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {total_urls} —Å—Å—ã–ª–æ–∫\n\n"
    combined_summary += f"**–í—Ä–µ–º—è –ø—Ä–æ–≤–µ—Ä–∫–∏:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S %Z')}\n\n"
    
    for i, result in enumerate(all_results, 1):
        combined_summary += f"## üîó {i}. {result['url']}\n\n"
        combined_summary += result['summary']
        combined_summary += "\n" + "="*80 + "\n\n"
    
    summary_area.value = combined_summary
    page.data['multiple_results'] = all_results
    progress_bar.value = 1.0
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ —á–µ—Ä–µ–∑ page.data
    page.data['links_export_btn_visible'] = True
    page.data['links_export_word_btn_visible'] = True
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    page.update()

def run_robots_check(site_url: str, ignore_ssl: bool, page: ft.Page, robots_area: ft.TextField, summary_area: ft.TextField):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É robots.txt."""
    summary = check_robots_summary(site_url, ignore_ssl)
    robots_area.value = summary
    summary_area.value = summary
    page.update()

def run_sitemap_check(site_url: str, ignore_ssl: bool, page: ft.Page, sitemap_area: ft.TextField, summary_area: ft.TextField):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É sitemap.xml."""
    summary = check_sitemap_summary(site_url, ignore_ssl)
    sitemap_area.value = summary
    summary_area.value = summary
    page.update()

def generate_report(summary, site_url, report_type='full', format='txt'):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –≤ TXT –∏–ª–∏ Excel."""
    report_path = save_results(site_url, '', summary, report_type, format)
    return report_path

def generate_sitemap_excel_report(site_url, ignore_ssl):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Excel –æ—Ç—á–µ—Ç —Å–æ –≤—Å–µ–º–∏ URL –∏–∑ sitemap."""
    global sitemap_export_data
    
    if not sitemap_export_data:
        # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É sitemap
        check_sitemap_summary(site_url, ignore_ssl)
    
    if not sitemap_export_data or not sitemap_export_data.get('urls'):
        return None
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = f"{REPORT_DIR}/sitemap_full_report_{timestamp}.xlsx"
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è Excel
    data = {
        'URL': [],
        '–°—Ç–∞—Ç—É—Å': [],
        '–ò—Å—Ç–æ—á–Ω–∏–∫ Sitemap': [],
        'Last Modified': [],
        'Priority': [],
        'Change Frequency': []
    }
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ URL –∏ –∏—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    urls = sitemap_export_data['urls']
    page_details = sitemap_export_data['page_details']
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    details_dict = {detail['url']: detail for detail in page_details}
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ URL –≤ –æ—Ç—á–µ—Ç
    for url in urls:
        detail = details_dict.get(url, {})
        data['URL'].append(url)
        data['–°—Ç–∞—Ç—É—Å'].append(detail.get('status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'))
        data['–ò—Å—Ç–æ—á–Ω–∏–∫ Sitemap'].append(detail.get('source_sitemap', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'))
        data['Last Modified'].append(detail.get('lastmod', '-'))
        data['Priority'].append(detail.get('priority', '-'))
        data['Change Frequency'].append(detail.get('changefreq', '-'))
    
    # –°–æ–∑–¥–∞–µ–º DataFrame –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
    df = pd.DataFrame(data)
    
    # –°–æ–∑–¥–∞–µ–º Excel —Ñ–∞–π–ª —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ª–∏—Å—Ç–∞–º–∏
    with pd.ExcelWriter(report_path, engine='openpyxl') as writer:
        # –û—Å–Ω–æ–≤–Ω–æ–π –ª–∏—Å—Ç —Å–æ –≤—Å–µ–º–∏ URL
        df.to_excel(writer, sheet_name='–í—Å–µ URL', index=False)
        
        # –õ–∏—Å—Ç —Ç–æ–ª—å–∫–æ —Å –ø—Ä–æ–±–ª–µ–º–Ω—ã–º–∏ URL
        broken_df = df[df['–°—Ç–∞—Ç—É—Å'] != '–û–ö'].copy()
        if not broken_df.empty:
            broken_df.to_excel(writer, sheet_name='–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ URL', index=False)
        
        # –õ–∏—Å—Ç —Ç–æ–ª—å–∫–æ —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ URL
        working_df = df[df['–°—Ç–∞—Ç—É—Å'] == '–û–ö'].copy()
        if not working_df.empty:
            working_df.to_excel(writer, sheet_name='–î–æ—Å—Ç—É–ø–Ω—ã–µ URL', index=False)
        
        # –õ–∏—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –Ω–∞ —Å–∞–π—Ç–µ, –Ω–æ –Ω–µ –≤ sitemap
        pages_not_in_sitemap = sitemap_export_data.get('pages_not_in_sitemap', [])
        if pages_not_in_sitemap:
            not_in_sitemap_df = pd.DataFrame({
                'URL': pages_not_in_sitemap,
                '–°—Ç–∞—Ç—É—Å': ['–ù–∞ —Å–∞–π—Ç–µ, –Ω–æ –Ω–µ –≤ sitemap'] * len(pages_not_in_sitemap)
            })
            not_in_sitemap_df.to_excel(writer, sheet_name='–°—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ –≤ sitemap', index=False)
        
        # –õ–∏—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –≤ sitemap, –Ω–æ –Ω–µ –Ω–∞ —Å–∞–π—Ç–µ
        pages_in_sitemap_not_on_site = sitemap_export_data.get('pages_in_sitemap_not_on_site', [])
        if pages_in_sitemap_not_on_site:
            not_on_site_df = pd.DataFrame({
                'URL': pages_in_sitemap_not_on_site,
                '–°—Ç–∞—Ç—É—Å': ['–í sitemap, –Ω–æ –Ω–µ –Ω–∞ —Å–∞–π—Ç–µ'] * len(pages_in_sitemap_not_on_site)
            })
            not_on_site_df.to_excel(writer, sheet_name='–°—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ –Ω–∞ —Å–∞–π—Ç–µ', index=False)
    
    return report_path

def generate_sitemap_word_report(site_url, ignore_ssl):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Word –æ—Ç—á–µ—Ç —Å–æ –≤—Å–µ–º–∏ URL –∏–∑ sitemap."""
    global sitemap_export_data
    
    if not Document:
        return None
    
    if not sitemap_export_data:
        # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É sitemap
        check_sitemap_summary(site_url, ignore_ssl)
    
    if not sitemap_export_data or not sitemap_export_data.get('urls'):
        return None
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = f"{REPORT_DIR}/sitemap_full_report_{timestamp}.docx"
    
    doc = Document()
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title = doc.add_heading(f'–ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç Sitemap –¥–ª—è {site_url}', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–µ
    doc.add_paragraph(f"–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    doc.add_paragraph(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ URL: {len(sitemap_export_data['urls'])}")
    doc.add_paragraph("="*50)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ sitemap
    sitemap_info = sitemap_export_data.get('sitemap_info', {})
    if sitemap_info.get('type') == 'sitemapindex':
        doc.add_heading('–¢–∏–ø Sitemap: Sitemap Index', level=1)
        if sitemap_info.get('sub_sitemaps'):
            doc.add_paragraph(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥—á–∏–Ω–µ–Ω–Ω—ã—Ö sitemap: {len(sitemap_info['sub_sitemaps'])}")
    else:
        doc.add_heading('–¢–∏–ø Sitemap: –û–±—ã—á–Ω—ã–π Sitemap', level=1)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    doc.add_heading('–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', level=1)
    doc.add_paragraph(f"–í—Å–µ–≥–æ URL: {len(sitemap_export_data['urls'])}")
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å—ã
    status_counts = {}
    for detail in sitemap_export_data['page_details']:
        status = detail.get('status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        status_counts[status] = status_counts.get(status, 0) + 1
    
    for status, count in status_counts.items():
        doc.add_paragraph(f"–°—Ç–∞—Ç—É—Å '{status}': {count} URL")
    
    # –°–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö URL
    broken_urls = [detail for detail in sitemap_export_data['page_details'] if detail.get('status') != '–û–ö']
    if broken_urls:
        doc.add_heading('–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ URL', level=1)
        doc.add_paragraph(f"–ù–∞–π–¥–µ–Ω–æ {len(broken_urls)} –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö URL:")
        
        for i, detail in enumerate(broken_urls, 1):
            doc.add_paragraph(f"{i}. {detail['url']} - {detail.get('status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
    
    # –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö URL
    working_urls = [detail for detail in sitemap_export_data['page_details'] if detail.get('status') == '–û–ö']
    if working_urls:
        doc.add_heading('–î–æ—Å—Ç—É–ø–Ω—ã–µ URL', level=1)
        doc.add_paragraph(f"–ù–∞–π–¥–µ–Ω–æ {len(working_urls)} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö URL:")
        
        for i, detail in enumerate(working_urls, 1):
            doc.add_paragraph(f"{i}. {detail['url']}")
    
    # –°—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Å–∞–π—Ç–µ, –Ω–æ –Ω–µ –≤ sitemap
    pages_not_in_sitemap = sitemap_export_data.get('pages_not_in_sitemap', [])
    if pages_not_in_sitemap:
        doc.add_heading('–°—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Å–∞–π—Ç–µ, –Ω–æ –Ω–µ –≤ sitemap', level=1)
        doc.add_paragraph(f"–ù–∞–π–¥–µ–Ω–æ {len(pages_not_in_sitemap)} —Å—Ç—Ä–∞–Ω–∏—Ü:")
        
        for i, url in enumerate(pages_not_in_sitemap, 1):
            doc.add_paragraph(f"{i}. {url}")
    
    # –°—Ç—Ä–∞–Ω–∏—Ü—ã –≤ sitemap, –Ω–æ –Ω–µ –Ω–∞ —Å–∞–π—Ç–µ
    pages_in_sitemap_not_on_site = sitemap_export_data.get('pages_in_sitemap_not_on_site', [])
    if pages_in_sitemap_not_on_site:
        doc.add_heading('–°—Ç—Ä–∞–Ω–∏—Ü—ã –≤ sitemap, –Ω–æ –Ω–µ –Ω–∞ —Å–∞–π—Ç–µ', level=1)
        doc.add_paragraph(f"–ù–∞–π–¥–µ–Ω–æ {len(pages_in_sitemap_not_on_site)} —Å—Ç—Ä–∞–Ω–∏—Ü:")
        
        for i, url in enumerate(pages_in_sitemap_not_on_site, 1):
            doc.add_paragraph(f"{i}. {url}")

    # –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ URL
    doc.add_heading('–ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ URL', level=1)
    
    urls = sitemap_export_data['urls']
    page_details = sitemap_export_data['page_details']
    details_dict = {detail['url']: detail for detail in page_details}
    
    for i, url in enumerate(urls, 1):
        detail = details_dict.get(url, {})
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ URL
        doc.add_heading(f'{i}. {url}', level=2)
        
        # –¢–∞–±–ª–∏—Ü–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        table = doc.add_table(rows=1, cols=2)
        table.style = 'Table Grid'
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
        hdr_cells = table.rows[0].cells
        hdr_cells[0].text = '–ü–∞—Ä–∞–º–µ—Ç—Ä'
        hdr_cells[1].text = '–ó–Ω–∞—á–µ–Ω–∏–µ'
        
        # –î–∞–Ω–Ω—ã–µ
        data_rows = [
            ('–°—Ç–∞—Ç—É—Å', detail.get('status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')),
            ('–ò—Å—Ç–æ—á–Ω–∏–∫ Sitemap', detail.get('source_sitemap', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')),
            ('Last Modified', detail.get('lastmod', '-')),
            ('Priority', detail.get('priority', '-')),
            ('Change Frequency', detail.get('changefreq', '-'))
        ]
        
        for param, value in data_rows:
            row_cells = table.add_row().cells
            row_cells[0].text = param
            row_cells[1].text = str(value)
        
        doc.add_paragraph()  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É URL
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
    doc.save(report_path)
    return report_path

def generate_word_report(data, site_url, report_type='parser'):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –≤ Word —Ñ–æ—Ä–º–∞—Ç–µ."""
    if not Document:
        return None
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = f"reports/{report_type}_report_{timestamp}.docx"
    
    doc = Document()
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title = doc.add_heading(f'SEO –û—Ç—á–µ—Ç –¥–ª—è {site_url}', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–µ
    doc.add_paragraph(f"–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    doc.add_paragraph(f"–¢–∏–ø –æ—Ç—á–µ—Ç–∞: {report_type}")
    doc.add_paragraph("="*50)
    
    if report_type == 'parser':
        if isinstance(data, list):
            # –û—Ç—á–µ—Ç –ø–∞—Ä—Å–µ—Ä–∞
            doc.add_heading('–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞', level=1)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
            if not data:
                doc.add_paragraph("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–µ—Ä–∞.")
                doc.save(report_path)
                return report_path
            
            doc.add_paragraph(f"–ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(data)}")
            doc.add_paragraph()
            
            for i, item in enumerate(data, 1):
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                doc.add_heading(f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {i}: {item["–°—Å—ã–ª–∫–∞"]}', level=2)
                
                # –¢–∞–±–ª–∏—Ü–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                table = doc.add_table(rows=1, cols=2)
                table.style = 'Table Grid'
                
                # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
                hdr_cells = table.rows[0].cells
                hdr_cells[0].text = '–ü–∞—Ä–∞–º–µ—Ç—Ä'
                hdr_cells[1].text = '–ó–Ω–∞—á–µ–Ω–∏–µ'
                
                # –î–∞–Ω–Ω—ã–µ
                data_rows = [
                    ('HTTP –°—Ç–∞—Ç—É—Å', str(item['HTTP'])),
                    ('–†–µ–¥–∏—Ä–µ–∫—Ç', item['–†–µ–¥–∏—Ä–µ–∫—Ç'] if item['–†–µ–¥–∏—Ä–µ–∫—Ç'] else '–ù–µ—Ç'),
                    ('SEO –°—Ç–∞—Ç—É—Å', item['SEO']),
                    ('Title', item['Title'] if item['Title'] else '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'),
                    ('Meta Description', item['Meta_Description'] if item['Meta_Description'] else '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'),
                    ('H1', item['H1'] if item['H1'] else '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'),
                ]
                
                for param, value in data_rows:
                    row_cells = table.add_row().cells
                    row_cells[0].text = param
                    row_cells[1].text = value
                
                doc.add_paragraph()  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
        else:
            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏—Ö–æ–¥—è—Ç –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏ (summary_content)
            doc.add_heading('–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞', level=1)
            doc.add_paragraph("–î–∞–Ω–Ω—ã–µ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ:")
            doc.add_paragraph(data)
    else:
        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç (–¥–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤)
        doc.add_heading('SEO –ê–Ω–∞–ª–∏–∑', level=1)
        doc.add_paragraph(data)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
    doc.save(report_path)
    return report_path

def crawl_site_without_sitemap(start_url, ignore_ssl, update_callback, done_callback, stop_event, max_threads=20, max_pages=15000):
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏—Ç –≤—Å–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–∞–π—Ç–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è sitemap."""
    visited = set()
    results = []
    lock = threading.Lock()
    domain = urlparse(start_url).netloc
    queue = [start_url]
    threads = []

    def check_redirect(url, ignore_ssl):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ."""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –¥–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –±–µ–∑ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π —Å—Ç–∞—Ç—É—Å
            r = requests.get(url, timeout=5, verify=not ignore_ssl, allow_redirects=False)
            if r.status_code in [301, 302, 303, 307, 308]:
                redirect_url = r.headers.get('Location', '')
                return f"{r.status_code} ‚Üí {redirect_url}"
            return ""
        except Exception:
            return ""

    def analyze_seo_basic(soup, url):
        """–ë–∞–∑–æ–≤—ã–π SEO –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        title = soup.title.string.strip() if soup.title and soup.title.string else ''
        h1 = soup.find('h1').text.strip() if soup.find('h1') else ''
        meta_desc = ''
        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})
        if meta_desc_tag:
            meta_desc = meta_desc_tag.get('content', '')
        
        seo_score = 0
        seo_issues = []
        
        if title:
            seo_score += 1
        else:
            seo_issues.append("–ù–µ—Ç title")
            
        if h1:
            seo_score += 1
        else:
            seo_issues.append("–ù–µ—Ç H1")
            
        if meta_desc:
            seo_score += 1
        else:
            seo_issues.append("–ù–µ—Ç meta description")
        
        if seo_score == 3:
            return "‚úÖ –û—Ç–ª–∏—á–Ω–æ", "–í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ SEO —ç–ª–µ–º–µ–Ω—Ç—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç"
        elif seo_score == 2:
            return "‚ö†Ô∏è –•–æ—Ä–æ—à–æ", f"–ü—Ä–æ–±–ª–µ–º—ã: {', '.join(seo_issues)}"
        elif seo_score == 1:
            return "‚ùå –ü–ª–æ—Ö–æ", f"–ü—Ä–æ–±–ª–µ–º—ã: {', '.join(seo_issues)}"
        else:
            return "‚ùå –ö—Ä–∏—Ç–∏—á–Ω–æ", f"–ü—Ä–æ–±–ª–µ–º—ã: {', '.join(seo_issues)}"

    def worker():
        while True:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            if stop_event.is_set():
                return
                
            with lock:
                if not queue or len(visited) >= max_pages:
                    return
                url = queue.pop(0)
                if url in visited:
                    continue
                visited.add(url)
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
                if stop_event.is_set():
                    return
                    
                # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                r = requests.get(url, timeout=5, verify=not ignore_ssl, allow_redirects=True)
                status = r.status_code
                soup = BeautifulSoup(r.text, 'html.parser')
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã
                redirect_info = check_redirect(url, ignore_ssl)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º SEO
                seo_status, seo_details = analyze_seo_basic(soup, url)
                
                results.append({
                    '–°—Å—ã–ª–∫–∞': url,
                    'HTTP': status,
                    '–†–µ–¥–∏—Ä–µ–∫—Ç': redirect_info,
                    'SEO': seo_status,
                    'SEO_Details': seo_details,
                    'Title': soup.title.string.strip() if soup.title and soup.title.string else '',
                    'H1': soup.find('h1').text.strip() if soup.find('h1') else '',
                    'Meta_Description': soup.find('meta', attrs={'name': 'description'}).get('content', '') if soup.find('meta', attrs={'name': 'description'}) else ''
                })
                
                # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏
                for a in soup.find_all('a', href=True):
                    link = urljoin(url, a['href'])
                    parsed = urlparse(link)
                    if parsed.netloc == domain and link not in visited and link.startswith('http'):
                        with lock:
                            if link not in queue:
                                queue.append(link)
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä
                time.sleep(0.05)  # 50ms –∑–∞–¥–µ—Ä–∂–∫–∞
                
                update_callback(len(visited), len(results))
            except Exception as e:
                results.append({
                    '–°—Å—ã–ª–∫–∞': url,
                    'HTTP': f"–û—à–∏–±–∫–∞: {str(e)}",
                    '–†–µ–¥–∏—Ä–µ–∫—Ç': '',
                    'SEO': '‚ùå –û—à–∏–±–∫–∞',
                    'SEO_Details': f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}',
                    'Title': '',
                    'H1': '',
                    'Meta_Description': ''
                })
                update_callback(len(visited), len(results))

    for _ in range(max_threads):
        t = threading.Thread(target=worker)
        t.start()
        threads.append(t)
    for t in threads:
        t.join()
    done_callback(results)

def analyze_text_content(html_content, url):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è SEO —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Selenium –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–¥–æ —É–¥–∞–ª–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
    soup_for_headers = BeautifulSoup(html_content, 'html.parser')
    
    # –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–¥–æ —É–¥–∞–ª–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
    h1_tags = soup_for_headers.find_all('h1')
    h2_tags = soup_for_headers.find_all('h2')
    h3_tags = soup_for_headers.find_all('h3')
    h4_tags = soup_for_headers.find_all('h4')
    h5_tags = soup_for_headers.find_all('h5')
    h6_tags = soup_for_headers.find_all('h6')
    
    h1_texts = [h.get_text(strip=True) for h in h1_tags]
    h2_texts = [h.get_text(strip=True) for h in h2_tags]
    h3_texts = [h.get_text(strip=True) for h in h3_tags]
    
    # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤
    title = soup.title.string.strip() if soup.title and soup.title.string else ''
    meta_desc = ''
    meta_keywords = ''
    
    meta_desc_tag = soup.find('meta', attrs={'name': 'description'})
    if meta_desc_tag:
        meta_desc = meta_desc_tag.get('content', '')
    
    meta_keywords_tag = soup.find('meta', attrs={'name': 'keywords'})
    if meta_keywords_tag:
        meta_keywords = meta_keywords_tag.get('content', '')
    
    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Selenium (–∫–∞–∫ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ —Å–∫–ª–æ–Ω–µ–Ω–∏–π)
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å –≤–∏–¥–∏–º—ã–π —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ —É–ª—É—á—à–µ–Ω–Ω—ã–π JavaScript —Å —Ç–æ—á–Ω—ã–º –ø–æ–¥—Å—á–µ—Ç–æ–º
        full_text = driver.execute_script("""
            function getAllVisibleText() {
                // –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
                let allText = '';
                let wordCount = 0;
                let processedNodes = new Set();
                
                // –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –æ–±—Ö–æ–¥–∞ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å —Ç–æ—á–Ω—ã–º –ø–æ–¥—Å—á–µ—Ç–æ–º
                function walkTextNodes(node) {
                    // –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —É–∑–ª–æ–≤
                    if (processedNodes.has(node)) {
                        return;
                    }
                    processedNodes.add(node);
                    
                    if (node.nodeType === Node.TEXT_NODE) {
                        // –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤ —Å —Ç–æ—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
                        let text = node.textContent.trim();
                        if (text && text.length > 0) {
                            // –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                            text = text.replace(/\\s+/g, ' ').trim();
                            if (text) {
                                allText += text + ' ';
                                // –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–ª–æ–≤–∞ —Ç–æ—á–Ω–æ
                                let words = text.split(/\\s+/).filter(word => word.length >= 3);
                                wordCount += words.length;
                            }
                        }
                    } else if (node.nodeType === Node.ELEMENT_NODE) {
                        // –ü—Ä–æ–ø—É—Å–∫–∞–µ–º header –∏ footer
                        if (node.tagName === 'HEADER' || node.tagName === 'FOOTER') {
                            return;
                        }
                        
                        // –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫—Ä—ã—Ç—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                        const style = window.getComputedStyle(node);
                        if (style.display === 'none' || style.visibility === 'hidden' || style.opacity === '0') {
                            return;
                        }
                        
                        // –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                        if (node.tagName === 'SCRIPT' || node.tagName === 'STYLE' || node.tagName === 'NOSCRIPT') {
                            return;
                        }
                        
                        // –ù–ï –ø–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ (alt, title, placeholder, aria-label)
                        // –ò—Å–∫–ª—é—á–∞–µ–º —ç—Ç–∏ –∞—Ç—Ä–∏–±—É—Ç—ã –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é
                        
                        // –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏–º –¥–æ—á–µ—Ä–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                        for (let child of node.childNodes) {
                            walkTextNodes(child);
                        }
                    }
                }
                
                // –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
                function waitForDynamicContent() {
                    return new Promise((resolve) => {
                        let attempts = 0;
                        const maxAttempts = 3;
                        
                        function tryExtract() {
                            attempts++;
                            
                            // –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ
                            allText = '';
                            wordCount = 0;
                            processedNodes.clear();
                            
                            // –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ö–æ–¥ —Å body
                            walkTextNodes(document.body);
                            
                            // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                            if (allText.trim().length > 100 || attempts >= maxAttempts) {
                                resolve({
                                    text: allText,
                                    wordCount: wordCount,
                                    attempts: attempts
                                });
                            } else {
                                // –ñ–¥–µ–º –µ—â–µ –∏ –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞
                                setTimeout(tryExtract, 500);
                            }
                        }
                        
                        setTimeout(tryExtract, 1000);
                    });
                }
                
                return waitForDynamicContent();
            }
            return getAllVisibleText();
        """)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–¥—Å—á–µ—Ç–µ
        if isinstance(full_text, dict):
            text_data = full_text
            full_text = text_data.get('text', '')
            selenium_word_count = text_data.get('wordCount', 0)
            attempts = text_data.get('attempts', 1)
        else:
            selenium_word_count = 0
            attempts = 1
        
        driver.quit()
        
        # –û—á–∏—â–∞–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        text = re.sub(r'\s+', ' ', full_text).strip()
        
    except Exception as e:
        # –ï—Å–ª–∏ Selenium –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
        # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ —Å–ª—É–∂–µ–±–Ω—ã—Ö –±–ª–æ–∫–æ–≤
        # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –±–ª–æ–∫–∏
        elements_to_remove = []
        
        # –ù–∞—Ö–æ–¥–∏–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        for element in soup.find_all():
            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            if element.name in ['script', 'style', 'noscript']:
                elements_to_remove.append(element)
                continue
                
            # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
            if element.name in ['nav']:
                elements_to_remove.append(element)
                continue
                
            # –£–¥–∞–ª—è–µ–º —Ñ—É—Ç–µ—Ä—ã –∏ —Ö–µ–¥–µ—Ä—ã (–Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ —è–≤–Ω–æ –ø–æ–º–µ—á–µ–Ω—ã)
            if element.name in ['footer', 'header']:
                elements_to_remove.append(element)
                continue
                
            # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–∞–º–∏, —É–∫–∞–∑—ã–≤–∞—é—â–∏–º–∏ –Ω–∞ –Ω–∞–≤–∏–≥–∞—Ü–∏—é/—Å–ª—É–∂–µ–±–Ω—ã–µ –±–ª–æ–∫–∏
            classes = element.get('class', [])
            if isinstance(classes, str):
                classes = [classes]
            
            navigation_classes = [
                'nav', 'navigation', 'menu', 'header', 'footer', 'sidebar', 
                'breadcrumb', 'breadcrumbs', 'pagination', 'pager',
                'social', 'socials', 'share', 'sharing', 'widget', 'widgets',
                'advertisement', 'ad', 'ads', 'banner', 'banners',
                'cookie', 'cookies', 'popup', 'modal', 'overlay',
                'search', 'search-form', 'searchbox', 'search-box',
                'login', 'signin', 'register', 'signup', 'auth',
                'cart', 'basket', 'checkout', 'order',
                'newsletter', 'subscribe', 'subscription',
                'toolbar', 'toolbar-top', 'toolbar-bottom',
                'announcement', 'notice', 'alert', 'notification'
            ]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª–∞—Å—Å—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            if any(nav_class in ' '.join(classes).lower() for nav_class in navigation_classes):
                elements_to_remove.append(element)
                continue
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º ID –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            element_id = element.get('id', '').lower()
            if any(nav_class in element_id for nav_class in navigation_classes):
                elements_to_remove.append(element)
                continue
        
        # –£–¥–∞–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in elements_to_remove:
            if element.parent:
                element.decompose()
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        text = soup.get_text(separator=' ', strip=True)
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
        text = re.sub(r'\s+', ' ', text).strip()
    
    # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç)
    # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç HTML —Ç–µ–≥–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
    clean_text = re.sub(r'<[^>]+>', '', text)  # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
    clean_text = re.sub(r'javascript:', '', clean_text, flags=re.IGNORECASE)
    clean_text = re.sub(r'http[s]?://[^\s]+', '', clean_text)  # –£–¥–∞–ª—è–µ–º URL
    clean_text = re.sub(r'www\.[^\s]+', '', clean_text)
    clean_text = re.sub(r'[^\w\s–∞-—è—ë]', ' ', clean_text)  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ –ø—Ä–æ–±–µ–ª—ã
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å–ª–æ–≤–∞ (—Ä—É—Å—Å–∫–∏–µ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ) —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–º –ø–æ–¥—Å—á–µ—Ç–æ–º
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π regex –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç–∏
    words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', clean_text.lower())
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    words = [word.strip() for word in words if word.strip() and len(word.strip()) >= 3]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–¥—Å—á–µ—Ç–∞ —Å Selenium
    if 'selenium_word_count' in locals() and selenium_word_count > 0:
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ–¥—Å—á–µ—Ç—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        python_word_count = len(words)
        if abs(python_word_count - selenium_word_count) > 2:
            # –ï—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –±–æ–ª—å—à–∞—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –º–µ—Ç–æ–¥
            print(f"‚ö†Ô∏è –†–∞–∑–Ω–∏—Ü–∞ –≤ –ø–æ–¥—Å—á–µ—Ç–µ: Python={python_word_count}, Selenium={selenium_word_count}")
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            words = [word for word in words if re.match(r'^[–∞-—è—ëa-z]{3,}$', word)]
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä—É—Å—Å–∫–∏—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤
    russian_stop_words = {
        '—ç—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '–∏', '–≤', '–Ω–∞–¥', '–∫', '–¥–æ', '–Ω–µ', '–Ω–∞', '–Ω–æ', '–∑–∞', '—Ç–æ', '—Å', '–ª–∏',
        '–∞', '–≤–æ', '–æ—Ç', '—Å–æ', '–¥–ª—è', '–æ', '–∂–µ', '–Ω—É', '–≤—ã', '–±—ã', '—á—Ç–æ', '–∫—Ç–æ', '–æ–Ω', '–æ–Ω–∞',
        '–∏', '–≤', '–Ω–∞', '–Ω–µ', '—Å', '–∞', '–æ', '–¥–ª—è', '–ø–æ', '–∏–∑', '–∫', '—É', '–æ—Ç', '–Ω–æ', '–∫–∞–∫',
        '—á—Ç–æ', '—ç—Ç–æ', '—Ç–æ', '–∏–ª–∏', '–∑–∞', '–ø—Ä–∏', '–¥–∞', '–Ω–æ', '–∂–µ', '–±—ã', '–ª–∏', '–±—ã—Ç—å', '–±—ã–ª',
        '–±—ã–ª–∞', '–±—ã–ª–∏', '–±—ã–ª–æ', '–µ—Å—Ç—å', '–±—ã—Ç—å', '–º–æ–π', '–º–æ—è', '–º–æ–∏', '—Ç–≤–æ–π', '—Ç–≤–æ—è', '—Ç–≤–æ–∏',
        '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–∏', '–≤–∞—à', '–≤–∞—à–∞', '–≤–∞—à–∏', '–µ–≥–æ', '–µ–µ', '–∏—Ö', '—Å–µ–±—è', '—Å–µ–±–µ',
        '—Å–µ–±—è', '–º–Ω–µ', '—Ç–µ–±–µ', '–µ–º—É', '–µ–π', '–Ω–∞–º', '–≤–∞–º', '–∏–º', '–º–µ–Ω—è', '—Ç–µ–±—è', '–µ–≥–æ', '–µ–µ',
        '–Ω–∞—Å', '–≤–∞—Å', '–Ω–∏—Ö', '–º–Ω–æ–π', '—Ç–æ–±–æ–π', '–∏–º', '–µ–π', '–Ω–∞–º–∏', '–≤–∞–º–∏', '–∏–º–∏', '–º–æ–π', '—Ç–≤–æ–π',
        '—Å–≤–æ–π', '–Ω–∞—à', '–≤–∞—à', '–µ–≥–æ', '–µ–µ', '–∏—Ö', '—ç—Ç–æ—Ç', '—Ç–æ—Ç', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–∞—è', '—Ç–∞–∫–æ–µ',
        '—Ç–∞–∫–∏–µ', '—Å—Ç–æ–ª—å–∫–æ', '—Å–∫–æ–ª—å–∫–æ', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–µ', '–∫–æ—Ç–æ—Ä—ã–µ', '–∫—Ç–æ',
        '—á—Ç–æ', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–æ–µ', '–∫–∞–∫–∏–µ', '—á–µ–π', '—á—å—è', '—á—å–µ', '—á—å–∏', '–≥–¥–µ', '–∫—É–¥–∞',
        '–æ—Ç–∫—É–¥–∞', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫–∞–∫', '—Å–∫–æ–ª—å–∫–æ', '–Ω–∞—Å–∫–æ–ª—å–∫–æ', '—Å—Ç–æ–ª—å–∫–æ',
        '—Ç–∞–∫–æ–π', '—Ç–∞–∫–∞—è', '—Ç–∞–∫–æ–µ', '—Ç–∞–∫–∏–µ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–æ', '—ç—Ç–∏', '—Ç–æ—Ç', '—Ç–∞', '—Ç–æ',
        '—Ç–µ', '—Å–∞–º', '—Å–∞–º–∞', '—Å–∞–º–æ', '—Å–∞–º–∏', '—Å–∞–º—ã–π', '—Å–∞–º–∞—è', '—Å–∞–º–æ–µ', '—Å–∞–º—ã–µ', '–≤–µ—Å—å',
        '–≤—Å—è', '–≤—Å–µ', '–≤—Å–µ', '–∫–∞–∂–¥—ã–π', '–∫–∞–∂–¥–∞—è', '–∫–∞–∂–¥–æ–µ', '–∫–∞–∂–¥—ã–µ', '–ª—é–±–æ–π', '–ª—é–±–∞—è',
        '–ª—é–±–æ–µ', '–ª—é–±—ã–µ', '–Ω–∏–∫–∞–∫–æ–π', '–Ω–∏–∫–∞–∫–∞—è', '–Ω–∏–∫–∞–∫–æ–µ', '–Ω–∏–∫–∞–∫–∏–µ', '–Ω–µ–∫–æ—Ç–æ—Ä—ã–π',
        '–Ω–µ–∫–æ—Ç–æ—Ä–∞—è', '–Ω–µ–∫–æ—Ç–æ—Ä–æ–µ', '–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ', '–≤—Å—è–∫–∏–π', '–≤—Å—è–∫–∞—è', '–≤—Å—è–∫–æ–µ', '–≤—Å—è–∫–∏–µ',
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
        'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
        'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall'
    }
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
    filtered_words = [word for word in words if word not in russian_stop_words and len(word) > 2]
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–º –ø–æ–¥—Å—á–µ—Ç–æ–º
    word_freq = Counter(filtered_words)
    top_keywords = word_freq.most_common(20)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ–¥—Å—á–µ—Ç–∞
    total_words_counted = len(filtered_words)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ–¥—Å—á–µ—Ç–∞ —Å Selenium
    if 'selenium_word_count' in locals() and selenium_word_count > 0:
        selenium_diff = abs(total_words_counted - selenium_word_count)
        if selenium_diff > 5:
            print(f"‚ö†Ô∏è –ë–æ–ª—å—à–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –ø–æ–¥—Å—á–µ—Ç–µ: Python={total_words_counted}, Selenium={selenium_word_count}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
            filtered_words = [word for word in filtered_words if re.match(r'^[–∞-—è—ëa-z]{3,}$', word) and word not in russian_stop_words]
            word_freq = Counter(filtered_words)
            top_keywords = word_freq.most_common(20)
            total_words_counted = len(filtered_words)
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    sentences = text.split('.')
    sentences = [s.strip() for s in sentences if s.strip()]
    avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0
    
    # –ê–Ω–∞–ª–∏–∑ –∞–±–∑–∞—Ü–µ–≤ (–∏–∑ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ soup)
    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p') if p.get_text(strip=True)]
    avg_paragraph_length = sum(len(p.split()) for p in paragraphs) / len(paragraphs) if paragraphs else 0
    
    # –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ soup)
    images = soup_for_headers.find_all('img')
    images_with_alt = [img for img in images if img.get('alt')]
    images_without_alt = [img for img in images if not img.get('alt')]
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ (–∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ soup)
    links = soup_for_headers.find_all('a', href=True)
    internal_links = [link for link in links if link['href'].startswith('/') or url in link['href']]
    external_links = [link for link in links if link['href'].startswith('http') and url not in link['href']]
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    structure_score = 0
    if len(h1_tags) == 1:
        structure_score += 20
    elif len(h1_tags) > 1:
        structure_score -= 10 * (len(h1_tags) - 1)
    
    if len(h2_tags) >= 2:
        structure_score += 15
    if len(h3_tags) >= 3:
        structure_score += 10
    
    # –ê–Ω–∞–ª–∏–∑ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–π)
    total_words = total_words_counted if 'total_words_counted' in locals() else len(filtered_words)
    keyword_density = {}
    for keyword, count in top_keywords[:20]:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 20 –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        density = (count / total_words) * 100 if total_words > 0 else 0
        keyword_density[keyword] = round(density, 2)
    
    # SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    recommendations = []
    
    if len(h1_tags) == 0:
        recommendations.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç H1 –∑–∞–≥–æ–ª–æ–≤–æ–∫")
    elif len(h1_tags) > 1:
        recommendations.append(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {len(h1_tags)} H1 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω)")
    
    if len(h2_tags) < 2:
        recommendations.append("‚ö†Ô∏è –ú–∞–ª–æ H2 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    
    if not meta_desc:
        recommendations.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç meta description")
    elif len(meta_desc) < 120:
        recommendations.append("‚ö†Ô∏è Meta description —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–µ–Ω–µ–µ 120 —Å–∏–º–≤–æ–ª–æ–≤)")
    elif len(meta_desc) > 160:
        recommendations.append("‚ö†Ô∏è Meta description —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–±–æ–ª–µ–µ 160 —Å–∏–º–≤–æ–ª–æ–≤)")
    
    if not title:
        recommendations.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç title")
    elif len(title) < 30:
        recommendations.append("‚ö†Ô∏è Title —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–µ–Ω–µ–µ 30 —Å–∏–º–≤–æ–ª–æ–≤)")
    elif len(title) > 60:
        recommendations.append("‚ö†Ô∏è Title —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–±–æ–ª–µ–µ 60 —Å–∏–º–≤–æ–ª–æ–≤)")
    
    if len(images_without_alt) > 0:
        recommendations.append(f"‚ö†Ô∏è {len(images_without_alt)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ alt –∞—Ç—Ä–∏–±—É—Ç–∞")
    
    if avg_sentence_length > 25:
        recommendations.append("‚ö†Ô∏è –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ (–≤ —Å—Ä–µ–¥–Ω–µ–º –±–æ–ª–µ–µ 25 —Å–ª–æ–≤)")
    
    if avg_paragraph_length > 150:
        recommendations.append("‚ö†Ô∏è –ê–±–∑–∞—Ü—ã —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ (–≤ —Å—Ä–µ–¥–Ω–µ–º –±–æ–ª–µ–µ 150 —Å–ª–æ–≤)")
    
    # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã
    positives = []
    
    if len(h1_tags) == 1:
        positives.append("‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ H1 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤")
    
    if len(h2_tags) >= 2:
        positives.append(f"‚úÖ –•–æ—Ä–æ—à–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å {len(h2_tags)} H2 –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏")
    
    if meta_desc and 120 <= len(meta_desc) <= 160:
        positives.append("‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ meta description")
    
    if title and 30 <= len(title) <= 60:
        positives.append("‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ title")
    
    if len(images_with_alt) > 0:
        positives.append(f"‚úÖ {len(images_with_alt)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å alt –∞—Ç—Ä–∏–±—É—Ç–æ–º")
    
    if len(internal_links) >= 3:
        positives.append(f"‚úÖ –•–æ—Ä–æ—à–∞—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∞ ({len(internal_links)} —Å—Å—ã–ª–æ–∫)")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –≤—ã–≤–æ–¥–∞
    full_text = text
    
    return {
        'url': url,
        'title': title,
        'meta_description': meta_desc,
        'meta_keywords': meta_keywords,
        'h1_count': len(h1_tags),
        'h2_count': len(h2_tags),
        'h3_count': len(h3_tags),
        'h1_texts': h1_texts,
        'h2_texts': h2_texts,
        'h3_texts': h3_texts,
        'total_words': total_words,
        'avg_sentence_length': round(avg_sentence_length, 1),
        'avg_paragraph_length': round(avg_paragraph_length, 1),
        'top_keywords': top_keywords,
        'keyword_density': keyword_density,
        'images_total': len(images),
        'images_with_alt': len(images_with_alt),
        'images_without_alt': len(images_without_alt),
        'internal_links': len(internal_links),
        'external_links': len(external_links),
        'structure_score': structure_score,
        'recommendations': recommendations,
        'positives': positives,
        'text_preview': text[:500] + '...' if len(text) > 500 else text,
        'full_text': full_text  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
    }

def analyze_code_content(html_content, url):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–æ–∫ –≤ HTML, CSS, JS, PHP —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π."""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    errors = []
    warnings = []
    positives = []
    
    # === HTML –ê–ù–ê–õ–ò–ó ===
    html_errors = []
    html_warnings = []
    html_stats = {}
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ DOCTYPE
    doctype_match = re.search(r'<!DOCTYPE[^>]*>', html_content, re.IGNORECASE)
    if not doctype_match:
        html_errors.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç DOCTYPE")
    else:
        doctype = doctype_match.group(0)
        positives.append(f"‚úÖ DOCTYPE: {doctype}")
        html_stats['doctype'] = doctype
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤
    if not soup.html:
        html_errors.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–≥ <html>")
    else:
        html_stats['html_lang'] = soup.html.get('lang', '–Ω–µ —É–∫–∞–∑–∞–Ω')
        if soup.html.get('lang'):
            positives.append(f"‚úÖ HTML lang: {soup.html.get('lang')}")
        else:
            html_warnings.append("‚ö†Ô∏è –ù–µ —É–∫–∞–∑–∞–Ω –∞—Ç—Ä–∏–±—É—Ç lang –≤ —Ç–µ–≥–µ <html>")
    
    if not soup.head:
        html_errors.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–≥ <head>")
    else:
        positives.append("‚úÖ –¢–µ–≥ <head> –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
    
    if not soup.body:
        html_errors.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–≥ <body>")
    else:
        positives.append("‚úÖ –¢–µ–≥ <body> –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
    
    if not soup.title:
        html_errors.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–≥ <title>")
    else:
        title_text = soup.title.string.strip() if soup.title.string else ""
        if title_text:
            positives.append(f"‚úÖ Title: {title_text[:50]}{'...' if len(title_text) > 50 else ''}")
            html_stats['title_length'] = len(title_text)
        else:
            html_warnings.append("‚ö†Ô∏è –¢–µ–≥ <title> –ø—É—Å—Ç–æ–π")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤
    all_tags = soup.find_all()
    tag_counts = {}
    for tag in all_tags:
        tag_name = tag.name
        tag_counts[tag_name] = tag_counts.get(tag_name, 0) + 1
    
    # –¢–æ–ø-10 —Å–∞–º—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ç–µ–≥–æ–≤
    top_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    html_stats['total_tags'] = len(all_tags)
    html_stats['unique_tags'] = len(tag_counts)
    html_stats['top_tags'] = top_tags
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–∫—Ä—ã—Ç—ã—Ö —Ç–µ–≥–æ–≤
    unclosed_tags = []
    for tag in soup.find_all():
        if tag.name in ['img', 'br', 'hr', 'input', 'meta', 'link', 'area', 'base', 'col', 'embed', 'source', 'track', 'wbr']:
            continue  # –û–¥–∏–Ω–æ—á–Ω—ã–µ —Ç–µ–≥–∏
        if not tag.string and not tag.find_all():
            unclosed_tags.append(tag.name)
    
    if unclosed_tags:
        html_warnings.append(f"‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–µ —Ç–µ–≥–∏: {', '.join(set(unclosed_tags))}")
        html_stats['unclosed_tags'] = list(set(unclosed_tags))
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ alt –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    images_without_alt = soup.find_all('img', alt=lambda x: not x or x.strip() == '')
    total_images = len(soup.find_all('img'))
    if images_without_alt:
        alt_coverage = ((total_images - len(images_without_alt)) / total_images * 100) if total_images > 0 else 0
        html_warnings.append(f"‚ö†Ô∏è {len(images_without_alt)} –∏–∑ {total_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ alt –∞—Ç—Ä–∏–±—É—Ç–∞ ({alt_coverage:.1f}% –ø–æ–∫—Ä—ã—Ç–∏–µ)")
        html_stats['images_without_alt'] = len(images_without_alt)
        html_stats['total_images'] = total_images
        html_stats['alt_coverage'] = alt_coverage
    else:
        if total_images > 0:
            positives.append(f"‚úÖ –í—Å–µ {total_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–º–µ—é—Ç alt –∞—Ç—Ä–∏–±—É—Ç")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Å—Å—ã–ª–æ–∫
    all_links = soup.find_all('a', href=True)
    invalid_links = []
    external_links = []
    internal_links = []
    broken_links = []
    
    for link in all_links:
        href = link['href']
        if href.startswith('javascript:') or href.startswith('mailto:') or href.startswith('tel:'):
            continue
        elif href.startswith('http'):
            if not href.startswith('http://') and not href.startswith('https://'):
                invalid_links.append(href)
            else:
                external_links.append(href)
        elif href.startswith('/') or href.startswith('./') or href.startswith('../'):
            internal_links.append(href)
        elif href.startswith('#'):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —è–∫–æ—Ä–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
            anchor_id = href[1:]
            if anchor_id and not soup.find(id=anchor_id):
                broken_links.append(href)
    
    html_stats['total_links'] = len(all_links)
    html_stats['external_links'] = len(external_links)
    html_stats['internal_links'] = len(internal_links)
    html_stats['broken_links'] = len(broken_links)
    
    if invalid_links:
        html_warnings.append(f"‚ö†Ô∏è –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏: {', '.join(invalid_links[:3])}")
    if broken_links:
        html_warnings.append(f"‚ö†Ô∏è –°–ª–æ–º–∞–Ω–Ω—ã–µ —è–∫–æ—Ä–Ω—ã–µ —Å—Å—ã–ª–∫–∏: {', '.join(broken_links[:3])}")
    
    positives.append(f"‚úÖ –°—Å—ã–ª–æ–∫: {len(all_links)} (–≤–Ω–µ—à–Ω–∏—Ö: {len(external_links)}, –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö: {len(internal_links)})")
    
    # === CSS –ê–ù–ê–õ–ò–ó ===
    css_errors = []
    css_warnings = []
    css_stats = {}
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ CSS –∏–∑ —Ç–µ–≥–æ–≤ style
    style_tags = soup.find_all('style')
    inline_styles = soup.find_all(style=True)
    external_css = soup.find_all('link', rel='stylesheet')
    
    css_code = '\n'.join([tag.string for tag in style_tags if tag.string])
    css_code += '\n'.join([tag['style'] for tag in inline_styles if tag.get('style')])
    
    css_stats['style_blocks'] = len(style_tags)
    css_stats['inline_styles'] = len(inline_styles)
    css_stats['external_css'] = len(external_css)
    
    if css_code:
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ CSS
        css_lines = css_code.split('\n')
        css_stats['total_css_lines'] = len(css_lines)
        
        # –ü–æ–¥—Å—á–µ—Ç CSS –ø—Ä–∞–≤–∏–ª
        css_rules = re.findall(r'[^{}]+{', css_code)
        css_stats['css_rules'] = len(css_rules)
        
        # –ü–æ–∏—Å–∫ –º–µ–¥–∏–∞-–∑–∞–ø—Ä–æ—Å–æ–≤
        media_queries = re.findall(r'@media[^{]+{', css_code, re.IGNORECASE)
        css_stats['media_queries'] = len(media_queries)
        
        # –ü–æ–∏—Å–∫ –∞–Ω–∏–º–∞—Ü–∏–π –∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤
        animations = re.findall(r'@keyframes[^{]+{', css_code, re.IGNORECASE)
        transitions = re.findall(r'transition:', css_code, re.IGNORECASE)
        css_stats['animations'] = len(animations)
        css_stats['transitions'] = len(transitions)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ CSS
        bracket_errors = 0
        semicolon_errors = 0
        
        for i, line in enumerate(css_lines, 1):
            line = line.strip()
            if not line or line.startswith('/*') or line.startswith('//'):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã—Ö —Å–∫–æ–±–æ–∫
            if line.count('{') != line.count('}'):
                css_warnings.append(f"‚ö†Ô∏è –°—Ç—Ä–æ–∫–∞ {i}: –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∫–æ–±–∫–∏ –≤ CSS")
                bracket_errors += 1
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ—á–µ–∫ —Å –∑–∞–ø—è—Ç–æ–π
            if ':' in line and not line.strip().endswith(';') and not line.strip().endswith('{') and not line.strip().endswith('}'):
                css_warnings.append(f"‚ö†Ô∏è –°—Ç—Ä–æ–∫–∞ {i}: –í–æ–∑–º–æ–∂–Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π")
                semicolon_errors += 1
        
        css_stats['bracket_errors'] = bracket_errors
        css_stats['semicolon_errors'] = semicolon_errors
        
        positives.append(f"‚úÖ CSS: {len(style_tags)} –±–ª–æ–∫–æ–≤, {len(css_rules)} –ø—Ä–∞–≤–∏–ª, {len(media_queries)} –º–µ–¥–∏–∞-–∑–∞–ø—Ä–æ—Å–æ–≤")
        
        if animations:
            positives.append(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(animations)} –∞–Ω–∏–º–∞—Ü–∏–π")
        if transitions:
            positives.append(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(transitions)} –ø–µ—Ä–µ—Ö–æ–¥–æ–≤")
    else:
        css_warnings.append("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö —Å—Ç–∏–ª–µ–π")
    
    if external_css:
        positives.append(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ {len(external_css)} –≤–Ω–µ—à–Ω–∏—Ö CSS —Ñ–∞–π–ª–æ–≤")
    
    # === JAVASCRIPT –ê–ù–ê–õ–ò–ó ===
    js_errors = []
    js_warnings = []
    js_stats = {}
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ JavaScript
    script_tags = soup.find_all('script')
    inline_scripts = [tag.string for tag in script_tags if tag.string]
    external_scripts = [tag.get('src') for tag in script_tags if tag.get('src')]
    
    js_stats['script_blocks'] = len(script_tags)
    js_stats['inline_scripts'] = len(inline_scripts)
    js_stats['external_scripts'] = len(external_scripts)
    
    if inline_scripts:
        js_code = '\n'.join(inline_scripts)
        js_stats['total_js_lines'] = len(js_code.split('\n'))
        
        # –ü–æ–∏—Å–∫ —Ñ—É–Ω–∫—Ü–∏–π
        functions = re.findall(r'function\s+\w+\s*\(', js_code)
        arrow_functions = re.findall(r'const\s+\w+\s*=\s*\([^)]*\)\s*=>', js_code)
        js_stats['functions'] = len(functions)
        js_stats['arrow_functions'] = len(arrow_functions)
        
        # –ü–æ–∏—Å–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        vars = re.findall(r'\bvar\s+\w+', js_code)
        lets = re.findall(r'\blet\s+\w+', js_code)
        consts = re.findall(r'\bconst\s+\w+', js_code)
        js_stats['var_declarations'] = len(vars)
        js_stats['let_declarations'] = len(lets)
        js_stats['const_declarations'] = len(consts)
        
        # –ü–æ–∏—Å–∫ console.log
        console_logs = re.findall(r'console\.log', js_code)
        js_stats['console_logs'] = len(console_logs)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ JavaScript
        bracket_errors = 0
        semicolon_errors = 0
        
        js_lines = js_code.split('\n')
        for i, line in enumerate(js_lines, 1):
            line = line.strip()
            if not line or line.startswith('//') or line.startswith('/*'):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã—Ö —Å–∫–æ–±–æ–∫
            if line.count('(') != line.count(')'):
                js_warnings.append(f"‚ö†Ô∏è –°—Ç—Ä–æ–∫–∞ {i}: –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∫–æ–±–∫–∏ –≤ JS")
                bracket_errors += 1
            
            if line.count('{') != line.count('}'):
                js_warnings.append(f"‚ö†Ô∏è –°—Ç—Ä–æ–∫–∞ {i}: –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–≥—É—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏ –≤ JS")
                bracket_errors += 1
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ—á–µ–∫ —Å –∑–∞–ø—è—Ç–æ–π
            if line and not line.endswith(';') and not line.endswith('{') and not line.endswith('}') and not line.endswith('('):
                if any(keyword in line for keyword in ['var ', 'let ', 'const ', 'return', 'console.log']):
                    js_warnings.append(f"‚ö†Ô∏è –°—Ç—Ä–æ–∫–∞ {i}: –í–æ–∑–º–æ–∂–Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π")
                    semicolon_errors += 1
        
        js_stats['bracket_errors'] = bracket_errors
        js_stats['semicolon_errors'] = semicolon_errors
        
        positives.append(f"‚úÖ JS: {len(inline_scripts)} –±–ª–æ–∫–æ–≤, {len(functions)} —Ñ—É–Ω–∫—Ü–∏–π, {len(vars + lets + consts)} –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
        
        if console_logs:
            js_warnings.append(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {len(console_logs)} console.log (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–±—Ä–∞—Ç—å –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ)")
    
    if external_scripts:
        positives.append(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ {len(external_scripts)} –≤–Ω–µ—à–Ω–∏—Ö JS —Ñ–∞–π–ª–æ–≤")
    
    # === PHP –ê–ù–ê–õ–ò–ó ===
    php_errors = []
    php_warnings = []
    php_stats = {}
    
    # –ü–æ–∏—Å–∫ PHP –∫–æ–¥–∞ –≤ HTML
    php_patterns = [
        r'<\?php.*?\?>',
        r'<\?=.*?\?>',
        r'<\?.*?\?>'
    ]
    
    php_code_found = False
    php_blocks = []
    
    for pattern in php_patterns:
        matches = re.findall(pattern, html_content, re.DOTALL | re.IGNORECASE)
        if matches:
            php_code_found = True
            php_blocks.extend(matches)
            
            for match in matches:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ PHP
                php_lines = match.split('\n')
                for i, line in enumerate(php_lines, 1):
                    line = line.strip()
                    if not line or line.startswith('//') or line.startswith('#'):
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã—Ö —Å–∫–æ–±–æ–∫
                    if line.count('(') != line.count(')'):
                        php_warnings.append(f"‚ö†Ô∏è PHP: –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∫–æ–±–∫–∏")
                    
                    if line.count('{') != line.count('}'):
                        php_warnings.append(f"‚ö†Ô∏è PHP: –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–≥—É—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏")
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ—á–µ–∫ —Å –∑–∞–ø—è—Ç–æ–π
                    if line and not line.endswith(';') and not line.endswith('{') and not line.endswith('}') and not line.endswith('('):
                        if any(keyword in line for keyword in ['echo', 'return', '$', 'function']):
                            php_warnings.append(f"‚ö†Ô∏è PHP: –í–æ–∑–º–æ–∂–Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π")
    
    php_stats['php_blocks'] = len(php_blocks)
    php_stats['total_php_lines'] = sum(len(block.split('\n')) for block in php_blocks)
    
    if php_code_found:
        positives.append(f"‚úÖ PHP: {len(php_blocks)} –±–ª–æ–∫–æ–≤, {php_stats['total_php_lines']} —Å—Ç—Ä–æ–∫")
    else:
        php_warnings.append("‚ÑπÔ∏è PHP –∫–æ–¥ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")
    
    # === SEO –ò –ú–ï–¢–ê-–¢–ï–ì–ò –ê–ù–ê–õ–ò–ó ===
    seo_errors = []
    seo_warnings = []
    seo_stats = {}
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
    meta_charset = soup.find('meta', charset=True)
    meta_content_type = soup.find('meta', attrs={'http-equiv': 'Content-Type'})
    
    if not meta_charset and not meta_content_type:
        seo_warnings.append("‚ö†Ô∏è –ù–µ —É–∫–∞–∑–∞–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
    else:
        charset = meta_charset.get('charset') if meta_charset else meta_content_type.get('content', '').split('charset=')[-1]
        positives.append(f"‚úÖ –ö–æ–¥–∏—Ä–æ–≤–∫–∞: {charset}")
        seo_stats['charset'] = charset
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ viewport
    viewport = soup.find('meta', attrs={'name': 'viewport'})
    if not viewport:
        seo_warnings.append("‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç viewport meta —Ç–µ–≥")
    else:
        positives.append("‚úÖ Viewport meta —Ç–µ–≥ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        seo_stats['viewport'] = viewport.get('content', '')
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ favicon
    favicon = soup.find('link', rel='icon') or soup.find('link', rel='shortcut icon')
    if not favicon:
        seo_warnings.append("‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç favicon")
    else:
        positives.append("‚úÖ Favicon –ø–æ–¥–∫–ª—é—á–µ–Ω")
        seo_stats['favicon'] = favicon.get('href', '')
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ meta description
    meta_description = soup.find('meta', attrs={'name': 'description'})
    if not meta_description:
        seo_warnings.append("‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç meta description")
    else:
        desc_text = meta_description.get('content', '')
        if desc_text:
            positives.append(f"‚úÖ Meta description: {desc_text[:50]}{'...' if len(desc_text) > 50 else ''}")
            seo_stats['description_length'] = len(desc_text)
        else:
            seo_warnings.append("‚ö†Ô∏è Meta description –ø—É—Å—Ç–æ–π")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ meta keywords
    meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
    if meta_keywords:
        keywords_text = meta_keywords.get('content', '')
        seo_stats['keywords'] = keywords_text
        positives.append("‚úÖ Meta keywords –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Open Graph
    og_tags = soup.find_all('meta', attrs={'property': re.compile(r'^og:')})
    if og_tags:
        positives.append(f"‚úÖ Open Graph —Ç–µ–≥–∏: {len(og_tags)}")
        seo_stats['og_tags'] = len(og_tags)
    else:
        seo_warnings.append("‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç Open Graph —Ç–µ–≥–∏")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Twitter Cards
    twitter_tags = soup.find_all('meta', attrs={'name': re.compile(r'^twitter:')})
    if twitter_tags:
        positives.append(f"‚úÖ Twitter Cards: {len(twitter_tags)}")
        seo_stats['twitter_tags'] = len(twitter_tags)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ canonical
    canonical = soup.find('link', attrs={'rel': 'canonical'})
    if canonical:
        positives.append("‚úÖ Canonical URL —É–∫–∞–∑–∞–Ω")
        seo_stats['canonical'] = canonical.get('href', '')
    else:
        seo_warnings.append("‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç canonical URL")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ robots
    robots = soup.find('meta', attrs={'name': 'robots'})
    if robots:
        positives.append("‚úÖ Robots meta —Ç–µ–≥ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        seo_stats['robots'] = robots.get('content', '')
    
    # === –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨ ===
    performance_stats = {}
    
    # –ü–æ–¥—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–æ–≤
    performance_stats['html_size_kb'] = len(html_content) / 1024
    performance_stats['css_size_kb'] = len(css_code) / 1024 if css_code else 0
    performance_stats['js_size_kb'] = len(js_code) / 1024 if 'js_code' in locals() else 0
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    if performance_stats['html_size_kb'] > 100:
        seo_warnings.append("‚ö†Ô∏è HTML —Ä–∞–∑–º–µ—Ä –ø—Ä–µ–≤—ã—à–∞–µ—Ç 100KB")
    if performance_stats['css_size_kb'] > 50:
        seo_warnings.append("‚ö†Ô∏è CSS —Ä–∞–∑–º–µ—Ä –ø—Ä–µ–≤—ã—à–∞–µ—Ç 50KB")
    if performance_stats['js_size_kb'] > 100:
        seo_warnings.append("‚ö†Ô∏è JavaScript —Ä–∞–∑–º–µ—Ä –ø—Ä–µ–≤—ã—à–∞–µ—Ç 100KB")
    
    # === –û–ë–©–ò–ï –ü–†–û–í–ï–†–ö–ò ===
    
    # –ü–æ–¥—Å—á–µ—Ç –æ—à–∏–±–æ–∫ –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
    total_errors = len(html_errors) + len(css_errors) + len(js_errors) + len(php_errors) + len(seo_errors)
    total_warnings = len(html_warnings) + len(css_warnings) + len(js_warnings) + len(php_warnings) + len(seo_warnings)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞
    quality_score = 100
    quality_score -= total_errors * 10
    quality_score -= total_warnings * 2
    
    # –ë–æ–Ω—É—Å—ã –∑–∞ —Ö–æ—Ä–æ—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
    if html_stats.get('alt_coverage', 0) == 100:
        quality_score += 5
    if seo_stats.get('description_length', 0) > 0:
        quality_score += 3
    if seo_stats.get('og_tags', 0) > 0:
        quality_score += 2
    if canonical:
        quality_score += 2
    
    quality_score = max(0, min(100, quality_score))
    
    return {
        'url': url,
        'html_errors': html_errors,
        'html_warnings': html_warnings,
        'css_errors': css_errors,
        'css_warnings': css_warnings,
        'js_errors': js_errors,
        'js_warnings': js_warnings,
        'php_errors': php_errors,
        'php_warnings': php_warnings,
        'seo_errors': seo_errors,
        'seo_warnings': seo_warnings,
        'positives': positives,
        'total_errors': total_errors,
        'total_warnings': total_warnings,
        'quality_score': quality_score,
        'html_stats': html_stats,
        'css_stats': css_stats,
        'js_stats': js_stats,
        'php_stats': php_stats,
        'seo_stats': seo_stats,
        'performance_stats': performance_stats,
        'style_blocks': len(style_tags),
        'script_blocks': len(script_tags),
        'external_scripts': len(external_scripts),
        'inline_styles': len(inline_styles),
        'images_without_alt': len(images_without_alt),
        'unclosed_tags': len(set(unclosed_tags)) if unclosed_tags else 0
    }

def check_redirects(urls, ignore_ssl, update_callback, done_callback):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã –¥–ª—è —Å–ø–∏—Å–∫–∞ URL."""
    import requests
    import threading
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    results = []
    lock = threading.Lock()
    
    def check_single_url(url):
        try:
            r = requests.get(url, timeout=5, verify=not ignore_ssl, allow_redirects=True)
            chain = [resp.url for resp in r.history] + [r.url] if r.history else [r.url]
            status = r.status_code
            redirected = len(chain) > 1
            return {
                '–ò—Å—Ö–æ–¥–Ω—ã–π URL': url,
                '–†–µ–¥–∏—Ä–µ–∫—Ç': '‚úÖ' if redirected else '‚ùå',
                '–ö–æ–Ω–µ—á–Ω—ã–π URL': chain[-1],
                'HTTP': status,
                'OK': status in (200, 301, 302)
            }
        except Exception as e:
            return {
                '–ò—Å—Ö–æ–¥–Ω—ã–π URL': url,
                '–†–µ–¥–∏—Ä–µ–∫—Ç': '‚ùå',
                '–ö–æ–Ω–µ—á–Ω—ã–π URL': str(e),
                'HTTP': '-',
                'OK': False
            }
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    max_workers = min(20, len(urls))  # –ú–∞–∫—Å–∏–º—É–º 20 –ø–æ—Ç–æ–∫–æ–≤
    completed = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {executor.submit(check_single_url, url): url for url in urls}
        
        for future in as_completed(future_to_url):
            result = future.result()
            with lock:
                results.append(result)
                completed += 1
                update_callback(completed, len(urls))
    
    done_callback(results)

def main(page: ft.Page):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ Flet —Å –±–æ–∫–æ–≤—ã–º –º–µ–Ω—é."""
    page.title = "üîç SEO –ê–≤—Ç–æ—Ç–µ—Å—Ç–µ—Ä"
    page.window_favicon = "assets/favicon.png"
    page.scroll = ft.ScrollMode.AUTO
    page.window.width = 1200
    page.window.height = 800
    page.theme_mode = ft.ThemeMode.DARK
    page.theme = ft.Theme(
        color_scheme_seed="#394459",
        use_material3=True,
    )
    page.data = {}

    # --- –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ü–≤–µ—Ç–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–µ–º—ã ---
    def get_text_color():
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ü–≤–µ—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π."""
        return "#FFFFFF"  # –ë–µ–ª—ã–π —Ü–≤–µ—Ç –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–π
    
    def get_label_color():
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ü–≤–µ—Ç –ª–µ–π–±–ª–æ–≤ –¥–ª—è –ø–æ–ª–µ–π –≤–≤–æ–¥–∞."""
        return "#394459"  # –¢–µ–º–Ω—ã–π —Ü–≤–µ—Ç –¥–ª—è –ª–µ–π–±–ª–æ–≤ –ø–æ–ª–µ–π –≤–≤–æ–¥–∞
    
    def get_secondary_text_color():
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ü–≤–µ—Ç –≤—Ç–æ—Ä–∏—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        return "#FFFFFF"  # –ë–µ–ª—ã–π —Ü–≤–µ—Ç –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏–π
    
    def get_input_text_color():
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ü–≤–µ—Ç —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ–ª—è—Ö –≤–≤–æ–¥–∞."""
        return "#394459"  # –¢–µ–º–Ω—ã–π —Ü–≤–µ—Ç –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ–ª—è—Ö –≤–≤–æ–¥–∞

    # --- Top Navigation Menu ---
    def nav_home(e):
        switch_page(0)
    
    def nav_links_check(e):
        switch_page(1)
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤–∏–¥–∏–º–æ—Å—Ç—å –∫–Ω–æ–ø–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–∏ –Ω–∞ –≤–∫–ª–∞–¥–∫—É
        update_links_export_buttons()
    
    def nav_parser(e):
        switch_page(2)
    
    def nav_text_analysis(e):
        switch_page(3)
    
    def nav_code_analysis(e):
        switch_page(4)
    
    def nav_redirects(e):
        switch_page(5)
    
    def nav_competitors(e):
        switch_page(6)
    
    def nav_exports(e):
        switch_page(7)
    
    def nav_serp_tracker(e):
        switch_page(8)
    
    nav_buttons = [
        ft.ElevatedButton(
            "üè† –ì–ª–∞–≤–Ω–∞—è", 
            on_click=nav_home,
            style=ft.ButtonStyle(
                bgcolor="#F2F2F2",
                color="#394459",
                shape=ft.RoundedRectangleBorder(radius=10),
                elevation=3
            )
        ),
        ft.ElevatedButton(
            "üîó –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫", 
            on_click=nav_links_check,
            style=ft.ButtonStyle(
                bgcolor="#F2F2F2",
                color="#394459",
                shape=ft.RoundedRectangleBorder(radius=10),
                elevation=3
            )
        ),
        ft.ElevatedButton(
            "üìã –ü–∞—Ä—Å–µ—Ä –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü", 
            on_click=nav_parser,
            style=ft.ButtonStyle(
                bgcolor="#F2F2F2",
                color="#394459",
                shape=ft.RoundedRectangleBorder(radius=10),
                elevation=3
            )
        ),
        ft.ElevatedButton(
            "üìù –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞", 
            on_click=nav_text_analysis,
            style=ft.ButtonStyle(
                bgcolor="#F2F2F2",
                color="#394459",
                shape=ft.RoundedRectangleBorder(radius=10),
                elevation=3
            )
        ),
        ft.ElevatedButton(
            "üíª –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞", 
            on_click=nav_code_analysis,
            style=ft.ButtonStyle(
                bgcolor="#F2F2F2",
                color="#394459",
                shape=ft.RoundedRectangleBorder(radius=10),
                elevation=3
            )
        ),
        ft.ElevatedButton(
            "üîÑ –†–µ–¥–∏—Ä–µ–∫—Ç—ã", 
            on_click=nav_redirects,
            style=ft.ButtonStyle(
                bgcolor="#F2F2F2",
                color="#394459",
                shape=ft.RoundedRectangleBorder(radius=10),
                elevation=3
            )
        ),
        ft.ElevatedButton(
            "üë• –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤", 
            on_click=nav_competitors,
            style=ft.ButtonStyle(
                bgcolor="#F2F2F2",
                color="#394459",
                shape=ft.RoundedRectangleBorder(radius=10),
                elevation=3
            )
        ),
        ft.ElevatedButton(
            "üìÅ –≠–∫—Å–ø–æ—Ä—Ç", 
            on_click=nav_exports,
            style=ft.ButtonStyle(
                bgcolor="#F2F2F2",
                color="#394459",
                shape=ft.RoundedRectangleBorder(radius=10),
                elevation=3
            )
        ),
        ft.ElevatedButton(
            "üìä SERP Tracker", 
            on_click=nav_serp_tracker,
            style=ft.ButtonStyle(
                bgcolor="#F2F2F2",
                color="#394459",
                shape=ft.RoundedRectangleBorder(radius=10),
                elevation=3
            )
        ),
    ]
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
    navigation_bar = ft.Container(
        content=ft.Row(
            nav_buttons,
            alignment=ft.MainAxisAlignment.CENTER,
            spacing=10
        ),
        padding=15,
        bgcolor="#394459",
        border_radius=15,
        margin=ft.Margin(0, 0, 0, 20),
        border=ft.border.all(2, "#F2E307")
    )

    # --- –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü ---
    main_content = ft.Container(visible=True)
    links_check_content = ft.Container(visible=False)
    parser_content = ft.Container(visible=False)
    text_analysis_content = ft.Container(visible=False)
    code_analysis_content = ft.Container(visible=False)
    redirects_content = ft.Container(visible=False)
    competitors_content = ft.Container(visible=False)
    exports_content = ft.Container(visible=False)
    serp_tracker_content = ft.Container(visible=False)

    def switch_page(idx):
        # –°–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        main_content.visible = False
        links_check_content.visible = False
        parser_content.visible = False
        text_analysis_content.visible = False
        code_analysis_content.visible = False
        redirects_content.visible = False
        competitors_content.visible = False
        exports_content.visible = False
        serp_tracker_content.visible = False
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∏–ª–∏ –≤—Å–µ—Ö –∫–Ω–æ–ø–æ–∫
        for btn in nav_buttons:
            btn.style = None
            btn.bgcolor = None
        
        # –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º –∞–∫—Ç–∏–≤–Ω—É—é –∫–Ω–æ–ø–∫—É
        if 0 <= idx < len(nav_buttons):
            nav_buttons[idx].bgcolor = "#F2E307"
            nav_buttons[idx].color = "#394459"
            nav_buttons[idx].style = ft.ButtonStyle(
                shape=ft.RoundedRectangleBorder(radius=10),
                elevation=5
            )
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω—É–∂–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        if idx == 0:
            main_content.visible = True
            # –ü—Ä–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–∏ –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
            if not page.data:
                summary_area.value = ""
                export_btn.visible = False
                export_word_btn.visible = False
                progress_bar.value = 0.0
        elif idx == 1:
            links_check_content.visible = True
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–∏–¥–∏–º–æ—Å—Ç—å –∫–Ω–æ–ø–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–∏ –Ω–∞ –≤–∫–ª–∞–¥–∫—É –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–æ–∫
            update_links_export_buttons()
        elif idx == 2:
            parser_content.visible = True
            parser_status.value = "–ù–∞–∂–º–∏—Ç–µ '–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–µ—Ä', —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞"
            parser_status.visible = True
            parser_table.rows = []
            parser_export_btn.visible = False
        elif idx == 3:
            text_analysis_content.visible = True
        elif idx == 4:
            code_analysis_content.visible = True
        elif idx == 5:
            redirects_content.visible = True
        elif idx == 6:
            competitors_content.visible = True
        elif idx == 7:
            exports_content.visible = True
            refresh_exports_list()
        elif idx == 8:
            serp_tracker_content.visible = True
        
        page.update()

    # --- –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–º–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª —Å –æ–¥–Ω–æ–π —Å–≤–æ–¥–∫–æ–π –∏ –∫–Ω–æ–ø–∫–∞–º–∏) ---
    url_input = ft.TextField(
        label="URL —Å–∞–π—Ç–∞", 
        width=400, 
        filled=True, 
        border_radius=10,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color="#394459",
        label_style=ft.TextStyle(color="#394459")
    )
    ssl_checkbox = ft.Checkbox(label="–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å SSL", value=True)
    run_btn = ft.ElevatedButton(
        "–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç", 
        icon=ft.Icons.PLAY_ARROW,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    stop_btn = ft.ElevatedButton(
        "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", 
        icon=ft.Icons.STOP,
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#FF5722",
            color="white",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    progress_bar = ft.ProgressBar(width=600, color="#F2E307", bgcolor="#394459", value=0.0, height=10, border_radius=20)
    summary_area = ft.TextField(
        label="–°–≤–æ–¥–∫–∞", 
        multiline=True, 
        min_lines=16, 
        max_lines=30, 
        width=1000, 
        filled=True, 
        border_radius=10, 
        expand=True,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color="#394459",
        label_style=ft.TextStyle(color="#394459")
    )

    # –ö–Ω–æ–ø–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–≤–æ–¥–æ–∫
    seo_btn = ft.ElevatedButton(
        "SEO", 
        icon=ft.Icons.SEARCH,
        style=ft.ButtonStyle(
            bgcolor="#F2CC0C",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    robots_btn = ft.ElevatedButton(
        "Robots", 
        icon=ft.Icons.ANDROID,
        style=ft.ButtonStyle(
            bgcolor="#F2CC0C",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    sitemap_btn = ft.ElevatedButton(
        "Sitemap", 
        icon=ft.Icons.MAP,
        style=ft.ButtonStyle(
            bgcolor="#F2CC0C",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    links_btn = ft.ElevatedButton(
        "–°—Å—ã–ª–∫–∏", 
        icon=ft.Icons.LINK,
        style=ft.ButtonStyle(
            bgcolor="#F2CC0C",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    images_btn = ft.ElevatedButton(
        "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", 
        icon=ft.Icons.IMAGE,
        style=ft.ButtonStyle(
            bgcolor="#F2CC0C",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    full_btn = ft.ElevatedButton(
        "–û–±—â–∞—è —Å–≤–æ–¥–∫–∞", 
        icon=ft.Icons.ASSIGNMENT,
        style=ft.ButtonStyle(
            bgcolor="#F2CC0C",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    clear_btn = ft.ElevatedButton(
        "–û—á–∏—Å—Ç–∏—Ç—å —Å–≤–æ–¥–∫—É", 
        icon=ft.Icons.CLEAR, 
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    export_btn = ft.ElevatedButton(
        "–≠–∫—Å–ø–æ—Ä—Ç –≤ Excel", 
        icon=ft.Icons.DOWNLOAD, 
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    export_word_btn = ft.ElevatedButton(
        "–≠–∫—Å–ø–æ—Ä—Ç –≤ Word", 
        icon=ft.Icons.DESCRIPTION, 
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    
    # –ö–Ω–æ–ø–∫–∏ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –ø–æ–ª–Ω–æ–≥–æ sitemap
    sitemap_full_excel_btn = ft.ElevatedButton(
        "üìä –ü–æ–ª–Ω—ã–π Sitemap Excel", 
        icon=ft.Icons.DOWNLOAD, 
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#4CAF50",
            color="white",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    sitemap_full_word_btn = ft.ElevatedButton(
        "üìÑ –ü–æ–ª–Ω—ã–π Sitemap Word", 
        icon=ft.Icons.DESCRIPTION, 
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#2196F3",
            color="white",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )

    # --- –°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–æ–∫ ---
    links_ssl_checkbox = ft.Checkbox(label="–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å SSL", value=True)
    links_multiple_input = ft.TextField(
        label="–°—Å—ã–ª–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–ø–æ –æ–¥–Ω–æ–π –Ω–∞ —Å—Ç—Ä–æ–∫—É)", 
        width=800, 
        filled=True, 
        border_radius=10,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color="#394459",
        label_style=ft.TextStyle(color="#394459"),
        multiline=True,
        min_lines=5,
        max_lines=12,
        hint_text="https://example.com\nhttps://example.com/page1\nhttps://example.com/page2\n\n–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–ø–æ –æ–¥–Ω–æ–π –Ω–∞ —Å—Ç—Ä–æ–∫—É)"
    )
    links_run_btn = ft.ElevatedButton(
        "–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É —Å—Å—ã–ª–æ–∫", 
        icon=ft.Icons.PLAY_ARROW,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    links_stop_btn = ft.ElevatedButton(
        "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", 
        icon=ft.Icons.STOP,
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#FF5722",
            color="white",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    links_progress_bar = ft.ProgressBar(width=600, color="#F2E307", bgcolor="#394459", value=0.0, height=10, border_radius=20)
    links_summary_area = ft.TextField(
        label="–°–≤–æ–¥–∫–∞", 
        multiline=True, 
        min_lines=16, 
        max_lines=30, 
        width=1000, 
        filled=True, 
        border_radius=10, 
        expand=True,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color="#394459",
        label_style=ft.TextStyle(color="#394459")
    )

    # –ö–Ω–æ–ø–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–≤–æ–¥–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å—Å—ã–ª–æ–∫
    links_seo_btn = ft.ElevatedButton(
        "SEO", 
        icon=ft.Icons.SEARCH,
        style=ft.ButtonStyle(
            bgcolor="#F2CC0C",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    links_links_btn = ft.ElevatedButton(
        "–°—Å—ã–ª–∫–∏", 
        icon=ft.Icons.LINK,
        style=ft.ButtonStyle(
            bgcolor="#F2CC0C",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    links_images_btn = ft.ElevatedButton(
        "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", 
        icon=ft.Icons.IMAGE,
        style=ft.ButtonStyle(
            bgcolor="#F2CC0C",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    links_full_btn = ft.ElevatedButton(
        "–û–±—â–∞—è —Å–≤–æ–¥–∫–∞", 
        icon=ft.Icons.ASSIGNMENT,
        style=ft.ButtonStyle(
            bgcolor="#F2CC0C",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    links_clear_btn = ft.ElevatedButton(
        "–û—á–∏—Å—Ç–∏—Ç—å —Å–≤–æ–¥–∫—É", 
        icon=ft.Icons.CLEAR, 
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    links_export_btn = ft.ElevatedButton(
        "–≠–∫—Å–ø–æ—Ä—Ç –≤ Excel", 
        icon=ft.Icons.DOWNLOAD, 
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    links_export_word_btn = ft.ElevatedButton(
        "–≠–∫—Å–ø–æ—Ä—Ç –≤ Word", 
        icon=ft.Icons.DESCRIPTION, 
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )



    main_content.content = ft.Column([
        ft.Text("üîç SEO –ê–≤—Ç–æ—Ç–µ—Å—Ç–µ—Ä", size=24, weight=ft.FontWeight.BOLD),
        ft.Text("–í–≤–µ–¥–∏—Ç–µ –∞–¥—Ä–µ—Å —Å–∞–π—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –Ω–∞–∂–º–∏—Ç–µ '–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç'", size=16),
        ft.Row([url_input, ssl_checkbox, run_btn, stop_btn], spacing=10),
        progress_bar,
        ft.Row([seo_btn, robots_btn, sitemap_btn, links_btn, images_btn, full_btn, clear_btn], spacing=10),
        summary_area,
        ft.Row([export_btn, export_word_btn]),
        ft.Row([sitemap_full_excel_btn, sitemap_full_word_btn])
    ], expand=True)

    links_check_content.content = ft.Column([
        ft.Text("üîó –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫", size=24, weight=ft.FontWeight.BOLD),
        ft.Text("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ –±–µ–∑ robots –∏ sitemap. –í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–ø–æ –æ–¥–Ω–æ–π –Ω–∞ —Å—Ç—Ä–æ–∫—É).", size=16),
        ft.Row([links_ssl_checkbox, links_run_btn, links_stop_btn], spacing=10),
        links_multiple_input,
        links_progress_bar,
        ft.Row([links_seo_btn, links_links_btn, links_images_btn, links_full_btn, links_clear_btn], spacing=10),
        links_summary_area,
        ft.Row([links_export_btn, links_export_word_btn])
    ], expand=True)

    # --- SERP Tracker UI —ç–ª–µ–º–µ–Ω—Ç—ã ---
    serp_domain_input = ft.TextField(
        label="–î–æ–º–µ–Ω —Å–∞–π—Ç–∞", 
        width=400, 
        filled=True, 
        border_radius=10,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color="#394459",
        label_style=ft.TextStyle(color="#394459"),
        hint_text="example.com"
    )
    
    serp_keywords_input = ft.TextField(
        label="–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)", 
        width=400, 
        filled=True, 
        border_radius=10,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color="#394459",
        label_style=ft.TextStyle(color="#394459"),
        multiline=True,
        min_lines=3,
        max_lines=5,
        hint_text="–∫—É–ø–∏—Ç—å —Ü–≤–µ—Ç—ã\n–¥–æ—Å—Ç–∞–≤–∫–∞ —Ü–≤–µ—Ç–æ–≤\n—Ü–≤–µ—Ç—ã –º–æ—Å–∫–≤–∞\n\nüí° –û—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–∑–∏—Ü–∏–π"
    )
    
    serp_engines_dropdown = ft.Dropdown(
        label="–ü–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã",
        width=200,
        options=[
            ft.dropdown.Option("google", "Google"),
            ft.dropdown.Option("yandex", "–Ø–Ω–¥–µ–∫—Å"),
            ft.dropdown.Option("both", "Google + –Ø–Ω–¥–µ–∫—Å")
        ],
        value="both"
    )
    
    serp_run_btn = ft.ElevatedButton(
        "üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑", 
        icon=ft.Icons.PLAY_ARROW,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    
    serp_stop_btn = ft.ElevatedButton(
        "‚èπ –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", 
        icon=ft.Icons.STOP,
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#FF5722",
            color="white",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    
    serp_progress_bar = ft.ProgressBar(width=600, color="#F2E307", bgcolor="#394459", value=0.0, height=10, border_radius=20)
    
    serp_results_area = ft.TextField(
        label="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—Ä–µ–∫–∏–Ω–≥–∞", 
        multiline=True, 
        min_lines=16, 
        max_lines=25,
        read_only=True,
        border_color="#394459",
        bgcolor="#F2F2F2",
        color="#394459"
    )
    
    serp_export_btn = ft.ElevatedButton(
        "üìä –≠–∫—Å–ø–æ—Ä—Ç –≤ Excel",
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#4CAF50",
            color="white",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    
    serp_chart_container = ft.Container(
        content=ft.Text("–ì—Ä–∞—Ñ–∏–∫ –ø–æ—è–≤–∏—Ç—Å—è –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–∑–∏—Ü–∏–π", 
                       color=ft.Colors.GREY_500, size=14),
        alignment=ft.alignment.center,
        padding=20
    )

    serp_tracker_content.content = ft.Column([
        ft.Text("üìä SERP Tracker - –ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–∑–∏—Ü–∏–π", size=24, weight=ft.FontWeight.BOLD),
        ft.Text("–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ –¥–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∑–∏—Ü–∏–π –≤ Google –∏ –Ø–Ω–¥–µ–∫—Å", size=16),
        ft.Row([serp_domain_input, serp_engines_dropdown], spacing=10),
        serp_keywords_input,
        ft.Row([serp_run_btn, serp_stop_btn], spacing=10),
        serp_progress_bar,
        serp_results_area,
        ft.Row([serp_export_btn]),
        serp_chart_container
    ], expand=True)

    # --- –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–Ω–æ–ø–æ–∫ ---
    def show_seo(e):
        summary = page.data.get('seo_summary', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ SEO')
        summary_area.value = summary
        export_btn.visible = True
        export_word_btn.visible = True
        sitemap_full_excel_btn.visible = False
        sitemap_full_word_btn.visible = False
        export_btn.data = ('seo', summary)
        page.update()
    seo_btn.on_click = show_seo

    def show_robots(e):
        summary = page.data.get('robots_summary', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ robots.txt')
        summary_area.value = summary
        export_btn.visible = True
        export_word_btn.visible = True
        sitemap_full_excel_btn.visible = False
        sitemap_full_word_btn.visible = False
        export_btn.data = ('robots', summary)
        page.update()
    robots_btn.on_click = show_robots

    def show_sitemap(e):
        summary = page.data.get('sitemap_summary', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ sitemap.xml')
        summary_area.value = summary
        export_btn.visible = True
        export_word_btn.visible = True
        sitemap_full_excel_btn.visible = True
        sitemap_full_word_btn.visible = True
        export_btn.data = ('sitemap', summary)
        page.update()
    sitemap_btn.on_click = show_sitemap

    def show_links(e):
        summary = page.data.get('links_summary', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Å—ã–ª–∫–∞–º')
        summary_area.value = summary
        export_btn.visible = True
        export_word_btn.visible = True
        sitemap_full_excel_btn.visible = False
        sitemap_full_word_btn.visible = False
        export_btn.data = ('links', summary)
        page.update()
    links_btn.on_click = show_links

    def show_images(e):
        summary = page.data.get('images_summary', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º')
        summary_area.value = summary
        export_btn.visible = True
        export_word_btn.visible = True
        sitemap_full_excel_btn.visible = False
        sitemap_full_word_btn.visible = False
        export_btn.data = ('images', summary)
        page.update()
    images_btn.on_click = show_images

    def show_full(e):
        summary = page.data.get('full_summary', '–ù–µ—Ç –æ–±—â–µ–π —Å–≤–æ–¥–∫–∏')
        summary_area.value = summary
        export_btn.visible = True
        export_word_btn.visible = True
        sitemap_full_excel_btn.visible = False
        sitemap_full_word_btn.visible = False
        export_btn.data = ('full', summary)
        page.update()
    full_btn.on_click = show_full

    # --- –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–Ω–æ–ø–æ–∫ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–æ–∫ ---
    def links_show_seo(e):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
        if 'multiple_seo_summary' in page.data:
            summary = page.data.get('multiple_seo_summary', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ SEO')
        else:
            summary = page.data.get('seo_summary', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ SEO')
        links_summary_area.value = summary
        page.data['links_export_btn_visible'] = True
        page.data['links_export_word_btn_visible'] = True
        links_export_btn.data = ('seo', summary)
        update_links_export_buttons()
    links_seo_btn.on_click = links_show_seo

    def links_show_links(e):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
        if 'multiple_links_summary' in page.data:
            detailed_summary = page.data.get('multiple_links_summary', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Å—ã–ª–∫–∞–º')
        else:
            link_statuses = page.data.get('link_statuses', {})
            if link_statuses:
                # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É —Å—Å—ã–ª–æ–∫
                detailed_summary = "### –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Å—ã–ª–∫–∞—Ö\n\n"
                for url, status in link_statuses.items():
                    status_emoji = "üü¢" if isinstance(status, int) and status == 200 else "üî¥"
                    detailed_summary += f"{status_emoji} **{url}**\n"
                    detailed_summary += f"   –°—Ç–∞—Ç—É—Å: {status}\n"
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Å—ã–ª–∫–µ
                    try:
                        result = check_resource(url, links_ssl_checkbox.value)
                        href, status_code, headers, size = result
                        detailed_summary += f"   –†–∞–∑–º–µ—Ä: {size:.2f} –ö–ë\n"
                        
                        if headers:
                            detailed_summary += f"   Content-Type: {headers.get('content-type', '–ù–µ —É–∫–∞–∑–∞–Ω')}\n"
                            detailed_summary += f"   Server: {headers.get('server', '–ù–µ —É–∫–∞–∑–∞–Ω')}\n"
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–¥–∏—Ä–µ–∫—Ç
                        try:
                            response = requests.head(url, timeout=5, verify=not links_ssl_checkbox.value, allow_redirects=False)
                            if response.status_code in [301, 302, 303, 307, 308]:
                                redirect_url = response.headers.get('Location', '–ù–µ —É–∫–∞–∑–∞–Ω')
                                detailed_summary += f"   üîÑ –†–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞: {redirect_url}\n"
                        except:
                            pass
                            
                    except Exception as e:
                        detailed_summary += f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {str(e)}\n"
                    
                    detailed_summary += "\n"
            else:
                detailed_summary = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Å—ã–ª–∫–∞–º"
        
        links_summary_area.value = detailed_summary
        page.data['links_export_btn_visible'] = True
        page.data['links_export_word_btn_visible'] = True
        links_export_btn.data = ('links', detailed_summary)
        update_links_export_buttons()
    links_links_btn.on_click = links_show_links

    def links_show_images(e):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
        if 'multiple_images_summary' in page.data:
            summary = page.data.get('multiple_images_summary', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º')
        else:
            summary = page.data.get('images_summary', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º')
        links_summary_area.value = summary
        page.data['links_export_btn_visible'] = True
        page.data['links_export_word_btn_visible'] = True
        links_export_btn.data = ('images', summary)
        update_links_export_buttons()
    links_images_btn.on_click = links_show_images

    def links_show_full(e):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
        if 'multiple_full_summary' in page.data:
            summary = page.data.get('multiple_full_summary', '–ù–µ—Ç –æ–±—â–µ–π —Å–≤–æ–¥–∫–∏')
        else:
            summary = page.data.get('full_summary', '–ù–µ—Ç –æ–±—â–µ–π —Å–≤–æ–¥–∫–∏')
        links_summary_area.value = summary
        page.data['links_export_btn_visible'] = True
        page.data['links_export_word_btn_visible'] = True
        links_export_btn.data = ('full', summary)
        update_links_export_buttons()
    links_full_btn.on_click = links_show_full

    def links_clear_summary(e):
        # –û—á–∏—â–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ page.data
        page.data.clear()
        
        # –û—á–∏—â–∞–µ–º –æ–±–ª–∞—Å—Ç—å —Å–≤–æ–¥–∫–∏
        links_summary_area.value = ""
        
        # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞
        page.data['links_export_btn_visible'] = False
        page.data['links_export_word_btn_visible'] = False
        
        # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
        links_stop_btn.visible = False
        links_run_btn.visible = True
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        links_progress_bar.value = 0.0
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
        update_links_export_buttons()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        page.snack_bar = ft.SnackBar(content=ft.Text("–°–≤–æ–¥–∫–∞ –æ—á–∏—â–µ–Ω–∞"))
        page.snack_bar.open = True
    links_clear_btn.on_click = links_clear_summary

    def links_export_summary(e):
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        if hasattr(links_export_btn, 'data') and links_export_btn.data is not None:
            try:
                report_type, summary = links_export_btn.data
            except (ValueError, TypeError):
                report_type, summary = 'full', links_summary_area.value
        else:
            report_type, summary = 'full', links_summary_area.value
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        if not summary or summary.strip() == '':
            page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑."))
            page.snack_bar.open = True
            page.update()
            return
        
        try:
            report_path = generate_report(summary, "multiple_links", report_type=report_type)
            if report_path:
                page.snack_bar = ft.SnackBar(content=ft.Text(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {os.path.basename(report_path)}"))
                page.snack_bar.open = True
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–æ–≤ –µ—Å–ª–∏ –æ—Ç–∫—Ä—ã—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
                if exports_content.visible:
                    refresh_exports_list()
        except Exception as e:
            page.snack_bar = ft.SnackBar(content=ft.Text(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {str(e)}"))
            page.snack_bar.open = True
        page.update()
    links_export_btn.on_click = links_export_summary

    def links_export_summary_word(e):
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        if hasattr(links_export_word_btn, 'data') and links_export_word_btn.data is not None:
            try:
                report_type, summary = links_export_word_btn.data
            except (ValueError, TypeError):
                report_type, summary = 'full', links_summary_area.value
        else:
            report_type, summary = 'full', links_summary_area.value
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        if not summary or summary.strip() == '':
            page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑."))
            page.snack_bar.open = True
            page.update()
            return
        
        try:
            report_path = generate_word_report(summary, "multiple_links", report_type=report_type)
            if report_path:
                page.snack_bar = ft.SnackBar(content=ft.Text(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {os.path.basename(report_path)}"))
                page.snack_bar.open = True
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–æ–≤ –µ—Å–ª–∏ –æ—Ç–∫—Ä—ã—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
                if exports_content.visible:
                    refresh_exports_list()
            else:
                page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –û—à–∏–±–∫–∞: –º–æ–¥—É–ª—å python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"))
                page.snack_bar.open = True
        except Exception as e:
            page.snack_bar = ft.SnackBar(content=ft.Text(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {str(e)}"))
            page.snack_bar.open = True
        page.update()
    links_export_word_btn.on_click = links_export_summary_word

    def show_link_detail(e):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Å—ã–ª–∫–µ."""
        url = e.control.data
        if url:
            # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Å—ã–ª–∫–µ
            detail_info = f"üîó –î–µ—Ç–∞–ª–∏ —Å—Å—ã–ª–∫–∏: {url}\n\n"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Å—ã–ª–∫—É
            try:
                result = check_resource(url, links_ssl_checkbox.value)
                href, status, headers, size = result
                
                detail_info += f"üìä –°—Ç–∞—Ç—É—Å: {status}\n"
                detail_info += f"üìè –†–∞–∑–º–µ—Ä: {size} –ö–ë\n"
                detail_info += f"üìã –ó–∞–≥–æ–ª–æ–≤–∫–∏:\n"
                
                if headers:
                    for header, value in headers.items():
                        detail_info += f"   {header}: {value}\n"
                else:
                    detail_info += "   –ó–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã\n"
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–¥–∏—Ä–µ–∫—Ç
                try:
                    response = requests.head(url, timeout=10, verify=not links_ssl_checkbox.value, allow_redirects=False)
                    if response.status_code in [301, 302, 303, 307, 308]:
                        redirect_url = response.headers.get('Location', '–ù–µ —É–∫–∞–∑–∞–Ω')
                        detail_info += f"üîÑ –†–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞: {redirect_url}\n"
                except:
                    detail_info += "üîÑ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–¥–∏—Ä–µ–∫—Ç–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞\n"
                
            except Exception as e:
                detail_info += f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {str(e)}\n"
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏ –≤ —Å–≤–æ–¥–∫–µ
            links_summary_area.value = detail_info
            page.update()
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
            page.snack_bar = ft.SnackBar(content=ft.Text(f"–ü–æ–∫–∞–∑–∞–Ω—ã –¥–µ—Ç–∞–ª–∏ –¥–ª—è: {url}"))
            page.snack_bar.open = True
            page.update()
    

    


    def export_summary(e):
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        if hasattr(export_btn, 'data') and export_btn.data is not None:
            try:
                report_type, summary = export_btn.data
            except (ValueError, TypeError):
                report_type, summary = 'full', summary_area.value
        else:
            report_type, summary = 'full', summary_area.value
        
        try:
            report_path = generate_report(summary, url_input.value.strip(), report_type=report_type)
            if report_path:
                page.snack_bar = ft.SnackBar(content=ft.Text(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {os.path.basename(report_path)}"))
                page.snack_bar.open = True
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–æ–≤ –µ—Å–ª–∏ –æ—Ç–∫—Ä—ã—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
                if exports_content.visible:
                    refresh_exports_list()
            else:
                page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞"))
                page.snack_bar.open = True
        except Exception as ex:
            page.snack_bar = ft.SnackBar(content=ft.Text(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {str(ex)}"))
            page.snack_bar.open = True
        page.update()
    export_btn.on_click = export_summary

    def export_summary_word(e):
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        if hasattr(export_btn, 'data') and export_btn.data is not None:
            try:
                report_type, summary = export_btn.data
            except (ValueError, TypeError):
                report_type, summary = 'full', summary_area.value
        else:
            report_type, summary = 'full', summary_area.value
        
        try:
            report_path = generate_word_report(summary, url_input.value.strip(), report_type)
            if report_path:
                page.snack_bar = ft.SnackBar(content=ft.Text(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {os.path.basename(report_path)}"))
                page.snack_bar.open = True
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–æ–≤ –µ—Å–ª–∏ –æ—Ç–∫—Ä—ã—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
                if exports_content.visible:
                    refresh_exports_list()
            else:
                page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –û—à–∏–±–∫–∞: –º–æ–¥—É–ª—å python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"))
                page.snack_bar.open = True
        except Exception as ex:
            page.snack_bar = ft.SnackBar(content=ft.Text(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {str(ex)}"))
            page.snack_bar.open = True
        page.update()
    export_word_btn.on_click = export_summary_word

    def export_sitemap_full_excel(e):
        try:
            report_path = generate_sitemap_excel_report(url_input.value.strip(), ssl_checkbox.value)
            if report_path:
                page.snack_bar = ft.SnackBar(content=ft.Text(f"‚úÖ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç Sitemap —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {os.path.basename(report_path)}"))
                page.snack_bar.open = True
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–æ–≤ –µ—Å–ª–∏ –æ—Ç–∫—Ä—ã—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
                if exports_content.visible:
                    refresh_exports_list()
            else:
                page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö sitemap –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞"))
                page.snack_bar.open = True
        except Exception as ex:
            page.snack_bar = ft.SnackBar(content=ft.Text(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {str(ex)}"))
            page.snack_bar.open = True
        page.update()
    sitemap_full_excel_btn.on_click = export_sitemap_full_excel

    def export_sitemap_full_word(e):
        try:
            report_path = generate_sitemap_word_report(url_input.value.strip(), ssl_checkbox.value)
            if report_path:
                page.snack_bar = ft.SnackBar(content=ft.Text(f"‚úÖ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç Sitemap —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {os.path.basename(report_path)}"))
                page.snack_bar.open = True
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–æ–≤ –µ—Å–ª–∏ –æ—Ç–∫—Ä—ã—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
                if exports_content.visible:
                    refresh_exports_list()
            else:
                page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö sitemap –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –∏–ª–∏ –º–æ–¥—É–ª—å python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"))
                page.snack_bar.open = True
        except Exception as ex:
            page.snack_bar = ft.SnackBar(content=ft.Text(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {str(ex)}"))
            page.snack_bar.open = True
        page.update()
    sitemap_full_word_btn.on_click = export_sitemap_full_word

    def run_main_test(e):
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ —Å–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
        stop_btn.visible = True
        run_btn.visible = False
        page.update()
        
        # –°–æ–∑–¥–∞–µ–º —Å–æ–±—ã—Ç–∏–µ –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        page.data['stop_event'] = threading.Event()
        
        run_test(
            url_input.value.strip(),
            summary_area,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ summary_area –¥–ª—è –≤—ã–≤–æ–¥–∞
            page,
            progress_bar,
            ssl_checkbox.value,
            ""  # –£–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        )
    run_btn.on_click = run_main_test

    def stop_main_test(e):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Å—Ç."""
        if 'stop_event' in page.data:
            page.data['stop_event'].set()
        
        # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
        stop_btn.visible = False
        run_btn.visible = True
        page.update()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        page.snack_bar = ft.SnackBar(content=ft.Text("–¢–µ—Å—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"))
        page.snack_bar.open = True
        page.update()
    stop_btn.on_click = stop_main_test

    def update_links_export_buttons():
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –≤–∏–¥–∏–º–æ—Å—Ç—å –∫–Ω–æ–ø–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–ª–∞–≥–æ–≤ –≤ page.data."""
        if page.data.get('links_export_btn_visible', False):
            links_export_btn.visible = True
        else:
            links_export_btn.visible = False
            
        if page.data.get('links_export_word_btn_visible', False):
            links_export_word_btn.visible = True
        else:
            links_export_word_btn.visible = False
        
        page.update()

    def run_links_test_handler(e):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É —Å—Å—ã–ª–æ–∫."""
        multiple_urls_text = links_multiple_input.value.strip()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ —Å–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
        links_stop_btn.visible = True
        links_run_btn.visible = False
        page.update()
        
        # –°–æ–∑–¥–∞–µ–º —Å–æ–±—ã—Ç–∏–µ –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        page.data['stop_event'] = threading.Event()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å—Å—ã–ª–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        if multiple_urls_text:
            urls = [url.strip() for url in multiple_urls_text.split('\n') if url.strip()]
            if not urls:
                page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏"))
                page.snack_bar.open = True
                page.update()
                return
            
            links_progress_bar.value = 0.0
            links_summary_area.value = f"üîÑ –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ {len(urls)} —Å—Å—ã–ª–æ–∫..."
            page.data['links_export_btn_visible'] = False
            page.data['links_export_word_btn_visible'] = False
            update_links_export_buttons()
            
            # –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            threading.Thread(
                target=run_multiple_links_test,
                args=(urls, links_summary_area, page, links_progress_bar, links_ssl_checkbox.value, ""),
                daemon=True
            ).start()
        else:
            page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏"))
            page.snack_bar.open = True
            page.update()
            return
    links_run_btn.on_click = run_links_test_handler

    def stop_links_test(e):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É —Å—Å—ã–ª–æ–∫."""
        if 'stop_event' in page.data:
            page.data['stop_event'].set()
        
        # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
        links_stop_btn.visible = False
        links_run_btn.visible = True
        
        # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞
        page.data['links_export_btn_visible'] = False
        page.data['links_export_word_btn_visible'] = False
        update_links_export_buttons()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        page.snack_bar = ft.SnackBar(content=ft.Text("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"))
        page.snack_bar.open = True
        page.update()
    links_stop_btn.on_click = stop_links_test

    def clear_summary(e):
        # –û—á–∏—â–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ page.data
        page.data.clear()
        
        # –û—á–∏—â–∞–µ–º –æ–±–ª–∞—Å—Ç—å —Å–≤–æ–¥–∫–∏
        summary_area.value = ""
        
        # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞
        export_btn.visible = False
        export_word_btn.visible = False
        sitemap_full_excel_btn.visible = False
        sitemap_full_word_btn.visible = False
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        progress_bar.value = 0.0
        
        # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        switch_page(0)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
        page.update()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        page.snack_bar = ft.SnackBar(content=ft.Text("–°–≤–æ–¥–∫–∞ –æ—á–∏—â–µ–Ω–∞"))
        page.snack_bar.open = True
    clear_btn.on_click = clear_summary

    # --- –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è SERP Tracker ---
    def run_serp_tracking(e):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç—Ä–µ–∫–∏–Ω–≥ –ø–æ–∑–∏—Ü–∏–π –∏–ª–∏ –∞–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞."""
        domain = serp_domain_input.value.strip()
        keywords_text = serp_keywords_input.value.strip()
        engines = serp_engines_dropdown.value
        
        if not domain:
            page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –ó–∞–ø–æ–ª–Ω–∏—Ç–µ –¥–æ–º–µ–Ω —Å–∞–π—Ç–∞"))
            page.snack_bar.open = True
            page.update()
            return
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã
        if engines == "google":
            search_engines = ["google"]
        elif engines == "yandex":
            search_engines = ["yandex"]
        else:
            search_engines = ["google", "yandex"]
        
        # –ï—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ —É–∫–∞–∑–∞–Ω—ã, –∑–∞–ø—É—Å–∫–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞
        if not keywords_text:
            run_detailed_site_analysis_ui(domain, search_engines)
            return
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ —Å–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
        serp_stop_btn.visible = True
        serp_run_btn.visible = False
        page.update()
        
        # –°–æ–∑–¥–∞–µ–º —Å–æ–±—ã—Ç–∏–µ –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        page.data['serp_stop_event'] = threading.Event()
        
        # –ü–∞—Ä—Å–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords_list = [kw.strip() for kw in keywords_text.split('\n') if kw.strip()]
        
        serp_progress_bar.value = 0.0
        serp_results_area.value = f"üîÑ –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–∫–∏–Ω–≥–∞ –ø–æ–∑–∏—Ü–∏–π –¥–ª—è {len(keywords_list)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ {', '.join(search_engines)}..."
        serp_export_btn.visible = False
        page.update()
        
        def update_serp_progress(current, total, message):
            if total > 0:
                serp_progress_bar.value = current / total
            serp_results_area.value = message
            page.update()
        
        # –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–∫–∏–Ω–≥–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        threading.Thread(
            target=lambda: run_serp_tracking_worker(keywords_list, domain, search_engines, update_serp_progress, serp_results_area, serp_export_btn, page, serp_stop_btn, serp_run_btn),
            daemon=True
        ).start()
    
    def run_serp_tracking_worker(keywords_list, domain, search_engines, update_callback, results_area, export_btn, page_ref, stop_btn, run_btn):
        """–†–∞–±–æ—á–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞ –ø–æ–∑–∏—Ü–∏–π."""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ –º–æ–¥—É–ª—å
            if run_serp_tracking is None:
                results_area.value = "‚ùå –ú–æ–¥—É–ª—å SERP Tracker –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
                return
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç—Ä–µ–∫–∏–Ω–≥
            result = run_serp_tracking(keywords_list, domain, search_engines, update_callback)
            
            if "error" in result:
                results_area.value = f"‚ùå –û—à–∏–±–∫–∞: {result['error']}"
            else:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
                report = "üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–†–ï–ö–ò–ù–ì–ê –ü–û–ó–ò–¶–ò–ô\n"
                report += "=" * 50 + "\n\n"
                
                for res in result['results']:
                    keyword = res['keyword']
                    engine = res['search_engine']
                    position = res['position']
                    url = res['url']
                    title = res['title']
                    status = res['status']
                    
                    if status == "success":
                        if position > 0:
                            report += f"‚úÖ '{keyword}' –≤ {engine}: –ø–æ–∑–∏—Ü–∏—è {position}\n"
                            if url:
                                report += f"   URL: {url}\n"
                            if title:
                                report += f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n"
                        else:
                            report += f"‚ùå '{keyword}' –≤ {engine}: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–æ–ø-10\n"
                    else:
                        report += f"‚ùå '{keyword}' –≤ {engine}: –æ—à–∏–±–∫–∞ - {status}\n"
                    report += "\n"
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                stats = result['statistics']
                report += "üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n"
                report += f"–í—Å–µ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {stats['total_keywords']}\n"
                report += f"–í —Ç–æ–ø-3: {stats['top_3']}\n"
                report += f"–í —Ç–æ–ø-10: {stats['top_10']}\n"
                report += f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ: {stats['not_found']}\n"
                
                results_area.value = report
                export_btn.visible = True
                page_ref.data['serp_results'] = result
                
        except Exception as e:
            results_area.value = f"‚ùå –û—à–∏–±–∫–∞ —Ç—Ä–µ–∫–∏–Ω–≥–∞: {str(e)}"
        finally:
            # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
            stop_btn.visible = False
            run_btn.visible = True
            page_ref.update()
    
    def run_detailed_site_analysis_ui(domain, search_engines):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤."""
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ —Å–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
        serp_stop_btn.visible = True
        serp_run_btn.visible = False
        page.update()
        
        # –°–æ–∑–¥–∞–µ–º —Å–æ–±—ã—Ç–∏–µ –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        page.data['serp_stop_event'] = threading.Event()
        
        serp_progress_bar.value = 0.0
        serp_results_area.value = f"üîÑ –ó–∞–ø—É—Å–∫ –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–∞–π—Ç–∞ {domain}...\n\nüîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤...\nüìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∑–∏—Ü–∏–π –≤ {', '.join(search_engines)}..."
        serp_export_btn.visible = False
        page.update()
        
        def update_site_progress(current, total, message):
            if total > 0:
                serp_progress_bar.value = current / total
            serp_results_area.value = message
            page.update()
        
        # –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        threading.Thread(
            target=lambda: run_detailed_site_analysis_worker(domain, search_engines, update_site_progress, serp_results_area, serp_export_btn, page, serp_stop_btn, serp_run_btn),
            daemon=True
        ).start()
    
    def run_detailed_site_analysis_worker(domain, search_engines, update_callback, results_area, export_btn, page_ref, stop_btn, run_btn):
        """–†–∞–±–æ—á–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–∞–π—Ç–∞."""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ –º–æ–¥—É–ª—å
            if run_detailed_site_analysis is None:
                results_area.value = "‚ùå –ú–æ–¥—É–ª—å SERP Tracker –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
                return
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            import serp_tracker
            result = serp_tracker.run_detailed_site_analysis(domain, search_engines, update_callback)
            
            if "error" in result:
                results_area.value = f"‚ùå –û—à–∏–±–∫–∞: {result['error']}"
            else:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç
                report = "üåê –ü–û–î–†–û–ë–ù–´–ô –ê–ù–ê–õ–ò–ó –°–ê–ô–¢–ê\n"
                report += "=" * 60 + "\n\n"
                
                report += f"üìã –û–°–ù–û–í–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:\n"
                report += f"–î–æ–º–µ–Ω: {result['domain']}\n"
                report += f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {len(result['keywords_checked'])}\n"
                report += f"–ü–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã: {', '.join(search_engines)}\n\n"
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                report += f"üîç –ü–†–û–í–ï–†–ï–ù–ù–´–ï –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê:\n"
                for i, keyword in enumerate(result['keywords_checked'][:10], 1):
                    report += f"{i}. {keyword}\n"
                if len(result['keywords_checked']) > 10:
                    report += f"... –∏ –µ—â–µ {len(result['keywords_checked']) - 10} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤\n"
                report += "\n"
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∑–∏—Ü–∏–π
                report += f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–ó–ò–¶–ò–ô:\n"
                report += "-" * 40 + "\n"
                
                found_positions = []
                not_found = []
                
                for res in result['detailed_results']:
                    keyword = res['keyword']
                    engine = res['search_engine']
                    position = res['position']
                    url = res['url']
                    title = res['title']
                    snippet = res.get('snippet', '')
                    
                    if position > 0:
                        found_positions.append(res)
                        report += f"‚úÖ '{keyword}' –≤ {engine}: –ø–æ–∑–∏—Ü–∏—è {position}\n"
                        if url:
                            report += f"   URL: {url}\n"
                        if title:
                            report += f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n"
                        if snippet:
                            report += f"   –°–Ω–∏–ø–ø–µ—Ç: {snippet[:100]}...\n"
                        report += "\n"
                    else:
                        not_found.append(res)
                        report += f"‚ùå '{keyword}' –≤ {engine}: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–æ–ø-10\n\n"
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–ø-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
                if found_positions:
                    report += f"üèÜ –î–ï–¢–ê–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ù–ê–ô–î–ï–ù–ù–´–• –ü–û–ó–ò–¶–ò–Ø–•:\n"
                    report += "-" * 50 + "\n"
                    
                    for res in found_positions[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5
                        keyword = res['keyword']
                        engine = res['search_engine']
                        position = res['position']
                        search_results = res.get('search_results', {})
                        
                        report += f"üîç '{keyword}' –≤ {engine} (–ø–æ–∑–∏—Ü–∏—è {position}):\n"
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
                        all_results = search_results.get('all_results', [])
                        for i, result in enumerate(all_results[:5], 1):
                            result_url = result.get('url', '')
                            result_title = result.get('title', '')
                            is_ours = domain in result_url
                            
                            if is_ours:
                                report += f"   {i}. üéØ {result_title}\n"
                                report += f"      {result_url}\n"
                            else:
                                report += f"   {i}. {result_title}\n"
                                report += f"      {result_url}\n"
                        report += "\n"
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                stats = result['statistics']
                report += f"üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n"
                report += f"–í—Å–µ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {stats['total_keywords']}\n"
                report += f"–í —Ç–æ–ø-3: {stats['top_3']}\n"
                report += f"–í —Ç–æ–ø-10: {stats['top_10']}\n"
                report += f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ: {stats['not_found']}\n\n"
                
                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≥—Ä–∞—Ñ–∏–∫–∞—Ö
                if result.get('charts'):
                    report += f"üìä –ì–†–ê–§–ò–ö–ò –î–í–ò–ñ–ï–ù–ò–Ø –ü–û–ó–ò–¶–ò–ô:\n"
                    report += f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {len(result['charts'])}\n"
                    report += f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π\n\n"
                
                report += f"‚úÖ –ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!"
                
                results_area.value = report
                export_btn.visible = True
                page_ref.data['detailed_site_analysis'] = result
                
        except Exception as e:
            results_area.value = f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"
        finally:
            # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
            stop_btn.visible = False
            run_btn.visible = True
            page_ref.update()
    
    def stop_serp_tracking(e):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ç—Ä–µ–∫–∏–Ω–≥ –ø–æ–∑–∏—Ü–∏–π."""
        if 'serp_stop_event' in page.data:
            page.data['serp_stop_event'].set()
        
        # –°–∫—Ä—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
        serp_stop_btn.visible = False
        serp_run_btn.visible = True
        page.update()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        page.snack_bar = ft.SnackBar(content=ft.Text("–¢—Ä–µ–∫–∏–Ω–≥ –ø–æ–∑–∏—Ü–∏–π –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"))
        page.snack_bar.open = True
        page.update()
    
    def export_serp_results(e):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—Ä–µ–∫–∏–Ω–≥–∞ –∏–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ –≤ Excel."""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        if 'serp_results' not in page.data and 'site_analysis' not in page.data and 'detailed_site_analysis' not in page.data:
            page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞"))
            page.snack_bar.open = True
            page.update()
            return
        
        try:
            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–∞–π—Ç–∞
            if 'detailed_site_analysis' in page.data:
                import pandas as pd
                from datetime import datetime
                
                analysis_data = page.data['detailed_site_analysis']
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"reports/detailed_analysis_{analysis_data['domain'].replace('.', '_')}_{timestamp}.xlsx"
                
                with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                    # –õ–∏—Å—Ç —Å –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                    basic_data = {
                        '–ü–∞—Ä–∞–º–µ—Ç—Ä': [
                            '–î–æ–º–µ–Ω',
                            '–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤',
                            '–ü–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã',
                            '–í—Å–µ–≥–æ –ø–æ–∑–∏—Ü–∏–π',
                            '–í —Ç–æ–ø-3',
                            '–í —Ç–æ–ø-10',
                            '–ù–µ –Ω–∞–π–¥–µ–Ω–æ'
                        ],
                        '–ó–Ω–∞—á–µ–Ω–∏–µ': [
                            analysis_data['domain'],
                            len(analysis_data['keywords_checked']),
                            ', '.join(set([res['search_engine'] for res in analysis_data['detailed_results']])),
                            analysis_data['statistics']['total_keywords'],
                            analysis_data['statistics']['top_3'],
                            analysis_data['statistics']['top_10'],
                            analysis_data['statistics']['not_found']
                        ]
                    }
                    basic_df = pd.DataFrame(basic_data)
                    basic_df.to_excel(writer, sheet_name='–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', index=False)
                    
                    # –õ–∏—Å—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∑–∏—Ü–∏–π
                    positions_data = []
                    for res in analysis_data['detailed_results']:
                        positions_data.append({
                            '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ': res['keyword'],
                            '–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞': res['search_engine'],
                            '–ü–æ–∑–∏—Ü–∏—è': res['position'],
                            'URL': res['url'],
                            '–ó–∞–≥–æ–ª–æ–≤–æ–∫': res['title'],
                            '–°–Ω–∏–ø–ø–µ—Ç': res.get('snippet', ''),
                            '–°—Ç–∞—Ç—É—Å': res['status']
                        })
                    
                    if positions_data:
                        positions_df = pd.DataFrame(positions_data)
                        positions_df.to_excel(writer, sheet_name='–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∑–∏—Ü–∏–π', index=False)
                    
                    # –õ–∏—Å—Ç —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
                    keywords_data = {
                        '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ': analysis_data['keywords_checked']
                    }
                    keywords_df = pd.DataFrame(keywords_data)
                    keywords_df.to_excel(writer, sheet_name='–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞', index=False)
                
                page.snack_bar = ft.SnackBar(content=ft.Text(f"‚úÖ –ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ {filename}"))
                page.snack_bar.open = True
                page.update()
                return
            
            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å–∞–π—Ç–∞
            if 'site_analysis' in page.data:
                import pandas as pd
                from datetime import datetime
                
                analysis_data = page.data['site_analysis']
                
                # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
                data = {
                    '–ü–∞—Ä–∞–º–µ—Ç—Ä': [
                        '–î–æ–º–µ–Ω',
                        '–ü—Ä–æ—Ç–æ–∫–æ–ª', 
                        '–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞',
                        'HTTP —Å—Ç–∞—Ç—É—Å',
                        'robots.txt',
                        'sitemap.xml'
                    ],
                    '–ó–Ω–∞—á–µ–Ω–∏–µ': [
                        analysis_data['domain_info']['domain'],
                        analysis_data['domain_info']['protocol'],
                        analysis_data['domain_info']['path'],
                        str(analysis_data['status_code']),
                        analysis_data['robots_info'],
                        analysis_data['sitemap_info']
                    ]
                }
                
                df = pd.DataFrame(data)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"reports/site_analysis_{analysis_data['domain'].replace('.', '_')}_{timestamp}.xlsx"
                
                # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ Excel
                df.to_excel(filename, index=False, sheet_name='–ê–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞')
                
                page.snack_bar = ft.SnackBar(content=ft.Text(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filename}"))
                page.snack_bar.open = True
                page.update()
                return
            
            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—Ä–µ–∫–∏–Ω–≥–∞ –ø–æ–∑–∏—Ü–∏–π
            if 'serp_results' in page.data:
                if SERPTracker is None:
                    page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –ú–æ–¥—É–ª—å SERP Tracker –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"))
                    page.snack_bar.open = True
                    page.update()
                    return
                
                tracker = SERPTracker()
                filename = tracker.export_to_excel()
                
                page.snack_bar = ft.SnackBar(content=ft.Text(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—Ä–µ–∫–∏–Ω–≥–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filename}"))
                page.snack_bar.open = True
                page.update()
            
        except Exception as e:
            page.snack_bar = ft.SnackBar(content=ft.Text(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {str(e)}"))
            page.snack_bar.open = True
            page.update()
    
    serp_run_btn.on_click = run_serp_tracking
    serp_stop_btn.on_click = stop_serp_tracking
    serp_export_btn.on_click = export_serp_results

    # --- –ü–∞—Ä—Å–µ—Ä –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü ---
    parser_url_input = ft.TextField(
        label="–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–∞–π—Ç–∞", 
        width=400, 
        filled=True, 
        border_radius=10,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    parser_ssl_checkbox = ft.Checkbox(label="–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å SSL", value=True)
    parser_max_pages = ft.TextField(
        label="–ú–∞–∫—Å. —Å—Ç—Ä–∞–Ω–∏—Ü", 
        value="1000", 
        width=100, 
        filled=True, 
        border_radius=10,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    parser_progress = ft.ProgressBar(width=400, color="#F2E307", bgcolor="#394459", value=0.0, height=10, border_radius=20)
    parser_table = ft.DataTable(
        columns=[
            ft.DataColumn(label=ft.Text("–°—Å—ã–ª–∫–∞", color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("HTTP", color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("–†–µ–¥–∏—Ä–µ–∫—Ç", color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("SEO –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", color=get_input_text_color())),
        ],
        rows=[],
        horizontal_lines=ft.BorderSide(1, "#394459"),
        vertical_lines=ft.BorderSide(1, "#394459"),
        bgcolor="#F2F2F2",
        border=ft.border.all(1, "#394459"),
        border_radius=10,
    )
    parser_export_btn = ft.ElevatedButton(
        "–≠–∫—Å–ø–æ—Ä—Ç –≤ Excel", 
        icon=ft.Icons.DOWNLOAD, 
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    parser_export_word_btn = ft.ElevatedButton(
        "–≠–∫—Å–ø–æ—Ä—Ç –≤ Word", 
        icon=ft.Icons.DESCRIPTION, 
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    parser_status = ft.Text(visible=False)
    
    # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–æ–º
    parser_stop_event = threading.Event()
    parser_thread = None

    def stop_parser(e):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞—Ä—Å–µ—Ä."""
        if parser_thread and parser_thread.is_alive():
            parser_stop_event.set()
            parser_status.value = "–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä—Å–µ—Ä..."
            parser_status.visible = True
            parser_run_btn.visible = True
            parser_stop_btn.visible = False
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
            page.snack_bar = ft.SnackBar(content=ft.Text("–ü–∞—Ä—Å–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"))
            page.snack_bar.open = True
            page.update()

    def parser_update(visited_count, found_count):
        if parser_stop_event.is_set():
            parser_status.value = f"–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º... –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {visited_count}"
        else:
            parser_progress.value = min(1.0, found_count / max(visited_count, 1))
        page.update()



    def parser_done(results):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        if parser_stop_event.is_set():
            parser_status.value = f"–ü–∞—Ä—Å–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(results)}"
        else:
            parser_status.value = f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(results)}"
        
        parser_table.rows = []
        for r in results:
            # –§–æ—Ä–º–∏—Ä—É–µ–º —á–∏—Ç–∞–µ–º—É—é SEO –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            title = r['Title'] if r['Title'] else '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'
            description = r['Meta_Description'] if r['Meta_Description'] else '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'
            h1 = r['H1'] if r['H1'] else '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'
            
            # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            seo_text = ft.Text(
                f"üìÑ Title: {title}\n"
                f"üìù Description: {description}\n"
                f"üî§ H1: {h1}\n"
                f"‚úÖ –°—Ç–∞—Ç—É—Å: {r['SEO']}",
                size=10,
                selectable=True,
                max_lines=8,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç–∏
                overflow=ft.TextOverflow.ELLIPSIS,  # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                weight=ft.FontWeight.NORMAL,
                color=get_input_text_color()
            )
            
            row = ft.DataRow(cells=[
                ft.DataCell(ft.Text(r['–°—Å—ã–ª–∫–∞'], color=get_input_text_color())),
                ft.DataCell(ft.Text(str(r['HTTP']), color=get_input_text_color())),
                ft.DataCell(ft.Text(r['–†–µ–¥–∏—Ä–µ–∫—Ç'], color=get_input_text_color())),
                ft.DataCell(seo_text)
            ])
            parser_table.rows.append(row)
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–Ω–æ–ø–æ–∫
        parser_run_btn.visible = True
        parser_stop_btn.visible = False
        
        parser_export_btn.visible = True if results else False
        parser_export_word_btn.visible = True if results else False
        parser_status.visible = True
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        page.data['parser_results'] = results
        page.data['parser_site_url'] = parser_url_input.value.strip()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
        import pandas as pd
        df = pd.DataFrame(results)
        fname = f"reports/allpages_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        df.to_excel(fname, index=False)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
        page.snack_bar = ft.SnackBar(content=ft.Text(f"‚úÖ Excel –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {os.path.basename(fname)}"))
        page.snack_bar.open = True
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–æ–≤ –µ—Å–ª–∏ –æ—Ç–∫—Ä—ã—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
        if exports_content.visible:
            refresh_exports_list()
        
        page.update()

    def export_to_word(e):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞ –≤ Word."""
        if 'parser_results' in page.data and 'parser_site_url' in page.data:
            results = page.data['parser_results']
            site_url = page.data['parser_site_url']
            
            try:
                report_path = generate_word_report(results, site_url, 'parser')
                if report_path:
                    page.snack_bar = ft.SnackBar(content=ft.Text(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {os.path.basename(report_path)}"))
                    page.snack_bar.open = True
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–æ–≤ –µ—Å–ª–∏ –æ—Ç–∫—Ä—ã—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
                    if exports_content.visible:
                        refresh_exports_list()
                else:
                    page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –û—à–∏–±–∫–∞: –º–æ–¥—É–ª—å python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"))
                    page.snack_bar.open = True
            except Exception as ex:
                page.snack_bar = ft.SnackBar(content=ft.Text(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {str(ex)}"))
                page.snack_bar.open = True
            page.update()
        else:
            page.snack_bar = ft.SnackBar(content=ft.Text("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞"))
            page.snack_bar.open = True
            page.update()

    def parser_run(e):
        url = parser_url_input.value.strip()
        if not url.startswith('http'):
            parser_status.value = "–í–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL!"
            parser_status.visible = True
            page.update()
            return
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ–±—ã—Ç–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        parser_stop_event.clear()
        
        parser_progress.value = 0.0
        parser_table.rows = []
        parser_export_btn.visible = False
        parser_status.value = "–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞..."
        parser_status.visible = True
        parser_run_btn.visible = False
        parser_stop_btn.visible = True
        page.update()
        
        try:
            max_pages = int(parser_max_pages.value)
        except ValueError:
            max_pages = 15000
        
        def parser_worker():
            crawl_site_without_sitemap(url, parser_ssl_checkbox.value, parser_update, parser_done, parser_stop_event, max_pages=max_pages)
        
        parser_thread = threading.Thread(target=parser_worker)
        parser_thread.start()

    parser_run_btn = ft.ElevatedButton(
        "–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–µ—Ä", 
        icon=ft.Icons.PLAY_ARROW, 
        on_click=parser_run,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    parser_stop_btn = ft.ElevatedButton(
        "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", 
        icon=ft.Icons.STOP, 
        visible=False, 
        on_click=lambda e: stop_parser(e),
        style=ft.ButtonStyle(
            bgcolor="#FF6B6B",
            color="#FFFFFF",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    parser_export_btn.on_click = lambda e: None  # –£–∂–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    parser_export_word_btn.on_click = export_to_word
    parser_content.content = ft.Column([
        ft.Text("–ü–∞—Ä—Å–µ—Ä –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å–∞–π—Ç–∞", size=20, weight=ft.FontWeight.BOLD, color=get_text_color()),
        ft.Text("–í–≤–µ–¥–∏—Ç–µ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–∞–π—Ç–∞. –ü–∞—Ä—Å–µ—Ä –Ω–∞–π–¥–µ—Ç –≤—Å–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –∏ –ø–æ–∫–∞–∂–µ—Ç SEO –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ç–∞–±–ª–∏—Ü–µ.", size=14, color=get_secondary_text_color()),
        ft.Row([parser_url_input, parser_ssl_checkbox, parser_max_pages, parser_run_btn, parser_stop_btn]),
        parser_progress,
        ft.Container(
            ft.Row([
                ft.Container(parser_table, width=2500, expand=True)
            ], scroll=ft.ScrollMode.ALWAYS),
            expand=True,
            bgcolor="#F2F2F2",
            border_radius=10,
            padding=5,
            alignment=ft.alignment.center,
            border=ft.border.all(1, "#394459"),
        ),
        ft.Row([parser_export_btn, parser_export_word_btn]),
        parser_status
    ], expand=True)
    
    # --- –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ ---
    text_analysis_url_input = ft.TextField(
        label="URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", 
        width=400, 
        filled=True, 
        border_radius=10,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    text_analysis_keywords_input = ft.TextField(
        label="–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)", 
        width=300, 
        filled=True, 
        border_radius=10,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    text_analysis_ssl_checkbox = ft.Checkbox(label="–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å SSL", value=True)
    text_analysis_progress = ft.ProgressBar(width=400, color="#F2E307", bgcolor="#394459", value=0.0, height=10, border_radius=20)
    text_analysis_status = ft.Text(visible=False)
    
    # –û–±–ª–∞—Å—Ç–∏ –¥–ª—è –≤—ã–≤–æ–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    text_analysis_summary = ft.TextField(
        label="SEO –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞", 
        multiline=True, 
        min_lines=20, 
        max_lines=40, 
        width=1000, 
        filled=True, 
        border_radius=10, 
        expand=True,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    text_analysis_keywords = ft.TextField(
        label="–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –ø–ª–æ—Ç–Ω–æ—Å—Ç—å", 
        multiline=True, 
        min_lines=25, 
        max_lines=50, 
        width=1000, 
        filled=True, 
        border_radius=10, 
        expand=True,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    text_analysis_structure = ft.TextField(
        label="–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤", 
        multiline=True, 
        min_lines=8, 
        max_lines=15, 
        width=1000, 
        filled=True, 
        border_radius=10, 
        expand=True,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    text_analysis_full_text = ft.TextField(
        label="–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã", 
        multiline=True, 
        min_lines=15, 
        max_lines=50, 
        width=1000, 
        filled=True, 
        border_radius=10, 
        expand=True,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    text_analysis_declensions = ft.TextField(
        label="–ê–Ω–∞–ª–∏–∑ —Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π", 
        multiline=True, 
        min_lines=25, 
        max_lines=50, 
        width=1000, 
        filled=True, 
        border_radius=10, 
        expand=True,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    
    def text_analysis_run(e):
        url = text_analysis_url_input.value.strip()
        if not url.startswith('http'):
            text_analysis_status.value = "–í–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL!"
            text_analysis_status.visible = True
            page.update()
            return
        
        text_analysis_progress.value = 0.0
        text_analysis_status.value = "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç (—É–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å Selenium)..."
        text_analysis_status.visible = True
        text_analysis_summary.value = ""
        text_analysis_keywords.value = ""
        text_analysis_structure.value = ""
        text_analysis_full_text.value = ""
        text_analysis_declensions.value = ""
        page.update()
        
        def worker():
            try:
                # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ Selenium (–∫–∞–∫ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ —Å–∫–ª–æ–Ω–µ–Ω–∏–π)
                from selenium import webdriver
                from selenium.webdriver.chrome.options import Options
                
                chrome_options = Options()
                chrome_options.add_argument('--headless')
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--disable-dev-shm-usage')
                
                driver = webdriver.Chrome(options=chrome_options)
                driver.get(url)
                
                # –ü–æ–ª—É—á–∞–µ–º HTML –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                html_content = driver.page_source
                driver.quit()
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
                analysis = analyze_text_content(html_content, url)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–≤–æ–¥–∫—É
                summary_parts = []
                summary_parts.append("üìä SEO –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–ê (–£–õ–£–ß–®–ï–ù–ù–´–ô)")
                summary_parts.append("=" * 50)
                summary_parts.append("üîç –ê–Ω–∞–ª–∏–∑ –≤–∫–ª—é—á–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ:")
                summary_parts.append("   ‚Ä¢ –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç")
                summary_parts.append("   ‚Ä¢ –¢–µ–∫—Å—Ç –≤ –∞–Ω–∫–æ—Ä–∞—Ö (—Å—Å—ã–ª–∫–∞—Ö)")
                summary_parts.append("   ‚Ä¢ –õ—é–±–æ–π –¥—Ä—É–≥–æ–π –≤–∏–¥–∏–º—ã–π —Ç–µ–∫—Å—Ç")
                summary_parts.append("   ‚Ä¢ –ò–°–ö–õ–Æ–ß–ê–ï–¢: header, footer, –∞—Ç—Ä–∏–±—É—Ç—ã (alt, title, placeholder, aria-label)")
                summary_parts.append("")
                summary_parts.append(f"URL: {analysis['url']}")
                summary_parts.append(f"Title: {analysis['title']}")
                summary_parts.append(f"Meta Description: {analysis['meta_description']}")
                summary_parts.append("")
                
                summary_parts.append("üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                summary_parts.append(f"‚Ä¢ –í—Å–µ–≥–æ —Å–ª–æ–≤: {analysis['total_words']}")
                summary_parts.append(f"‚Ä¢ H1 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {analysis['h1_count']}")
                summary_parts.append(f"‚Ä¢ H2 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {analysis['h2_count']}")
                summary_parts.append(f"‚Ä¢ H3 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {analysis['h3_count']}")
                summary_parts.append(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {analysis['avg_sentence_length']} —Å–ª–æ–≤")
                summary_parts.append(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –∞–±–∑–∞—Ü–∞: {analysis['avg_paragraph_length']} —Å–ª–æ–≤")
                summary_parts.append(f"‚Ä¢ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {analysis['images_total']} (—Å alt: {analysis['images_with_alt']}, –±–µ–∑ alt: {analysis['images_without_alt']})")
                summary_parts.append(f"‚Ä¢ –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {analysis['internal_links']}")
                summary_parts.append(f"‚Ä¢ –í–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {analysis['external_links']}")
                summary_parts.append(f"‚Ä¢ –û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {analysis['structure_score']}/100")
                summary_parts.append("")
                
                if analysis['positives']:
                    summary_parts.append("‚úÖ –ü–û–õ–û–ñ–ò–¢–ï–õ–¨–ù–´–ï –ú–û–ú–ï–ù–¢–´:")
                    for positive in analysis['positives']:
                        summary_parts.append(f"‚Ä¢ {positive}")
                    summary_parts.append("")
                
                if analysis['recommendations']:
                    summary_parts.append("‚ö†Ô∏è –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
                    for rec in analysis['recommendations']:
                        summary_parts.append(f"‚Ä¢ {rec}")
                    summary_parts.append("")
                
                summary_parts.append("üìù –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–ô –ü–†–û–°–ú–û–¢–† –¢–ï–ö–°–¢–ê:")
                summary_parts.append(analysis['text_preview'])
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                keywords_text = "üîë –¢–û–ü-20 –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í –ò –ü–õ–û–¢–ù–û–°–¢–¨ (–£–õ–£–ß–®–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó):\n"
                keywords_text += "=" * 60 + "\n"
                keywords_text += "üìä –ê–Ω–∞–ª–∏–∑ –≤–∫–ª—é—á–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–≤–∫–ª—é—á–∞—è –∞–Ω–∫–æ—Ä—ã, –∏—Å–∫–ª—é—á–∞—è header/footer –∏ –∞—Ç—Ä–∏–±—É—Ç—ã)\n"
                keywords_text += f"üìà –í—Å–µ–≥–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ: {analysis['total_words']}\n"
                keywords_text += "=" * 60 + "\n\n"
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                sorted_keywords = sorted(analysis['keyword_density'].items(), key=lambda x: x[1], reverse=True)
                
                keywords_text += "üìä –ü–û –ü–õ–û–¢–ù–û–°–¢–ò (–æ—Ç –≤—ã—Å–æ–∫–æ–π –∫ –Ω–∏–∑–∫–æ–π):\n"
                keywords_text += "-" * 40 + "\n"
                for i, (word, density) in enumerate(sorted_keywords[:20], 1):
                    count = next((count for w, count in analysis['top_keywords'] if w == word), 0)
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                    if isinstance(density, (int, float)):
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ü–≤–µ—Ç–æ–≤—É—é –∏–Ω–¥–∏–∫–∞—Ü–∏—é –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                        if density > 3.0:
                            indicator = "üî¥"  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
                        elif density > 2.0:
                            indicator = "üü°"  # –í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
                        elif density > 1.0:
                            indicator = "üü¢"  # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
                        else:
                            indicator = "‚ö™"  # –ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
                        
                        keywords_text += f"{i:2d}. {indicator} {word}\n"
                        keywords_text += f"    –ß–∞—Å—Ç–æ—Ç–∞: {count} —Ä–∞–∑ | –ü–ª–æ—Ç–Ω–æ—Å—Ç—å: {density:.2f}%\n\n"
                    else:
                        keywords_text += f"{i:2d}. ‚ö™ {word}\n"
                        keywords_text += f"    –ß–∞—Å—Ç–æ—Ç–∞: {count} —Ä–∞–∑ | –ü–ª–æ—Ç–Ω–æ—Å—Ç—å: {density}\n\n"
                
                keywords_text += "\nüìà –ü–û –ß–ê–°–¢–û–¢–ï (–æ—Ç —á–∞—Å—Ç–æ–π –∫ —Ä–µ–¥–∫–æ–π):\n"
                keywords_text += "-" * 40 + "\n"
                for i, (word, count) in enumerate(analysis['top_keywords'], 1):
                    density = analysis['keyword_density'].get(word, 0)
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                    if isinstance(density, (int, float)):
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ü–≤–µ—Ç–æ–≤—É—é –∏–Ω–¥–∏–∫–∞—Ü–∏—é –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                        if density > 3.0:
                            indicator = "üî¥"  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
                        elif density > 2.0:
                            indicator = "üü°"  # –í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
                        elif density > 1.0:
                            indicator = "üü¢"  # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
                        else:
                            indicator = "‚ö™"  # –ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
                        
                        keywords_text += f"{i:2d}. {indicator} {word}\n"
                        keywords_text += f"    –ß–∞—Å—Ç–æ—Ç–∞: {count} —Ä–∞–∑ | –ü–ª–æ—Ç–Ω–æ—Å—Ç—å: {density:.2f}%\n\n"
                    else:
                        keywords_text += f"{i:2d}. ‚ö™ {word}\n"
                        keywords_text += f"    –ß–∞—Å—Ç–æ—Ç–∞: {count} —Ä–∞–∑ | –ü–ª–æ—Ç–Ω–æ—Å—Ç—å: {density}\n\n"
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                keywords_text += "\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ü–õ–û–¢–ù–û–°–¢–ò:\n"
                keywords_text += "-" * 40 + "\n"
                high_density_words = [(word, density) for word, density in sorted_keywords if density > 3.0]
                low_density_words = [(word, density) for word, density in sorted_keywords if density < 0.5]
                
                if high_density_words:
                    keywords_text += "üî¥ –°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å (>3%):\n"
                    for word, density in high_density_words[:5]:
                        if isinstance(density, (int, float)):
                            keywords_text += f"   ‚Ä¢ {word}: {density:.2f}% - —Å–Ω–∏–∑—å—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n"
                        else:
                            keywords_text += f"   ‚Ä¢ {word}: {density} - —Å–Ω–∏–∑—å—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n"
                    keywords_text += "\n"
                
                if low_density_words:
                    keywords_text += "‚ö™ –°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å (<0.5%):\n"
                    for word, density in low_density_words[:5]:
                        if isinstance(density, (int, float)):
                            keywords_text += f"   ‚Ä¢ {word}: {density:.2f}% - —É–≤–µ–ª–∏—á—å—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n"
                        else:
                            keywords_text += f"   ‚Ä¢ {word}: {density} - —É–≤–µ–ª–∏—á—å—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n"
                    keywords_text += "\n"
                
                keywords_text += "‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: 1-2% –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤\n"
                keywords_text += "‚ö†Ô∏è –ò–∑–±–µ–≥–∞–π—Ç–µ –ø–µ—Ä–µ—Å–ø–∞–º–∞ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏\n"
                keywords_text += "\nüîß –¢–ï–•–ù–ò–ß–ï–°–ö–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:\n"
                keywords_text += "‚Ä¢ –ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Selenium WebDriver\n"
                keywords_text += "‚Ä¢ –ü–æ–ª—É—á–µ–Ω –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n"
                keywords_text += "‚Ä¢ –ò—Å–∫–ª—é—á–µ–Ω—ã: header, footer, –∞—Ç—Ä–∏–±—É—Ç—ã (alt, title, placeholder, aria-label)\n"
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                structure_text = "üìã –°–¢–†–£–ö–¢–£–†–ê –ó–ê–ì–û–õ–û–í–ö–û–í:\n"
                structure_text += "=" * 30 + "\n"
                
                if analysis['h1_texts']:
                    structure_text += "H1:\n"
                    for h1 in analysis['h1_texts']:
                        structure_text += f"‚Ä¢ {h1}\n"
                    structure_text += "\n"
                
                if analysis['h2_texts']:
                    structure_text += "H2:\n"
                    for h2 in analysis['h2_texts']:
                        structure_text += f"‚Ä¢ {h2}\n"
                    structure_text += "\n"
                
                if analysis['h3_texts']:
                    structure_text += "H3:\n"
                    for h3 in analysis['h3_texts']:
                        structure_text += f"‚Ä¢ {h3}\n"
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
                full_text = "üìÑ –ü–û–õ–ù–´–ô –¢–ï–ö–°–¢ –°–¢–†–ê–ù–ò–¶–´ (–£–õ–£–ß–®–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó):\n"
                full_text += "=" * 60 + "\n"
                full_text += "üîç –í–∫–ª—é—á–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç: —Ç–µ–∫—Å—Ç, –∞–Ω–∫–æ—Ä—ã (–∏—Å–∫–ª—é—á–∞–µ—Ç header/footer –∏ –∞—Ç—Ä–∏–±—É—Ç—ã)\n"
                full_text += f"üìè –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(analysis['full_text'])} —Å–∏–º–≤–æ–ª–æ–≤\n"
                full_text += f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {analysis['total_words']}\n"
                full_text += "=" * 60 + "\n\n"
                full_text += analysis['full_text']
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
                text_analysis_summary.value = "\n".join(summary_parts)
                text_analysis_keywords.value = keywords_text
                text_analysis_structure.value = structure_text
                text_analysis_full_text.value = full_text
                text_analysis_status.value = "–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! (–æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç, –∏—Å–∫–ª—é—á–µ–Ω—ã header/footer –∏ –∞—Ç—Ä–∏–±—É—Ç—ã)"
                text_analysis_progress.value = 1.0
                
            except Exception as ex:
                text_analysis_status.value = f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(ex)}"
                text_analysis_progress.value = 0.0
            
            page.update()
        
        threading.Thread(target=worker).start()
    
    text_analysis_run_btn = ft.ElevatedButton(
        "–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç (–£–õ–£–ß–®–ï–ù–ù–´–ô)", 
        icon=ft.Icons.PLAY_ARROW, 
        on_click=text_analysis_run,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    
    def text_analysis_declensions_run(e):
        url = text_analysis_url_input.value.strip()
        keywords = text_analysis_keywords_input.value.strip()
        
        if not url.startswith('http'):
            text_analysis_status.value = "–í–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL!"
            text_analysis_status.visible = True
            page.update()
            return
        
        if not keywords:
            text_analysis_status.value = "–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!"
            text_analysis_status.visible = True
            page.update()
            return
        
        text_analysis_progress.value = 0.0
        text_analysis_status.value = "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π..."
        text_analysis_status.visible = True
        text_analysis_declensions.value = ""
        page.update()
        
        def worker():
            try:
                # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                response = requests.get(url, timeout=15, verify=not text_analysis_ssl_checkbox.value)
                if response.status_code != 200:
                    text_analysis_status.value = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {response.status_code}"
                    page.update()
                    return
                
                # –°–æ–∑–¥–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                from selenium import webdriver
                from selenium.webdriver.chrome.options import Options
                
                chrome_options = Options()
                chrome_options.add_argument('--headless')
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--disable-dev-shm-usage')
                
                driver = webdriver.Chrome(options=chrome_options)
                driver.get(url)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π
                keywords_result, density, target_analysis = analyze_keywords(driver, url, keywords)
                
                driver.quit()
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–≤–æ–¥–∫—É –∞–Ω–∞–ª–∏–∑–∞ —Å–∫–ª–æ–Ω–µ–Ω–∏–π
                declensions_text = []
                declensions_text.append("üîç –ê–ù–ê–õ–ò–ó –¶–ï–õ–ï–í–´–• –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í –° –£–ß–ï–¢–û–ú –°–ö–õ–û–ù–ï–ù–ò–ô")
                declensions_text.append("=" * 60)
                declensions_text.append(f"URL: {url}")
                declensions_text.append(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords}")
                declensions_text.append("")
                
                if isinstance(target_analysis, dict) and target_analysis:
                    for tkw, data in target_analysis.items():
                        declensions_text.append(f"üéØ –¶–ï–õ–ï–í–û–ï –ö–õ–Æ–ß–ï–í–û–ï –°–õ–û–í–û: '{tkw}'")
                        declensions_text.append(f"üìä –û–ë–©–ê–Ø –ß–ê–°–¢–û–¢–ê (—Å–æ —Å–∫–ª–æ–Ω–µ–Ω–∏—è–º–∏): {data['freq']} —Ä–∞–∑")
                        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                        density_value = data['density']
                        if isinstance(density_value, (int, float)):
                            declensions_text.append(f"üìà –ü–õ–û–¢–ù–û–°–¢–¨: {density_value:.2%}")
                        else:
                            declensions_text.append(f"üìà –ü–õ–û–¢–ù–û–°–¢–¨: {density_value}")
                        declensions_text.append("")
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è
                        if 'declensions_found' in data and data['declensions_found']:
                            declensions_text.append("üìù –ù–ê–ô–î–ï–ù–ù–´–ï –°–ö–õ–û–ù–ï–ù–ò–Ø:")
                            for declension, count in data['declensions_found'].items():
                                declensions_text.append(f"  ‚úÖ '{declension}': {count} —Ä–∞–∑")
                            declensions_text.append("")
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π
                            declensions_text.append("üìÑ –ü–û–õ–ù–´–ô –¢–ï–ö–°–¢ –° –ü–û–î–°–í–ï–¢–ö–û–ô:")
                            declensions_text.append("-" * 40)
                            
                            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                            soup = BeautifulSoup(response.text, 'html.parser')
                            text = soup.get_text(separator=' ', strip=True)
                            
                            # –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è
                            highlighted_text = text
                            for declension, count in data['declensions_found'].items():
                                if count > 0:
                                    # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è –Ω–∞ –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
                                    pattern = r'\b' + re.escape(declension) + r'\b'
                                    highlighted_text = re.sub(pattern, f"„Äê{declension}„Äë", highlighted_text, flags=re.IGNORECASE)
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π
                            preview = highlighted_text[:500] + "..." if len(highlighted_text) > 500 else highlighted_text
                            declensions_text.append(preview)
                            declensions_text.append("")
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —á–µ—Ä–µ–∑ DevTools
                            declensions_text.append("üîç –ü–†–û–í–ï–†–ö–ê –ß–ï–†–ï–ó DEVTOOLS:")
                            declensions_text.append("-" * 40)
                            declensions_text.append("1. –û—Ç–∫—Ä–æ–π—Ç–µ DevTools (F12)")
                            declensions_text.append("2. –ù–∞–∂–º–∏—Ç–µ Ctrl+F –¥–ª—è –ø–æ–∏—Å–∫–∞")
                            declensions_text.append("3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞–∂–¥–æ–µ —Å–∫–ª–æ–Ω–µ–Ω–∏–µ:")
                            for declension, count in data['declensions_found'].items():
                                if count > 0:
                                    declensions_text.append(f"   ‚Ä¢ '{declension}' - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å {count} —Ä–∞–∑")
                            declensions_text.append("")
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è
                            declensions_text.append("üìö –ù–ê–ô–î–ï–ù–ù–´–ï –°–ö–õ–û–ù–ï–ù–ò–Ø:")
                            declensions_text.append("-" * 50)
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è —Å –≥–∞–ª–æ—á–∫–∞–º–∏
                            for declension, count in data['declensions_found'].items():
                                if count > 0:
                                    declensions_text.append(f"  ‚úÖ '{declension}' - –Ω–∞–π–¥–µ–Ω–æ {count} —Ä–∞–∑")
                            
                            declensions_text.append("")
                            
                        else:
                            declensions_text.append("‚ùå –°–∫–ª–æ–Ω–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                            declensions_text.append("")
                        
                        declensions_text.append("=" * 60)
                        declensions_text.append("")
                else:
                    declensions_text.append("‚ùå –¶–µ–ª–µ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
                    declensions_text.append("")
                
                text_analysis_declensions.value = "\n".join(declensions_text)
                text_analysis_progress.value = 1.0
                text_analysis_status.value = "–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!"
                page.update()
                
            except Exception as ex:
                text_analysis_status.value = f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(ex)}"
                text_analysis_progress.value = 0.0
                page.update()
        
        threading.Thread(target=worker).start()
    
    text_analysis_declensions_btn = ft.ElevatedButton(
        "–ê–Ω–∞–ª–∏–∑ —Å–∫–ª–æ–Ω–µ–Ω–∏–π", 
        icon=ft.Icons.SEARCH, 
        on_click=text_analysis_declensions_run,
        style=ft.ButtonStyle(
            bgcolor="#F2CC0C",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    
    text_analysis_content.content = ft.Column([
        ft.Text("–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è SEO (–£–õ–£–ß–®–ï–ù–ù–´–ô)", size=24, weight=ft.FontWeight.BOLD, color=get_text_color()),
        ft.Text("–í–≤–µ–¥–∏—Ç–µ URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ (–≤–∫–ª—é—á–∞—è –∞–Ω–∫–æ—Ä—ã, –∏—Å–∫–ª—é—á–∞—è header/footer –∏ –∞—Ç—Ä–∏–±—É—Ç—ã)", size=16, color=get_secondary_text_color()),
        ft.Row([text_analysis_url_input, text_analysis_keywords_input, text_analysis_ssl_checkbox], spacing=10),
        ft.Row([text_analysis_run_btn, text_analysis_declensions_btn], spacing=10),
        text_analysis_progress,
        text_analysis_status,
        ft.Column([
            text_analysis_summary,
            ft.Container(height=20),
            text_analysis_keywords,
            ft.Container(height=20),
            text_analysis_structure,
            ft.Container(height=20),
            text_analysis_full_text
        ], expand=True),
        ft.Container(height=20),
        text_analysis_declensions
    ], expand=True)
    
    # --- –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ ---
    code_analysis_url_input = ft.TextField(
        label="URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞", 
        width=400, 
        filled=True, 
        border_radius=10,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    code_analysis_ssl_checkbox = ft.Checkbox(label="–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å SSL", value=True)
    code_analysis_progress = ft.ProgressBar(width=400, color="#F2E307", bgcolor="#394459", value=0.0, height=10, border_radius=20)
    code_analysis_status = ft.Text(visible=False)
    
    # –û–±–ª–∞—Å—Ç–∏ –¥–ª—è –≤—ã–≤–æ–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    code_analysis_summary = ft.TextField(
        label="–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞", 
        multiline=True, 
        min_lines=15, 
        max_lines=30, 
        width=1000, 
        filled=True, 
        border_radius=10, 
        expand=True,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    code_analysis_details = ft.TextField(
        label="–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑", 
        multiline=True, 
        min_lines=20, 
        max_lines=40, 
        width=1000, 
        filled=True, 
        border_radius=10,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    
    def code_analysis_run(e):
        url = code_analysis_url_input.value.strip()
        if not url.startswith('http'):
            code_analysis_status.value = "–í–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL!"
            code_analysis_status.visible = True
            page.update()
            return
        
        code_analysis_progress.value = 0.0
        code_analysis_status.value = "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–¥..."
        code_analysis_status.visible = True
        code_analysis_summary.value = ""
        code_analysis_details.value = ""
        page.update()
        
        def worker():
            try:
                # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                response = requests.get(url, timeout=15, verify=not code_analysis_ssl_checkbox.value)
                if response.status_code != 200:
                    code_analysis_status.value = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {response.status_code}"
                    page.update()
                    return
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–¥
                analysis = analyze_code_content(response.text, url)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—â—É—é —Å–≤–æ–¥–∫—É
                summary_parts = []
                summary_parts.append("üîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–û–î–ê –°–¢–†–ê–ù–ò–¶–´")
                summary_parts.append("=" * 60)
                summary_parts.append(f"URL: {analysis['url']}")
                summary_parts.append("")
                
                summary_parts.append("üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                summary_parts.append(f"‚Ä¢ –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞: {analysis['quality_score']}/100")
                summary_parts.append(f"‚Ä¢ –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {analysis['total_errors']}")
                summary_parts.append(f"‚Ä¢ –í—Å–µ–≥–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {analysis['total_warnings']}")
                summary_parts.append("")
                
                # HTML —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                if 'html_stats' in analysis:
                    html_stats = analysis['html_stats']
                    summary_parts.append("üåê HTML –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                    summary_parts.append(f"‚Ä¢ –í—Å–µ–≥–æ —Ç–µ–≥–æ–≤: {html_stats.get('total_tags', 0)}")
                    summary_parts.append(f"‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤: {html_stats.get('unique_tags', 0)}")
                    summary_parts.append(f"‚Ä¢ –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫: {html_stats.get('total_links', 0)}")
                    summary_parts.append(f"‚Ä¢ –í–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {html_stats.get('external_links', 0)}")
                    summary_parts.append(f"‚Ä¢ –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {html_stats.get('internal_links', 0)}")
                    summary_parts.append(f"‚Ä¢ –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {html_stats.get('total_images', 0)}")
                    summary_parts.append(f"‚Ä¢ –ü–æ–∫—Ä—ã—Ç–∏–µ alt: {html_stats.get('alt_coverage', 0):.1f}%")
                    if 'title_length' in html_stats:
                        summary_parts.append(f"‚Ä¢ –î–ª–∏–Ω–∞ title: {html_stats['title_length']} —Å–∏–º–≤–æ–ª–æ–≤")
                    summary_parts.append("")
                
                # CSS —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                if 'css_stats' in analysis:
                    css_stats = analysis['css_stats']
                    summary_parts.append("üé® CSS –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                    summary_parts.append(f"‚Ä¢ –ë–ª–æ–∫–æ–≤ —Å—Ç–∏–ª–µ–π: {css_stats.get('style_blocks', 0)}")
                    summary_parts.append(f"‚Ä¢ –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö —Å—Ç–∏–ª–µ–π: {css_stats.get('inline_styles', 0)}")
                    summary_parts.append(f"‚Ä¢ –í–Ω–µ—à–Ω–∏—Ö CSS —Ñ–∞–π–ª–æ–≤: {css_stats.get('external_css', 0)}")
                    summary_parts.append(f"‚Ä¢ CSS –ø—Ä–∞–≤–∏–ª: {css_stats.get('css_rules', 0)}")
                    summary_parts.append(f"‚Ä¢ –ú–µ–¥–∏–∞-–∑–∞–ø—Ä–æ—Å–æ–≤: {css_stats.get('media_queries', 0)}")
                    summary_parts.append(f"‚Ä¢ –ê–Ω–∏–º–∞—Ü–∏–π: {css_stats.get('animations', 0)}")
                    summary_parts.append(f"‚Ä¢ –ü–µ—Ä–µ—Ö–æ–¥–æ–≤: {css_stats.get('transitions', 0)}")
                    summary_parts.append("")
                
                # JavaScript —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                if 'js_stats' in analysis:
                    js_stats = analysis['js_stats']
                    summary_parts.append("‚ö° JAVASCRIPT –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                    summary_parts.append(f"‚Ä¢ –ë–ª–æ–∫–æ–≤ —Å–∫—Ä–∏–ø—Ç–æ–≤: {js_stats.get('script_blocks', 0)}")
                    summary_parts.append(f"‚Ä¢ –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤: {js_stats.get('inline_scripts', 0)}")
                    summary_parts.append(f"‚Ä¢ –í–Ω–µ—à–Ω–∏—Ö JS —Ñ–∞–π–ª–æ–≤: {js_stats.get('external_scripts', 0)}")
                    summary_parts.append(f"‚Ä¢ –§—É–Ω–∫—Ü–∏–π: {js_stats.get('functions', 0)}")
                    summary_parts.append(f"‚Ä¢ –°—Ç—Ä–µ–ª–æ—á–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π: {js_stats.get('arrow_functions', 0)}")
                    summary_parts.append(f"‚Ä¢ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (var): {js_stats.get('var_declarations', 0)}")
                    summary_parts.append(f"‚Ä¢ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (let): {js_stats.get('let_declarations', 0)}")
                    summary_parts.append(f"‚Ä¢ –ö–æ–Ω—Å—Ç–∞–Ω—Ç (const): {js_stats.get('const_declarations', 0)}")
                    summary_parts.append(f"‚Ä¢ Console.log: {js_stats.get('console_logs', 0)}")
                    summary_parts.append("")
                
                # PHP —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                if 'php_stats' in analysis:
                    php_stats = analysis['php_stats']
                    summary_parts.append("üêò PHP –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                    summary_parts.append(f"‚Ä¢ PHP –±–ª–æ–∫–æ–≤: {php_stats.get('php_blocks', 0)}")
                    summary_parts.append(f"‚Ä¢ PHP —Å—Ç—Ä–æ–∫: {php_stats.get('total_php_lines', 0)}")
                    summary_parts.append("")
                
                # SEO —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                if 'seo_stats' in analysis:
                    seo_stats = analysis['seo_stats']
                    summary_parts.append("üîç SEO –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                    if 'charset' in seo_stats:
                        summary_parts.append(f"‚Ä¢ –ö–æ–¥–∏—Ä–æ–≤–∫–∞: {seo_stats['charset']}")
                    if 'description_length' in seo_stats:
                        summary_parts.append(f"‚Ä¢ Meta description: {seo_stats['description_length']} —Å–∏–º–≤–æ–ª–æ–≤")
                    summary_parts.append(f"‚Ä¢ Open Graph —Ç–µ–≥–æ–≤: {seo_stats.get('og_tags', 0)}")
                    summary_parts.append(f"‚Ä¢ Twitter Cards: {seo_stats.get('twitter_tags', 0)}")
                    if 'canonical' in seo_stats:
                        summary_parts.append("‚Ä¢ Canonical URL: ‚úÖ")
                    summary_parts.append("")
                
                # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                if 'performance_stats' in analysis:
                    perf_stats = analysis['performance_stats']
                    summary_parts.append("‚ö° –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:")
                    summary_parts.append(f"‚Ä¢ HTML —Ä–∞–∑–º–µ—Ä: {perf_stats.get('html_size_kb', 0):.1f} KB")
                    summary_parts.append(f"‚Ä¢ CSS —Ä–∞–∑–º–µ—Ä: {perf_stats.get('css_size_kb', 0):.1f} KB")
                    summary_parts.append(f"‚Ä¢ JS —Ä–∞–∑–º–µ—Ä: {perf_stats.get('js_size_kb', 0):.1f} KB")
                    summary_parts.append("")
                
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                if analysis['quality_score'] >= 90:
                    summary_parts.append("üü¢ –û–¢–õ–ò–ß–ù–û–ï –ö–ê–ß–ï–°–¢–í–û –ö–û–î–ê")
                elif analysis['quality_score'] >= 70:
                    summary_parts.append("üü° –•–û–†–û–®–ï–ï –ö–ê–ß–ï–°–¢–í–û –ö–û–î–ê")
                elif analysis['quality_score'] >= 50:
                    summary_parts.append("üü† –°–†–ï–î–ù–ï–ï –ö–ê–ß–ï–°–¢–í–û –ö–û–î–ê")
                else:
                    summary_parts.append("üî¥ –ü–õ–û–•–û–ï –ö–ê–ß–ï–°–¢–í–û –ö–û–î–ê")
                summary_parts.append("")
                
                if analysis['positives']:
                    summary_parts.append("‚úÖ –ü–û–õ–û–ñ–ò–¢–ï–õ–¨–ù–´–ï –ú–û–ú–ï–ù–¢–´:")
                    for positive in analysis['positives']:
                        summary_parts.append(f"‚Ä¢ {positive}")
                    summary_parts.append("")
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                details_parts = []
                details_parts.append("üîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–û–î–ê")
                details_parts.append("=" * 60)
                details_parts.append("")
                
                # HTML –∞–Ω–∞–ª–∏–∑
                details_parts.append("üåê HTML –ê–ù–ê–õ–ò–ó:")
                details_parts.append("-" * 30)
                if analysis['html_errors']:
                    details_parts.append("‚ùå –û–®–ò–ë–ö–ò:")
                    for error in analysis['html_errors']:
                        details_parts.append(f"  {error}")
                    details_parts.append("")
                
                if analysis['html_warnings']:
                    details_parts.append("‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:")
                    for warning in analysis['html_warnings']:
                        details_parts.append(f"  {warning}")
                    details_parts.append("")
                
                # –¢–æ–ø —Ç–µ–≥–æ–≤
                if 'html_stats' in analysis and 'top_tags' in analysis['html_stats']:
                    details_parts.append("üìä –¢–û–ü-10 –¢–ï–ì–û–í:")
                    for tag, count in analysis['html_stats']['top_tags']:
                        details_parts.append(f"  ‚Ä¢ <{tag}>: {count} —Ä–∞–∑")
                    details_parts.append("")
                
                # CSS –∞–Ω–∞–ª–∏–∑
                details_parts.append("üé® CSS –ê–ù–ê–õ–ò–ó:")
                details_parts.append("-" * 30)
                if analysis['css_errors']:
                    details_parts.append("‚ùå –û–®–ò–ë–ö–ò:")
                    for error in analysis['css_errors']:
                        details_parts.append(f"  {error}")
                    details_parts.append("")
                
                if analysis['css_warnings']:
                    details_parts.append("‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:")
                    for warning in analysis['css_warnings']:
                        details_parts.append(f"  {warning}")
                    details_parts.append("")
                
                # JavaScript –∞–Ω–∞–ª–∏–∑
                details_parts.append("‚ö° JAVASCRIPT –ê–ù–ê–õ–ò–ó:")
                details_parts.append("-" * 30)
                if analysis['js_errors']:
                    details_parts.append("‚ùå –û–®–ò–ë–ö–ò:")
                    for error in analysis['js_errors']:
                        details_parts.append(f"  {error}")
                    details_parts.append("")
                
                if analysis['js_warnings']:
                    details_parts.append("‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:")
                    for warning in analysis['js_warnings']:
                        details_parts.append(f"  {warning}")
                    details_parts.append("")
                
                # PHP –∞–Ω–∞–ª–∏–∑
                details_parts.append("üêò PHP –ê–ù–ê–õ–ò–ó:")
                details_parts.append("-" * 30)
                if analysis['php_errors']:
                    details_parts.append("‚ùå –û–®–ò–ë–ö–ò:")
                    for error in analysis['php_errors']:
                        details_parts.append(f"  {error}")
                    details_parts.append("")
                
                if analysis['php_warnings']:
                    details_parts.append("‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:")
                    for warning in analysis['php_warnings']:
                        details_parts.append(f"  {warning}")
                    details_parts.append("")
                
                # SEO –∞–Ω–∞–ª–∏–∑
                details_parts.append("üîç SEO –ê–ù–ê–õ–ò–ó:")
                details_parts.append("-" * 30)
                if analysis.get('seo_errors'):
                    details_parts.append("‚ùå –û–®–ò–ë–ö–ò:")
                    for error in analysis['seo_errors']:
                        details_parts.append(f"  {error}")
                    details_parts.append("")
                
                if analysis.get('seo_warnings'):
                    details_parts.append("‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:")
                    for warning in analysis['seo_warnings']:
                        details_parts.append(f"  {warning}")
                    details_parts.append("")
                
                # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
                details_parts.append("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ:")
                details_parts.append("-" * 40)
                
                if analysis['total_errors'] > 0:
                    details_parts.append("‚Ä¢ –ò—Å–ø—Ä–∞–≤—å—Ç–µ –≤—Å–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å")
                
                if analysis['html_warnings']:
                    details_parts.append("‚Ä¢ –î–æ–±–∞–≤—å—Ç–µ alt –∞—Ç—Ä–∏–±—É—Ç—ã –∫–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º")
                    details_parts.append("‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏ –∏—Å–ø—Ä–∞–≤—å—Ç–µ –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–µ HTML —Ç–µ–≥–∏")
                    details_parts.append("‚Ä¢ –£–∫–∞–∂–∏—Ç–µ –∞—Ç—Ä–∏–±—É—Ç lang –≤ —Ç–µ–≥–µ <html>")
                
                if analysis['css_warnings']:
                    details_parts.append("‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å CSS –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã—Ö —Å–∫–æ–±–æ–∫")
                    details_parts.append("‚Ä¢ –î–æ–±–∞–≤—å—Ç–µ —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π –≥–¥–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ")
                    details_parts.append("‚Ä¢ –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–Ω–µ—Å–µ–Ω–∏—è CSS –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã")
                
                if analysis['js_warnings']:
                    details_parts.append("‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å JavaScript")
                    details_parts.append("‚Ä¢ –î–æ–±–∞–≤—å—Ç–µ —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π –≤ –∫–æ–Ω—Ü–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–π")
                    details_parts.append("‚Ä¢ –£–±–µ—Ä–∏—Ç–µ console.log –∏–∑ –ø—Ä–æ–¥–∞–∫—à–µ–Ω –∫–æ–¥–∞")
                    details_parts.append("‚Ä¢ –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–Ω–µ—Å–µ–Ω–∏—è JS –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã")
                
                if analysis['php_warnings']:
                    details_parts.append("‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å PHP –∫–æ–¥–∞")
                    details_parts.append("‚Ä¢ –£–±–µ–¥–∏—Ç–µ—Å—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–∫—Ä—ã—Ç–∏—è –≤—Å–µ—Ö —Å–∫–æ–±–æ–∫")
                
                if analysis.get('seo_warnings'):
                    details_parts.append("‚Ä¢ –î–æ–±–∞–≤—å—Ç–µ meta description")
                    details_parts.append("‚Ä¢ –£–∫–∞–∂–∏—Ç–µ canonical URL")
                    details_parts.append("‚Ä¢ –î–æ–±–∞–≤—å—Ç–µ Open Graph —Ç–µ–≥–∏")
                    details_parts.append("‚Ä¢ –£–∫–∞–∂–∏—Ç–µ viewport meta —Ç–µ–≥")
                
                # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                if 'performance_stats' in analysis:
                    perf_stats = analysis['performance_stats']
                    if perf_stats.get('html_size_kb', 0) > 100:
                        details_parts.append("‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Ä–∞–∑–º–µ—Ä HTML (—Å–µ–π—á–∞—Å > 100KB)")
                    if perf_stats.get('css_size_kb', 0) > 50:
                        details_parts.append("‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Ä–∞–∑–º–µ—Ä CSS (—Å–µ–π—á–∞—Å > 50KB)")
                    if perf_stats.get('js_size_kb', 0) > 100:
                        details_parts.append("‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Ä–∞–∑–º–µ—Ä JavaScript (—Å–µ–π—á–∞—Å > 100KB)")
                
                if analysis['quality_score'] < 70:
                    details_parts.append("‚Ä¢ –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –∫–æ–¥–∞")
                    details_parts.append("‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä—ã HTML/CSS/JS –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
                    details_parts.append("‚Ä¢ –í–Ω–µ–¥—Ä–∏—Ç–µ –ª–∏–Ω—Ç–µ—Ä—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–¥–∞")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
                code_analysis_summary.value = "\n".join(summary_parts)
                code_analysis_details.value = "\n".join(details_parts)
                code_analysis_status.value = "–ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ –∑–∞–≤–µ—Ä—à–µ–Ω!"
                code_analysis_progress.value = 1.0
                
            except Exception as ex:
                code_analysis_status.value = f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(ex)}"
                code_analysis_progress.value = 0.0
            
            page.update()
        
        threading.Thread(target=worker).start()
    
    code_analysis_run_btn = ft.ElevatedButton(
        "–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥ (–£–õ–£–ß–®–ï–ù–ù–´–ô)", 
        icon=ft.Icons.PLAY_ARROW, 
        on_click=code_analysis_run,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    
    code_analysis_content.content = ft.Column([
        ft.Text("–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", size=24, weight=ft.FontWeight.BOLD, color=get_text_color()),
        ft.Text("–í–≤–µ–¥–∏—Ç–µ URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ HTML, CSS, JavaScript, PHP –∏ SEO", size=16, color=get_secondary_text_color()),
        ft.Row([code_analysis_url_input, code_analysis_ssl_checkbox, code_analysis_run_btn], spacing=10),
        code_analysis_progress,
        code_analysis_status,
        ft.Column([
            code_analysis_summary,
            ft.Container(height=20),
            code_analysis_details
        ], expand=True)
    ], expand=True)
    
    # --- –†–µ–¥–∏—Ä–µ–∫—Ç—ã ---
    redirects_input = ft.TextField(
        label="–°–ø–∏—Å–æ–∫ URL (–ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ)", 
        multiline=True, 
        min_lines=5, 
        width=600, 
        filled=True, 
        border_radius=10,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    redirects_ssl_checkbox = ft.Checkbox(label="–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å SSL", value=True)
    redirects_progress = ft.ProgressBar(width=400, color="#F2E307", bgcolor="#394459", value=0.0, height=10, border_radius=20)
    redirects_table = ft.DataTable(
        columns=[
            ft.DataColumn(label=ft.Text("–ò—Å—Ö–æ–¥–Ω—ã–π URL", color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("–†–µ–¥–∏—Ä–µ–∫—Ç", color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("–ö–æ–Ω–µ—á–Ω—ã–π URL", color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("HTTP", color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("OK", color=get_input_text_color())),
        ],
        rows=[],
        horizontal_lines=ft.BorderSide(1, "#394459"),
        vertical_lines=ft.BorderSide(1, "#394459"),
        bgcolor="#F2F2F2",
        border=ft.border.all(1, "#394459"),
        border_radius=10,
    )
    redirects_export_btn = ft.ElevatedButton(
        "–≠–∫—Å–ø–æ—Ä—Ç –≤ Excel", 
        icon=ft.Icons.DOWNLOAD, 
        visible=False,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=8),
            elevation=3
        )
    )
    redirects_status = ft.Text(visible=False)

    def redirects_update(done, total):
        redirects_progress.value = min(1.0, done / max(total, 1))
        page.update()

    def redirects_done(results):
        redirects_table.rows = [
            ft.DataRow(cells=[
                ft.DataCell(ft.Text(r['–ò—Å—Ö–æ–¥–Ω—ã–π URL'], color=get_input_text_color())),
                ft.DataCell(ft.Text(r['–†–µ–¥–∏—Ä–µ–∫—Ç'], color=get_input_text_color())),
                ft.DataCell(ft.Text(r['–ö–æ–Ω–µ—á–Ω—ã–π URL'], color=get_input_text_color())),
                ft.DataCell(ft.Text(str(r['HTTP']), color=get_input_text_color())),
                ft.DataCell(ft.Text('üü¢' if r['OK'] else 'üî¥', color=get_input_text_color()))
            ]) for r in results
        ]
        redirects_export_btn.visible = True
        redirects_status.value = f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: {len(results)} URL"
        redirects_status.visible = True
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
        import pandas as pd
        df = pd.DataFrame(results)
        fname = f"reports/redirects_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        df.to_excel(fname, index=False)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
        page.snack_bar = ft.SnackBar(content=ft.Text(f"‚úÖ Excel –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {os.path.basename(fname)}"))
        page.snack_bar.open = True
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–æ–≤ –µ—Å–ª–∏ –æ—Ç–∫—Ä—ã—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
        if exports_content.visible:
            refresh_exports_list()
        
        page.update()

    def redirects_run(e):
        urls = [u.strip() for u in redirects_input.value.splitlines() if u.strip()]
        if not urls:
            redirects_status.value = "–í–≤–µ–¥–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω URL!"
            redirects_status.visible = True
            page.update()
            return
        redirects_progress.value = 0.0
        redirects_table.rows = []
        redirects_export_btn.visible = False
        redirects_status.visible = False
        page.update()
        threading.Thread(target=lambda: check_redirects(urls, redirects_ssl_checkbox.value, redirects_update, redirects_done)).start()

    redirects_run_btn = ft.ElevatedButton(
        "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã", 
        icon=ft.Icons.PLAY_ARROW, 
        on_click=redirects_run,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    redirects_export_btn.on_click = lambda e: None  # –£–∂–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    redirects_content.content = ft.Column([
        ft.Text("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤", size=20, weight=ft.FontWeight.BOLD, color=get_text_color()),
        ft.Row([redirects_input, redirects_ssl_checkbox, redirects_run_btn]),
        redirects_progress,
        ft.Container(
            ft.Row([
                ft.Container(redirects_table, width=2000, expand=True)
            ], scroll=ft.ScrollMode.ALWAYS),
            expand=True,
            bgcolor="#F2F2F2",
            border_radius=10,
            padding=5,
            alignment=ft.alignment.center,
            border=ft.border.all(1, "#394459"),
        ),
        redirects_export_btn,
        redirects_status
    ], expand=True)

    # --- –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ ---
    competitors_input = ft.TextField(
        label="URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–¥–æ 5, —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)", 
        width=600, 
        filled=True, 
        border_radius=10,
        bgcolor="#F2F2F2",
        border_color="#394459",
        focused_border_color="#F2E307",
        color=get_input_text_color(),
        label_style=ft.TextStyle(color=get_label_color())
    )
    competitors_btn = ft.ElevatedButton(
        "–°—Ä–∞–≤–Ω–∏—Ç—å", 
        icon=ft.Icons.SEARCH,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=5
        )
    )
    competitors_status = ft.Text(visible=False)
    competitors_table = ft.DataTable(
        columns=[
            ft.DataColumn(label=ft.Text("URL", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("–°–∫–æ—Ä–æ—Å—Ç—å\n–∑–∞–≥—Ä—É–∑–∫–∏, —Å–µ–∫", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("H1/\nH2", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("Title", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("Description", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("URL\n–≤ sitemap", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("Top-3\n–∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("–í–Ω—É—Ç—Ä.\n—Å—Å—ã–ª–∫–∏", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("–í–Ω–µ—à–Ω.\n—Å—Å—ã–ª–∫–∏", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("–ò–∑–æ–±—Ä.", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("–°–∏–º–≤–æ–ª–æ–≤\n–Ω–∞ —Å—Ç—Ä.", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("Meta\nkeywords", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("Canonical", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("OpenGraph", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("Twitter\nCard", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("JSON-LD", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
            ft.DataColumn(label=ft.Text("–û—à–∏–±–∫–∞", size=12, text_align=ft.TextAlign.CENTER, color=get_input_text_color())),
        ],
        rows=[],
        horizontal_lines=ft.BorderSide(1, "#394459"),
        vertical_lines=ft.BorderSide(1, "#394459"),
        data_row_min_height=32,
        data_row_max_height=160,
        bgcolor="#F2F2F2",
        border=ft.border.all(1, "#394459"),
        border_radius=10,
    )
    competitors_ideas = ft.Text(visible=False)

    def analyze_competitors(e):
        urls = [u.strip() for u in competitors_input.value.split(',') if u.strip()][:5]
        if not urls:
            competitors_status.value = "–í–≤–µ–¥–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞!"
            competitors_status.visible = True
            page.update()
            return
        competitors_status.value = "–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤..."
        competitors_status.visible = True
        competitors_table.rows = []
        competitors_ideas.value = ""
        competitors_ideas.visible = False
        page.update()
        import threading
        def worker():
            import requests
            from bs4 import BeautifulSoup
            import xml.etree.ElementTree as ET
            results = []
            ideas = []
            all_keywords = set()
            for url in urls:
                try:
                    t0 = time.time()
                    r = requests.get(url, timeout=15, verify=False)
                    load_time = time.time() - t0
                    soup = BeautifulSoup(r.text, 'html.parser')
                    h1 = len(soup.find_all('h1'))
                    h2 = len(soup.find_all('h2'))
                    title = soup.title.string.strip() if soup.title and soup.title.string else ''
                    if len(title) > 60:
                        title = title[:57] + '...'
                    desc = ''
                    meta_desc = soup.find('meta', attrs={'name': 'description'})
                    if meta_desc:
                        desc = meta_desc.get('content', '')
                    if len(desc) > 120:
                        desc = desc[:117] + '...'
                    # sitemap
                    sitemap_url = url.rstrip('/') + '/sitemap.xml'
                    try:
                        r_s = requests.get(sitemap_url, timeout=10, verify=False)
                        if r_s.status_code == 200:
                            root = ET.fromstring(r_s.text)
                            sitemap_count = len([u for u in root.iter('{http://www.sitemaps.org/schemas/sitemap/0.9}url')])
                        else:
                            sitemap_count = 0
                    except Exception as ex_s:
                        sitemap_count = 0
                    # keywords
                    text = soup.get_text(separator=' ', strip=True).lower()
                    words = re.findall(r'\w+', text)
                    stop_words = {'–∏','–≤','–Ω–∞','–Ω–µ','—Å','–∞','–æ','–¥–ª—è','–ø–æ','–∏–∑','–∫','—É','–æ—Ç','–Ω–æ','–∫–∞–∫','—á—Ç–æ','—ç—Ç–æ','—Ç–æ','–∏–ª–∏','–∑–∞','–ø—Ä–∏'}
                    word_freq = {}
                    for word in words:
                        if word not in stop_words and len(word) > 3:
                            word_freq[word] = word_freq.get(word, 0) + 1
                    keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:3]
                    all_keywords.update([k[0] for k in keywords])
                    keywords_str = '\n'.join([k[0] for k in keywords])
                    # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ/–≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏
                    domain = url.split('//')[-1].split('/')[0]
                    a_tags = soup.find_all('a', href=True)
                    internal_links = [a for a in a_tags if domain in a['href'] or a['href'].startswith('/')]
                    external_links = [a for a in a_tags if domain not in a['href'] and a['href'].startswith('http')]
                    # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    img_count = len(soup.find_all('img'))
                    # –°–∏–º–≤–æ–ª—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                    char_count = len(text)
                    # meta keywords
                    meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
                    meta_keywords_count = len(meta_keywords.get('content', '').split(',')) if meta_keywords and meta_keywords.get('content') else 0
                    # canonical
                    canonicals = soup.find_all('link', rel='canonical')
                    canonical_count = len(canonicals)
                    # OpenGraph
                    og_tags = [m for m in soup.find_all('meta') if m.get('property', '').startswith('og:')]
                    og_count = len(og_tags)
                    # Twitter Card
                    tw_tags = [m for m in soup.find_all('meta') if m.get('name', '').startswith('twitter:')]
                    tw_count = len(tw_tags)
                    # JSON-LD
                    jsonld_blocks = soup.find_all('script', {'type': 'application/ld+json'})
                    jsonld_count = len(jsonld_blocks)
                    results.append([
                        url,
                        f"{load_time:.2f}",
                        f"{h1}/{h2}",
                        title,
                        desc,
                        str(sitemap_count),
                        keywords_str,
                        len(internal_links),
                        len(external_links),
                        img_count,
                        char_count,
                        meta_keywords_count,
                        canonical_count,
                        og_count,
                        tw_count,
                        jsonld_count,
                        ''
                    ])
                except Exception as ex:
                    results.append([url, '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', f'{ex}'])
            # –ò–¥–µ–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            if results:
                min_speed = min([float(r[1]) for r in results if r[1] not in ('–û—à–∏–±–∫–∞','') and not r[1].startswith('–û—à–∏–±–∫–∞:')], default=None)
                max_h1 = max([int(r[2].split('/')[0]) for r in results if r[2] and r[2] != '–û—à–∏–±–∫–∞' and not r[2].startswith('–û—à–∏–±–∫–∞:')], default=None)
                max_h2 = max([int(r[2].split('/')[1]) for r in results if r[2] and r[2] != '–û—à–∏–±–∫–∞' and not r[2].startswith('–û—à–∏–±–∫–∞:')], default=None)
                max_sitemap = max([int(r[5]) for r in results if r[5].isdigit()], default=None)
                if min_speed is not None:
                    ideas.append(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ —É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: {min_speed:.2f} —Å–µ–∫. –°—Ç–∞—Ä–∞–π—Ç–µ—Å—å –±—ã—Ç—å –Ω–µ —Ö—É–∂–µ!")
                if max_h1 is not None and max_h1 > 1:
                    ideas.append(f"–£ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –±–æ–ª—å—à–µ 1 H1 ‚Äî —ç—Ç–æ –ø–ª–æ—Ö–æ. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω H1!")
                if max_h2 is not None and max_h2 < 2:
                    ideas.append(f"–£ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –º–∞–ª–æ H2 ‚Äî –¥–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.")
                if max_sitemap is not None:
                    ideas.append(f"–£ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤ sitemap –¥–æ {max_sitemap} URL. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–ª–Ω–æ—Ç—É —Å–≤–æ–µ–π –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞.")
                if all_keywords:
                    ideas.append(f"–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: {', '.join(list(all_keywords)[:10])}")
            competitors_table.rows = [ft.DataRow(cells=[ft.DataCell(ft.Text(str(cell), color=get_input_text_color())) for cell in row]) for row in results]
            competitors_status.value = f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: {len(results)}"
            competitors_status.visible = True
            competitors_ideas.value = '\n'.join(ideas)
            competitors_ideas.visible = True if ideas else False
            page.update()
        threading.Thread(target=worker).start()
    competitors_btn.on_click = analyze_competitors
    competitors_content.content = ft.Column([
        ft.Text("–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤", size=24, weight=ft.FontWeight.BOLD, color=get_text_color()),
        ft.Text("–í–≤–µ–¥–∏—Ç–µ –¥–æ 5 URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é. –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä–µ, –º–µ—Ç–∞-—Ç–µ–≥–∞–º –∏ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.", color=get_secondary_text_color()),
        ft.Row([competitors_input, competitors_btn]),
        competitors_status,
        ft.Container(
            ft.Row([
                ft.Container(competitors_table, width=2400, expand=True)
            ], scroll=ft.ScrollMode.ALWAYS),
            expand=True,
            bgcolor="#F2F2F2",
            border_radius=10,
            padding=5,
            alignment=ft.alignment.center,
            border=ft.border.all(1, "#394459"),
        ),
        competitors_ideas
    ], expand=True)

    # --- –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —ç–∫—Å–ø–æ—Ä—Ç–∞–º–∏ ---
    def get_file_icon(file_path):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∫–æ–Ω–∫—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞."""
        if file_path.lower().endswith('.xlsx'):
            return ft.Icon(ft.Icons.TABLE_CHART, color="#217346", size=32)
        elif file_path.lower().endswith('.docx'):
            return ft.Icon(ft.Icons.DESCRIPTION, color="#2B579A", size=32)
        else:
            return ft.Icon(ft.Icons.INSERT_DRIVE_FILE, color="#666666", size=32)
    
    def get_file_size(file_path):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ —á–∏—Ç–∞–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."""
        try:
            size_bytes = os.path.getsize(file_path)
            if size_bytes < 1024:
                return f"{size_bytes} –ë"
            elif size_bytes < 1024 * 1024:
                return f"{size_bytes / 1024:.1f} –ö–ë"
            else:
                return f"{size_bytes / (1024 * 1024):.1f} –ú–ë"
        except:
            return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    def get_file_date(file_path):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞—Ç—É —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞."""
        try:
            timestamp = os.path.getctime(file_path)
            return datetime.fromtimestamp(timestamp).strftime('%d.%m.%Y %H:%M')
        except:
            return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    def open_file(file_path):
        """–û—Ç–∫—Ä—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –≤ —Å–∏—Å—Ç–µ–º–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
        try:
            import subprocess
            import platform
            if platform.system() == 'Windows':
                os.startfile(file_path)
            elif platform.system() == 'Darwin':  # macOS
                subprocess.run(['open', file_path])
            else:  # Linux
                subprocess.run(['xdg-open', file_path])
            page.snack_bar = ft.SnackBar(content=ft.Text(f"–§–∞–π–ª –æ—Ç–∫—Ä—ã—Ç: {os.path.basename(file_path)}"))
            page.snack_bar.open = True
        except Exception as e:
            page.snack_bar = ft.SnackBar(content=ft.Text(f"–û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è —Ñ–∞–π–ª–∞: {str(e)}"))
            page.snack_bar.open = True
        page.update()
    
    def refresh_exports_list():
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤."""
        try:
            exports_list.controls.clear()
            
            if not os.path.exists(REPORT_DIR):
                exports_list.controls.append(
                    ft.Container(
                        content=ft.Text("–ü–∞–ø–∫–∞ reports –Ω–µ –Ω–∞–π–¥–µ–Ω–∞", color="#666666"),
                        padding=20,
                        alignment=ft.alignment.center
                    )
                )
                page.update()
                return
            
            files = []
            for filename in os.listdir(REPORT_DIR):
                if filename.endswith(('.xlsx', '.docx')):
                    file_path = os.path.join(REPORT_DIR, filename)
                    files.append({
                        'name': filename,
                        'path': file_path,
                        'size': get_file_size(file_path),
                        'date': get_file_date(file_path),
                        'type': 'Excel' if filename.endswith('.xlsx') else 'Word'
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –¥–∞—Ç–µ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ —Å–≤–µ—Ä—Ö—É)
            files.sort(key=lambda x: os.path.getctime(x['path']), reverse=True)
            
            if not files:
                exports_list.controls.append(
                    ft.Container(
                        content=ft.Text("–ù–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤", color="#666666"),
                        padding=20,
                        alignment=ft.alignment.center
                    )
                )
            else:
                for file_info in files:
                    file_card = ft.Card(
                        content=ft.Container(
                            content=ft.Row([
                                get_file_icon(file_info['path']),
                                ft.Column([
                                    ft.Text(
                                        file_info['name'], 
                                        weight=ft.FontWeight.BOLD,
                                        color=get_text_color(),
                                        size=16
                                    ),
                                    ft.Text(
                                        f"–¢–∏–ø: {file_info['type']} ‚Ä¢ –†–∞–∑–º–µ—Ä: {file_info['size']} ‚Ä¢ –°–æ–∑–¥–∞–Ω: {file_info['date']}",
                                        color=get_text_color(),
                                        size=12
                                    )
                                ], expand=True),
                                ft.ElevatedButton(
                                    "–û—Ç–∫—Ä—ã—Ç—å",
                                    icon=ft.Icons.OPEN_IN_NEW,
                                    style=ft.ButtonStyle(
                                        bgcolor="#F2E307",
                                        color="#394459",
                                        shape=ft.RoundedRectangleBorder(radius=8),
                                        elevation=3
                                    ),
                                    on_click=lambda e, path=file_info['path']: open_file(path)
                                )
                            ], spacing=15),
                            padding=20
                        ),
                        elevation=5,
                        margin=ft.Margin(0, 0, 0, 10)
                    )
                    exports_list.controls.append(file_card)
            
            page.update()
        except Exception as e:
            exports_list.controls.clear()
            exports_list.controls.append(
                ft.Container(
                    content=ft.Text(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤: {str(e)}", color="#FF6B6B"),
                    padding=20,
                    alignment=ft.alignment.center
                )
            )
            page.update()
    
    # --- –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã —ç–∫—Å–ø–æ—Ä—Ç–∞ ---
    exports_list = ft.Column([], scroll=ft.ScrollMode.AUTO, expand=True)
    refresh_exports_btn = ft.ElevatedButton(
        "üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫",
        icon=ft.Icons.REFRESH,
        style=ft.ButtonStyle(
            bgcolor="#F2E307",
            color="#394459",
            shape=ft.RoundedRectangleBorder(radius=10),
            elevation=3
        ),
        on_click=lambda e: refresh_exports_list()
    )
    
    exports_content.content = ft.Column([
        ft.Text("üìÅ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã", size=24, weight=ft.FontWeight.BOLD, color=get_text_color()),
        ft.Text("–ó–¥–µ—Å—å –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –≤—Å–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç—á–µ—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–∞—Ö Excel –∏ Word", color=get_secondary_text_color()),
        ft.Row([refresh_exports_btn], alignment=ft.MainAxisAlignment.END),
        ft.Container(
            exports_list,
            expand=True,
            bgcolor="#F2F2F2",
            border_radius=10,
            padding=20,
            border=ft.border.all(1, "#394459"),
        )
    ], expand=True)

    # --- Layout ---
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–∫—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É (–≥–ª–∞–≤–Ω–∞—è)
    switch_page(0)
    
    page.add(
        ft.Column([
            navigation_bar,
            ft.Stack([
                main_content,
                links_check_content,
                parser_content,
                text_analysis_content,
                code_analysis_content,
                redirects_content,
                competitors_content,
                exports_content,
                serp_tracker_content
            ], expand=True)
        ], expand=True)
    )

# --- –î–û–ë–ê–í–ò–¢–¨ –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---
def get_core_web_vitals(driver):
    """–°–æ–±–∏—Ä–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ Core Web Vitals —á–µ—Ä–µ–∑ JS."""
    try:
        # LCP –∏ CLS –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑ PerformanceObserver, FID ‚Äî —Ç–æ–ª—å–∫–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏, –Ω–æ –º–æ–∂–Ω–æ —ç–º—É–ª–∏—Ä–æ–≤–∞—Ç—å
        script = '''
        var resolve = arguments[0];
        let lcp = 0, cls = 0;
        try {
            new PerformanceObserver((entryList) => {
                for (const entry of entryList.getEntries()) {
                    if (entry.entryType === 'largest-contentful-paint') {
                        lcp = entry.renderTime || entry.loadTime || entry.startTime;
                    }
                    if (entry.entryType === 'layout-shift' && !entry.hadRecentInput) {
                        cls += entry.value;
                    }
                }
            }).observe({type: 'largest-contentful-paint', buffered: true});
            new PerformanceObserver((entryList) => {
                for (const entry of entryList.getEntries()) {
                    if (entry.entryType === 'layout-shift' && !entry.hadRecentInput) {
                        cls += entry.value;
                    }
                }
            }).observe({type: 'layout-shift', buffered: true});
        } catch (e) {
            console.log('PerformanceObserver error:', e);
        }
        setTimeout(() => {
            resolve({lcp, cls});
        }, 3500);
        '''
        result = driver.execute_async_script(script)
        lcp = result.get('lcp', 0)
        cls = result.get('cls', 0)
        # FID —ç–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω–æ, –Ω–æ –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å 0 (–∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∑–∂–µ —á–µ—Ä–µ–∑ Lighthouse)
        fid = 0
        return lcp, fid, cls
    except Exception as ex:
        log_to_file(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è Core Web Vitals: {str(ex)}")
        return 0, 0, 0

def get_microdata(driver):
    """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –≤–∏–¥—ã –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏: Schema.org, JSON-LD, OpenGraph, Twitter Cards."""
    try:
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        # Schema.org (itemscope/itemtype)
        schema_items = []
        for tag in soup.find_all(attrs={"itemscope": True}):
            itemtype = tag.get("itemtype", "")
            schema_items.append(itemtype)
        # JSON-LD
        jsonld_blocks = []
        for script in soup.find_all("script", {"type": "application/ld+json"}):
            try:
                data = json.loads(script.string)
                jsonld_blocks.append(data)
            except Exception:
                pass
        # OpenGraph
        og_tags = {}
        for meta in soup.find_all("meta"):
            prop = meta.get("property", "")
            if prop.startswith("og:"):
                og_tags[prop] = meta.get("content", "")
        # Twitter Cards
        twitter_tags = {}
        for meta in soup.find_all("meta"):
            name = meta.get("name", "")
            if name.startswith("twitter:"):
                twitter_tags[name] = meta.get("content", "")
        return schema_items, jsonld_blocks, og_tags, twitter_tags
    except Exception as ex:
        log_to_file(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏: {str(ex)}")
        return [], [], {}, {}

def process_sitemap_recursively(sitemap_url, ignore_ssl, visited_sitemaps=None, max_depth=3):
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç sitemap index —Ñ–∞–π–ª—ã –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ URL —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
    
    Args:
        sitemap_url: URL sitemap —Ñ–∞–π–ª–∞
        ignore_ssl: –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å SSL –æ—à–∏–±–∫–∏
        visited_sitemaps: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —É–∂–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö sitemap (–¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ü–∏–∫–ª–æ–≤)
        max_depth: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ —Ä–µ–∫—É—Ä—Å–∏–∏
    
    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö URL, –∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    """
    if visited_sitemaps is None:
        visited_sitemaps = set()
    
    if max_depth <= 0 or sitemap_url in visited_sitemaps:
        return {
            'urls': [], 
            'sources': {}, 
            'metadata': {},  # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ URL
            'errors': [f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –∏–ª–∏ —Ü–∏–∫–ª–∏—á–µ—Å–∫–∞—è —Å—Å—ã–ª–∫–∞: {sitemap_url}"]
        }
    
    visited_sitemaps.add(sitemap_url)
    result = {'urls': [], 'sources': {}, 'metadata': {}, 'errors': []}
    
    try:
        response = requests.get(sitemap_url, timeout=10, verify=not ignore_ssl)
        if response.status_code != 200:
            result['errors'].append(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å sitemap: {sitemap_url} (—Å—Ç–∞—Ç—É—Å: {response.status_code})")
            return result
        
        root = ET.fromstring(response.text)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ sitemap index –∏–ª–∏ –æ–±—ã—á–Ω—ã–π sitemap
        if 'sitemapindex' in root.tag:
            # –≠—Ç–æ sitemap index - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
            sitemap_elems = root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}sitemap')
            for sitemap_elem in sitemap_elems:
                sub_sitemap_url = sitemap_elem.findtext('{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
                if sub_sitemap_url:
                    sub_result = process_sitemap_recursively(
                        sub_sitemap_url, 
                        ignore_ssl, 
                        visited_sitemaps.copy(), 
                        max_depth - 1
                    )
                    result['urls'].extend(sub_result['urls'])
                    result['sources'].update(sub_result['sources'])
                    result['metadata'].update(sub_result['metadata'])
                    result['errors'].extend(sub_result['errors'])
        
        elif 'urlset' in root.tag:
            # –≠—Ç–æ –æ–±—ã—á–Ω—ã–π sitemap - –∏–∑–≤–ª–µ–∫–∞–µ–º URL —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            url_elems = root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url')
            for url_elem in url_elems:
                url = url_elem.findtext('{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
                if url:
                    result['urls'].append(url)
                    result['sources'][url] = sitemap_url
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    lastmod = url_elem.findtext('{http://www.sitemaps.org/schemas/sitemap/0.9}lastmod') or '-'
                    priority = url_elem.findtext('{http://www.sitemaps.org/schemas/sitemap/0.9}priority') or '-'
                    changefreq = url_elem.findtext('{http://www.sitemaps.org/schemas/sitemap/0.9}changefreq') or '-'
                    
                    result['metadata'][url] = {
                        'lastmod': lastmod,
                        'priority': priority,
                        'changefreq': changefreq
                    }
        
        else:
            result['errors'].append(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç sitemap: {sitemap_url}")
    
    except ET.ParseError as e:
        result['errors'].append(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML –≤ {sitemap_url}: {str(e)}")
    except Exception as e:
        result['errors'].append(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {sitemap_url}: {str(e)}")
    
    return result

if __name__ == "__main__":
    ft.app(target=main)